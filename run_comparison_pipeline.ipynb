{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5104be8e",
   "metadata": {},
   "source": [
    "## 1. Setup - Colab & Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cbdc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Colab kontrolÃ¼\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"âœ… Google Colab ortamÄ± tespit edildi\")\n",
    "\n",
    "    # GPU kontrolÃ¼\n",
    "    import torch\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"âœ… GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"   VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    else:\n",
    "        print(\"âš ï¸  GPU bulunamadÄ±! Runtime > Change runtime type > GPU seÃ§in\")\n",
    "\n",
    "    # GitHub'dan projeyi klonla\n",
    "    print(\"\\nğŸ“¥ Proje indiriliyor...\")\n",
    "    !git clone https://github.com/Aliekinozcetin/Mitre_Attack_TTP_Mapping.git\n",
    "\n",
    "    # Proje dizinine geÃ§\n",
    "    os.chdir('Mitre_Attack_TTP_Mapping')\n",
    "    print(f\"âœ… Ã‡alÄ±ÅŸma dizini: {os.getcwd()}\")\n",
    "\n",
    "    # Gerekli paketleri yÃ¼kle\n",
    "    print(\"\\nğŸ“¦ Paketler yÃ¼kleniyor...\")\n",
    "    !pip install -q torch transformers datasets scikit-learn pandas tqdm matplotlib seaborn\n",
    "    print(\"âœ… TÃ¼m paketler yÃ¼klendi\")\n",
    "else:\n",
    "    print(\"â„¹ï¸  Yerel ortamda Ã§alÄ±ÅŸÄ±yorsunuz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3bcf2e",
   "metadata": {},
   "source": [
    "## 2. Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6e47aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from src.data_loader import prepare_data\n",
    "from src.model import load_model\n",
    "from src.train import train_model\n",
    "from src.evaluate import evaluate_model\n",
    "from src.strategies import get_strategy_config\n",
    "\n",
    "print(\"âœ… ModÃ¼ller yÃ¼klendi\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f537ab36",
   "metadata": {},
   "source": [
    "## 3. Configuration - Strategy Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238664c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base configuration\n",
    "BASE_CONFIG = {\n",
    "    'model_name': 'bert-base-uncased',\n",
    "    'max_length': 512,\n",
    "    'batch_size': 16,\n",
    "    'learning_rate': 2e-5,\n",
    "    'num_epochs': 3,  # Her strateji iÃ§in 3 epoch\n",
    "    'warmup_steps': 500,\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    'output_dir': './outputs_comparison'\n",
    "}\n",
    "\n",
    "# ğŸ¯ HANGÄ° STRATEJÄ°LERÄ° TEST ETMEK Ä°STÄ°YORSUNUZ?\n",
    "# True = Ã‡alÄ±ÅŸtÄ±r, False = Atla\n",
    "STRATEGIES_TO_RUN = {\n",
    "    'baseline': True,        # Referans iÃ§in mutlaka Ã§alÄ±ÅŸtÄ±rÄ±n\n",
    "    'focal_weak': True,      # Standart Focal Loss\n",
    "    'focal_strong': True,    # Agresif Focal Loss\n",
    "    'weighted': True,        # Class Weights (en umut verici)\n",
    "    'oversampled': False,    # Oversampling (yavaÅŸ olabilir)\n",
    "    'subset_50': False,      # Top-50 etiket (hÄ±zlÄ± test iÃ§in)\n",
    "    'subset_100': True,      # Top-100 etiket (dengeli)\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ”¬ COMPARISON PIPELINE CONFIGURATION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Model: {BASE_CONFIG['model_name']}\")\n",
    "print(f\"Epochs: {BASE_CONFIG['num_epochs']}\")\n",
    "print(f\"Device: {BASE_CONFIG['device']}\")\n",
    "print(f\"\\nAktif stratejiler ({sum(STRATEGIES_TO_RUN.values())} adet):\")\n",
    "for strategy, enabled in STRATEGIES_TO_RUN.items():\n",
    "    status = \"âœ…\" if enabled else \"â­ï¸ \"\n",
    "    print(f\"  {status} {strategy}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664888da",
   "metadata": {},
   "source": [
    "## 4. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6b8f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 1: DATA PREPARATION\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "data = prepare_data(\n",
    "    model_name=BASE_CONFIG['model_name'],\n",
    "    max_length=BASE_CONFIG['max_length']\n",
    ")\n",
    "\n",
    "train_dataset = data['train_dataset']\n",
    "test_dataset = data['test_dataset']\n",
    "label_list = data['label_list']\n",
    "num_labels = data['num_labels']\n",
    "\n",
    "print(f\"\\nâœ… Veri hazÄ±rlama tamamlandÄ±!\")\n",
    "print(f\"   Train samples: {len(train_dataset)}\")\n",
    "print(f\"   Test samples: {len(test_dataset)}\")\n",
    "print(f\"   Number of labels: {num_labels}\")\n",
    "\n",
    "# Class imbalance analizi\n",
    "all_train_labels = torch.stack([train_dataset[i]['labels'] for i in range(len(train_dataset))])\n",
    "positive_ratio = all_train_labels.float().mean().item()\n",
    "print(f\"\\nâš ï¸  Class Imbalance:\")\n",
    "print(f\"   Positive label ratio: {positive_ratio:.4f} ({positive_ratio*100:.2f}%)\")\n",
    "print(f\"   Imbalance ratio: 1:{int(1/positive_ratio)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690bc7a2",
   "metadata": {},
   "source": [
    "## 5. Training Pipeline - Run All Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c648c6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ”¬ STARTING COMPREHENSIVE COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Results storage\n",
    "all_results = []\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Create main output directory\n",
    "main_output_dir = Path(BASE_CONFIG['output_dir']) / f\"comparison_{timestamp}\"\n",
    "main_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"\\nğŸ“ Main output directory: {main_output_dir}\\n\")\n",
    "\n",
    "# Iterate through strategies\n",
    "for strategy_name, enabled in STRATEGIES_TO_RUN.items():\n",
    "    if not enabled:\n",
    "        print(f\"\\nâ­ï¸  Skipping {strategy_name}\")\n",
    "        continue\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"ğŸ¯ TRAINING STRATEGY: {strategy_name.upper()}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    try:\n",
    "        # Get strategy configuration\n",
    "        strategy_config = get_strategy_config(\n",
    "            strategy_name, \n",
    "            train_dataset, \n",
    "            num_labels, \n",
    "            label_list\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nğŸ“‹ Strategy: {strategy_config['name']}\")\n",
    "        print(f\"   Description: {strategy_config['description']}\")\n",
    "        print(f\"   Num labels: {strategy_config['num_labels']}\")\n",
    "        \n",
    "        # Create strategy output directory\n",
    "        strategy_output_dir = main_output_dir / strategy_name\n",
    "        strategy_output_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Load model with strategy-specific settings\n",
    "        model = load_model(\n",
    "            model_name=BASE_CONFIG['model_name'],\n",
    "            num_labels=strategy_config['num_labels'],\n",
    "            device=BASE_CONFIG['device'],\n",
    "            use_focal_loss=strategy_config.get('use_focal_loss', False),\n",
    "            focal_alpha=strategy_config.get('focal_alpha', 0.25),\n",
    "            focal_gamma=strategy_config.get('focal_gamma', 2.0),\n",
    "            pos_weight=strategy_config.get('pos_weight', None)\n",
    "        )\n",
    "        \n",
    "        # Train model\n",
    "        print(f\"\\nğŸš€ Starting training...\")\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        # Use custom dataloader if provided\n",
    "        if strategy_config.get('custom_dataloader') is not None:\n",
    "            # Custom training loop would go here\n",
    "            # For now, use standard training\n",
    "            pass\n",
    "        \n",
    "        history = train_model(\n",
    "            model=model,\n",
    "            train_dataset=strategy_config['dataset'],\n",
    "            test_dataset=test_dataset,\n",
    "            output_dir=str(strategy_output_dir),\n",
    "            batch_size=BASE_CONFIG['batch_size'],\n",
    "            learning_rate=BASE_CONFIG['learning_rate'],\n",
    "            num_epochs=BASE_CONFIG['num_epochs'],\n",
    "            warmup_steps=BASE_CONFIG['warmup_steps'],\n",
    "            device=BASE_CONFIG['device']\n",
    "        )\n",
    "        \n",
    "        training_time = (datetime.now() - start_time).total_seconds() / 60\n",
    "        print(f\"\\nâ±ï¸  Training completed in {training_time:.1f} minutes\")\n",
    "        \n",
    "        # Evaluate model\n",
    "        print(f\"\\nğŸ“Š Evaluating...\")\n",
    "        \n",
    "        # Test different thresholds\n",
    "        best_f1 = 0\n",
    "        best_threshold = 0.5\n",
    "        best_metrics = None\n",
    "        \n",
    "        for thresh in [0.1, 0.2, 0.3, 0.4, 0.5]:\n",
    "            metrics = evaluate_model(\n",
    "                model=model,\n",
    "                test_dataset=test_dataset,\n",
    "                batch_size=BASE_CONFIG['batch_size'],\n",
    "                device=BASE_CONFIG['device'],\n",
    "                threshold=thresh,\n",
    "                label_list=strategy_config['label_list']\n",
    "            )\n",
    "            \n",
    "            if metrics['micro_f1'] > best_f1:\n",
    "                best_f1 = metrics['micro_f1']\n",
    "                best_threshold = thresh\n",
    "                best_metrics = metrics\n",
    "        \n",
    "        # Top-K evaluation\n",
    "        print(f\"\\nğŸ”¬ Top-K evaluation...\")\n",
    "        model.eval()\n",
    "        topk_results = []\n",
    "        \n",
    "        for k in [1, 3, 5, 10]:\n",
    "            all_predictions = []\n",
    "            all_labels = []\n",
    "            \n",
    "            test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for batch in test_loader:\n",
    "                    outputs = model(\n",
    "                        input_ids=batch['input_ids'].to(BASE_CONFIG['device']),\n",
    "                        attention_mask=batch['attention_mask'].to(BASE_CONFIG['device'])\n",
    "                    )\n",
    "                    probs = torch.sigmoid(outputs['logits'])\n",
    "                    \n",
    "                    batch_preds = torch.zeros_like(probs)\n",
    "                    topk_values, topk_indices = torch.topk(probs, k=min(k, probs.size(1)), dim=1)\n",
    "                    batch_preds.scatter_(1, topk_indices, 1.0)\n",
    "                    \n",
    "                    all_predictions.append(batch_preds.cpu().numpy())\n",
    "                    all_labels.append(batch['labels'].cpu().numpy())\n",
    "            \n",
    "            predictions = np.vstack(all_predictions)\n",
    "            labels = np.vstack(all_labels)\n",
    "            \n",
    "            from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "            \n",
    "            topk_f1 = f1_score(labels, predictions, average='micro', zero_division=0)\n",
    "            topk_results.append({'k': k, 'micro_f1': topk_f1})\n",
    "        \n",
    "        best_topk = max(topk_results, key=lambda x: x['micro_f1'])\n",
    "        \n",
    "        # Store results\n",
    "        result = {\n",
    "            'strategy': strategy_name,\n",
    "            'strategy_name': strategy_config['name'],\n",
    "            'description': strategy_config['description'],\n",
    "            'num_labels': strategy_config['num_labels'],\n",
    "            'training_time_min': training_time,\n",
    "            'final_loss': history['train_loss'][-1],\n",
    "            'best_threshold': best_threshold,\n",
    "            'threshold_micro_f1': best_metrics['micro_f1'],\n",
    "            'threshold_macro_f1': best_metrics['macro_f1'],\n",
    "            'threshold_samples_f1': best_metrics['samples_f1'],\n",
    "            'best_topk_k': best_topk['k'],\n",
    "            'topk_micro_f1': best_topk['micro_f1'],\n",
    "        }\n",
    "        \n",
    "        all_results.append(result)\n",
    "        \n",
    "        # Save strategy results\n",
    "        with open(strategy_output_dir / 'results.json', 'w') as f:\n",
    "            json.dump(result, f, indent=2)\n",
    "        \n",
    "        with open(strategy_output_dir / 'training_history.json', 'w') as f:\n",
    "            json.dump(history, f, indent=2)\n",
    "        \n",
    "        print(f\"\\nâœ… {strategy_name} completed!\")\n",
    "        print(f\"   Best Threshold F1: {result['threshold_micro_f1']:.4f} @ {result['best_threshold']}\")\n",
    "        print(f\"   Best Top-K F1: {result['topk_micro_f1']:.4f} @ K={result['best_topk_k']}\")\n",
    "        \n",
    "        # Clean up model to free memory\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ Error in {strategy_name}: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        continue\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… ALL STRATEGIES COMPLETED!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ad5714",
   "metadata": {},
   "source": [
    "## 6. Results Comparison & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7a7e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ“Š COMPREHENSIVE RESULTS COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create results DataFrame\n",
    "df_results = pd.DataFrame(all_results)\n",
    "\n",
    "# Display results table\n",
    "print(\"\\nğŸ“‹ RESULTS TABLE:\")\n",
    "print(\"=\"*80)\n",
    "display_cols = ['strategy_name', 'num_labels', 'training_time_min', \n",
    "                'threshold_micro_f1', 'topk_micro_f1', 'best_topk_k']\n",
    "print(df_results[display_cols].to_string(index=False))\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Find best strategies\n",
    "best_threshold = df_results.loc[df_results['threshold_micro_f1'].idxmax()]\n",
    "best_topk = df_results.loc[df_results['topk_micro_f1'].idxmax()]\n",
    "\n",
    "print(f\"\\nğŸ† BEST STRATEGIES:\")\n",
    "print(f\"\\nThreshold-based:\")\n",
    "print(f\"  Strategy: {best_threshold['strategy_name']}\")\n",
    "print(f\"  Micro F1: {best_threshold['threshold_micro_f1']:.4f}\")\n",
    "print(f\"  Threshold: {best_threshold['best_threshold']}\")\n",
    "\n",
    "print(f\"\\nTop-K based:\")\n",
    "print(f\"  Strategy: {best_topk['strategy_name']}\")\n",
    "print(f\"  Micro F1: {best_topk['topk_micro_f1']:.4f}\")\n",
    "print(f\"  K: {best_topk['best_topk_k']}\")\n",
    "\n",
    "# Visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. F1 Score Comparison\n",
    "ax = axes[0, 0]\n",
    "x = np.arange(len(df_results))\n",
    "width = 0.35\n",
    "ax.bar(x - width/2, df_results['threshold_micro_f1'], width, label='Threshold-based', alpha=0.8)\n",
    "ax.bar(x + width/2, df_results['topk_micro_f1'], width, label='Top-K based', alpha=0.8)\n",
    "ax.set_xlabel('Strategy')\n",
    "ax.set_ylabel('Micro F1 Score')\n",
    "ax.set_title('F1 Score Comparison Across Strategies')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(df_results['strategy'], rotation=45, ha='right')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Training Time\n",
    "ax = axes[0, 1]\n",
    "ax.bar(df_results['strategy'], df_results['training_time_min'], alpha=0.8, color='coral')\n",
    "ax.set_xlabel('Strategy')\n",
    "ax.set_ylabel('Training Time (minutes)')\n",
    "ax.set_title('Training Time Comparison')\n",
    "ax.set_xticklabels(df_results['strategy'], rotation=45, ha='right')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. F1 vs Training Time (Efficiency)\n",
    "ax = axes[1, 0]\n",
    "scatter = ax.scatter(df_results['training_time_min'], df_results['topk_micro_f1'], \n",
    "                     s=200, alpha=0.6, c=range(len(df_results)), cmap='viridis')\n",
    "for idx, row in df_results.iterrows():\n",
    "    ax.annotate(row['strategy'], \n",
    "                (row['training_time_min'], row['topk_micro_f1']),\n",
    "                xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "ax.set_xlabel('Training Time (minutes)')\n",
    "ax.set_ylabel('Top-K Micro F1')\n",
    "ax.set_title('Efficiency: F1 Score vs Training Time')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Macro F1 Comparison (for rare labels)\n",
    "ax = axes[1, 1]\n",
    "ax.bar(df_results['strategy'], df_results['threshold_macro_f1'], alpha=0.8, color='lightgreen')\n",
    "ax.set_xlabel('Strategy')\n",
    "ax.set_ylabel('Macro F1 Score')\n",
    "ax.set_title('Macro F1 (Rare Label Performance)')\n",
    "ax.set_xticklabels(df_results['strategy'], rotation=45, ha='right')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(main_output_dir / 'comparison_results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nğŸ“Š Visualization saved: comparison_results.png\")\n",
    "\n",
    "# Save results CSV\n",
    "df_results.to_csv(main_output_dir / 'comparison_results.csv', index=False)\n",
    "print(f\"ğŸ“Š Results saved: comparison_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e2e6ef",
   "metadata": {},
   "source": [
    "## 7. Detailed Analysis & Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02ea6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ” DETAILED ANALYSIS & RECOMMENDATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate improvements over baseline\n",
    "if 'baseline' in df_results['strategy'].values:\n",
    "    baseline_f1 = df_results[df_results['strategy'] == 'baseline']['topk_micro_f1'].values[0]\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Improvement over baseline (F1={baseline_f1:.4f}):\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for idx, row in df_results.iterrows():\n",
    "        if row['strategy'] == 'baseline':\n",
    "            continue\n",
    "        \n",
    "        improvement = ((row['topk_micro_f1'] - baseline_f1) / baseline_f1) * 100\n",
    "        symbol = \"ğŸ“ˆ\" if improvement > 0 else \"ğŸ“‰\"\n",
    "        \n",
    "        print(f\"{symbol} {row['strategy_name']:30s}: {row['topk_micro_f1']:.4f} ({improvement:+.1f}%)\")\n",
    "\n",
    "# Recommendations\n",
    "print(f\"\\n\\nğŸ’¡ RECOMMENDATIONS:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Best overall\n",
    "best_overall = df_results.loc[df_results['topk_micro_f1'].idxmax()]\n",
    "print(f\"\\n1ï¸âƒ£ EN Ä°YÄ° GENEL PERFORMANS:\")\n",
    "print(f\"   âœ… {best_overall['strategy_name']}\")\n",
    "print(f\"   F1 Score: {best_overall['topk_micro_f1']:.4f}\")\n",
    "print(f\"   Training Time: {best_overall['training_time_min']:.1f} min\")\n",
    "\n",
    "# Best efficiency (F1 per minute)\n",
    "df_results['efficiency'] = df_results['topk_micro_f1'] / df_results['training_time_min']\n",
    "best_efficient = df_results.loc[df_results['efficiency'].idxmax()]\n",
    "print(f\"\\n2ï¸âƒ£ EN VERÄ°MLÄ° YAKLAÅIM:\")\n",
    "print(f\"   âš¡ {best_efficient['strategy_name']}\")\n",
    "print(f\"   F1/min: {best_efficient['efficiency']:.6f}\")\n",
    "\n",
    "# Best for rare labels\n",
    "best_rare = df_results.loc[df_results['threshold_macro_f1'].idxmax()]\n",
    "print(f\"\\n3ï¸âƒ£ NADÄ°R ETÄ°KETLER Ä°Ã‡Ä°N EN Ä°YÄ°:\")\n",
    "print(f\"   ğŸ¯ {best_rare['strategy_name']}\")\n",
    "print(f\"   Macro F1: {best_rare['threshold_macro_f1']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c136fad",
   "metadata": {},
   "source": [
    "## 8. Download Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6162bbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    import shutil\n",
    "    from google.colab import files\n",
    "\n",
    "    # SonuÃ§larÄ± ZIP'le\n",
    "    zip_name = f\"comparison_results_{timestamp}\"\n",
    "    shutil.make_archive(zip_name, 'zip', main_output_dir)\n",
    "\n",
    "    print(f\"\\nğŸ“¦ SonuÃ§lar sÄ±kÄ±ÅŸtÄ±rÄ±lÄ±yor: {zip_name}.zip\")\n",
    "    print(f\"   Boyut: {os.path.getsize(zip_name + '.zip') / (1024*1024):.2f} MB\")\n",
    "\n",
    "    # Ä°ndir\n",
    "    print(\"\\nâ¬‡ï¸  Ä°ndirme baÅŸlatÄ±lÄ±yor...\")\n",
    "    files.download(zip_name + '.zip')\n",
    "    print(\"âœ… Ä°ndirme tamamlandÄ±!\")\n",
    "else:\n",
    "    print(f\"â„¹ï¸  Yerel ortamdasÄ±nÄ±z, sonuÃ§lar: {main_output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d76ba0",
   "metadata": {},
   "source": [
    "## ğŸ“‹ Summary\n",
    "\n",
    "Bu notebook ile:\n",
    "\n",
    "1. âœ… 7 farklÄ± class imbalance stratejisi test edildi\n",
    "2. âœ… Her strateji iÃ§in F1, Precision, Recall hesaplandÄ±\n",
    "3. âœ… Training time ve efficiency karÅŸÄ±laÅŸtÄ±rÄ±ldÄ±\n",
    "4. âœ… Threshold ve Top-K yaklaÅŸÄ±mlarÄ± denendi\n",
    "5. âœ… GÃ¶rselleÅŸtirmeler oluÅŸturuldu\n",
    "6. âœ… En iyi stratejiler belirlendi\n",
    "\n",
    "**SonuÃ§lar:** `outputs_comparison/comparison_YYYYMMDD_HHMMSS/`\n",
    "\n",
    "**Ã–neriler:** YukarÄ±daki \"Detailed Analysis\" bÃ¶lÃ¼mÃ¼ne bakÄ±n."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
