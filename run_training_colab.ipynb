{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "bb270d6a",
      "metadata": {
        "id": "bb270d6a"
      },
      "source": [
        "## 1. Setup - Colab Kontrol√º ve Proje Kurulumu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89af471e",
      "metadata": {
        "id": "89af471e"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "# Colab kontrol√º\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    print(\"‚úÖ Google Colab ortamƒ± tespit edildi\")\n",
        "\n",
        "    # GPU kontrol√º\n",
        "    import torch\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"‚úÖ GPU: {torch.cuda.get_device_name(0)}\")\n",
        "        print(f\"   VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è  GPU bulunamadƒ±! Runtime > Change runtime type > GPU se√ßin\")\n",
        "\n",
        "    # GitHub'dan projeyi klonla\n",
        "    print(\"\\nüì• Proje indiriliyor...\")\n",
        "    !git clone https://github.com/Aliekinozcetin/Mitre_Attack_TTP_Mapping.git\n",
        "\n",
        "    # Proje dizinine ge√ß\n",
        "    os.chdir('Mitre_Attack_TTP_Mapping')\n",
        "    print(f\"‚úÖ √áalƒ±≈üma dizini: {os.getcwd()}\")\n",
        "\n",
        "    # Gerekli paketleri y√ºkle (sadece temel ML paketleri, Jupyter paketleri hari√ß)\n",
        "    print(\"\\nüì¶ Paketler y√ºkleniyor...\")\n",
        "    !pip install -q torch transformers datasets scikit-learn pandas tqdm wandb matplotlib seaborn\n",
        "    print(\"‚úÖ T√ºm paketler y√ºklendi\")\n",
        "else:\n",
        "    print(\"‚ÑπÔ∏è  Yerel ortamda √ßalƒ±≈üƒ±yorsunuz\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e6ca280",
      "metadata": {
        "id": "9e6ca280"
      },
      "source": [
        "## 2. Import Mod√ºller"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9cf6a18",
      "metadata": {
        "id": "d9cf6a18"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "from src.data_loader import prepare_data\n",
        "from src.model import load_model\n",
        "from src.train import train_model\n",
        "from src.evaluate import evaluate_model\n",
        "\n",
        "print(\"‚úÖ Mod√ºller y√ºklendi\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f6279ff",
      "metadata": {
        "id": "2f6279ff"
      },
      "source": [
        "## 3. Konfig√ºrasyon"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4943e1b9",
      "metadata": {
        "id": "4943e1b9"
      },
      "outputs": [],
      "source": [
        "# Training parametreleri\n",
        "CONFIG = {\n",
        "    'model_name': 'bert-base-uncased',  # veya 'jackaduma/SecBERT', 'distilbert-base-uncased'\n",
        "    'max_length': 512,\n",
        "    'batch_size': 16,  # GPU varsa 32'ye √ßƒ±karabilirsin\n",
        "    'learning_rate': 2e-5,\n",
        "    'num_epochs': 3,\n",
        "    'warmup_steps': 500,\n",
        "    'threshold': 0.5,\n",
        "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
        "    'output_dir': './outputs'\n",
        "}\n",
        "\n",
        "# Konfig√ºrasyonu yazdƒ±r\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"TRAINING CONFIGURATION\")\n",
        "print(\"=\"*70)\n",
        "for key, value in CONFIG.items():\n",
        "    print(f\"  {key:20s}: {value}\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16a4715c",
      "metadata": {
        "id": "16a4715c"
      },
      "source": [
        "## 4. Veri Y√ºkleme ve Hazƒ±rlama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10ace414",
      "metadata": {
        "id": "10ace414"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STEP 1: DATA PREPARATION\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "data = prepare_data(\n",
        "    model_name=CONFIG['model_name'],\n",
        "    max_length=CONFIG['max_length']\n",
        ")\n",
        "\n",
        "train_dataset = data['train_dataset']\n",
        "test_dataset = data['test_dataset']\n",
        "label_list = data['label_list']\n",
        "num_labels = data['num_labels']\n",
        "\n",
        "print(f\"\\n‚úÖ Veri hazƒ±rlama tamamlandƒ±!\")\n",
        "print(f\"   Train samples: {len(train_dataset)}\")\n",
        "print(f\"   Test samples: {len(test_dataset)}\")\n",
        "print(f\"   Number of labels: {num_labels}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0c9a687",
      "metadata": {
        "id": "f0c9a687"
      },
      "source": [
        "## 5. Model Y√ºkleme"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78f12260",
      "metadata": {
        "id": "78f12260"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STEP 2: MODEL INITIALIZATION\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "model = load_model(\n",
        "    model_name=CONFIG['model_name'],\n",
        "    num_labels=num_labels,\n",
        "    device=CONFIG['device']\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úÖ Model y√ºklendi ve {CONFIG['device']} cihazƒ±na ta≈üƒ±ndƒ±!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d65fec12",
      "metadata": {
        "id": "d65fec12"
      },
      "source": [
        "## 6. Model Eƒüitimi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "730aed8b",
      "metadata": {
        "id": "730aed8b"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STEP 3: MODEL TRAINING\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "# Output dizini olu≈ütur\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "run_name = f\"{CONFIG['model_name'].replace('/', '_')}_{timestamp}\"\n",
        "output_dir = os.path.join(CONFIG['output_dir'], run_name)\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Label listesini kaydet\n",
        "label_file = os.path.join(output_dir, \"labels.json\")\n",
        "with open(label_file, 'w') as f:\n",
        "    json.dump(label_list, f, indent=2)\n",
        "\n",
        "# Eƒüitimi ba≈ülat\n",
        "history = train_model(\n",
        "    model=model,\n",
        "    train_dataset=train_dataset,\n",
        "    test_dataset=test_dataset,\n",
        "    output_dir=output_dir,\n",
        "    batch_size=CONFIG['batch_size'],\n",
        "    learning_rate=CONFIG['learning_rate'],\n",
        "    num_epochs=CONFIG['num_epochs'],\n",
        "    warmup_steps=CONFIG['warmup_steps'],\n",
        "    device=CONFIG['device']\n",
        ")\n",
        "\n",
        "# Training ge√ßmi≈üini kaydet\n",
        "history_file = os.path.join(output_dir, \"training_history.json\")\n",
        "with open(history_file, 'w') as f:\n",
        "    json.dump(history, f, indent=2)\n",
        "\n",
        "print(f\"\\n‚úÖ Eƒüitim tamamlandƒ±!\")\n",
        "print(f\"   Final train loss: {history['train_loss'][-1]:.4f}\")\n",
        "if 'val_loss' in history:\n",
        "    print(f\"   Final val loss: {history['val_loss'][-1]:.4f}\")\n",
        "\n",
        "# Loss grafiƒüi\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(history['train_loss'], label='Train Loss')\n",
        "if 'val_loss' in history:\n",
        "    plt.plot(history['val_loss'], label='Val Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training History')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.savefig(os.path.join(output_dir, 'training_loss.png'))\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nüìä Loss grafiƒüi kaydedildi: training_loss.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e9bbb0e",
      "metadata": {
        "id": "3e9bbb0e"
      },
      "source": [
        "## 7. Model Deƒüerlendirme"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d2ae708",
      "metadata": {
        "id": "6d2ae708"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STEP 4: MODEL EVALUATION\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "# √ñnce model √ßƒ±ktƒ±larƒ±nƒ± kontrol et\n",
        "print(\"üîç Model √ßƒ±ktƒ±larƒ±nƒ± kontrol ediyorum...\")\n",
        "model.eval()\n",
        "sample_batch = next(iter(torch.utils.data.DataLoader(test_dataset, batch_size=16)))\n",
        "with torch.no_grad():\n",
        "    sample_out = model(\n",
        "        input_ids=sample_batch['input_ids'].to(CONFIG['device']),\n",
        "        attention_mask=sample_batch['attention_mask'].to(CONFIG['device'])\n",
        "    )\n",
        "    sample_probs = torch.sigmoid(sample_out['logits'])\n",
        "\n",
        "print(f\"\\nSample sigmoid outputs:\")\n",
        "print(f\"  Min: {sample_probs.min().item():.6f}\")\n",
        "print(f\"  Max: {sample_probs.max().item():.6f}\")\n",
        "print(f\"  Mean: {sample_probs.mean().item():.6f}\")\n",
        "print(f\"  Median: {sample_probs.median().item():.6f}\")\n",
        "\n",
        "# Otomatik threshold belirleme\n",
        "optimal_threshold = float(sample_probs.median().item())\n",
        "print(f\"\\nüí° √ñnerilen threshold: {optimal_threshold:.4f}\")\n",
        "\n",
        "# Farklƒ± threshold deƒüerleri ile deƒüerlendir\n",
        "thresholds_to_test = [0.1, 0.2, 0.3, optimal_threshold, 0.5]\n",
        "print(f\"\\nüìä Farklƒ± threshold deƒüerleri test ediliyor...\")\n",
        "\n",
        "best_f1 = 0\n",
        "best_threshold = 0.5\n",
        "best_metrics = None\n",
        "\n",
        "for thresh in thresholds_to_test:\n",
        "    metrics = evaluate_model(\n",
        "        model=model,\n",
        "        test_dataset=test_dataset,\n",
        "        batch_size=CONFIG['batch_size'],\n",
        "        device=CONFIG['device'],\n",
        "        threshold=thresh,\n",
        "        label_list=label_list\n",
        "    )\n",
        "\n",
        "    if metrics['micro_f1'] > best_f1:\n",
        "        best_f1 = metrics['micro_f1']\n",
        "        best_threshold = thresh\n",
        "        best_metrics = metrics\n",
        "\n",
        "print(f\"\\nüèÜ En iyi threshold: {best_threshold:.4f}\")\n",
        "print(f\"   Micro F1: {best_f1:.4f}\")\n",
        "\n",
        "# En iyi metrikleri kaydet\n",
        "metrics_to_save = {k: float(v) if isinstance(v, (float, int)) else v\n",
        "                   for k, v in best_metrics.items()\n",
        "                   if k not in ['predictions', 'labels']}\n",
        "metrics_to_save['best_threshold'] = best_threshold\n",
        "\n",
        "metrics_file = os.path.join(output_dir, \"evaluation_metrics.json\")\n",
        "with open(metrics_file, 'w') as f:\n",
        "    json.dump(metrics_to_save, f, indent=2)\n",
        "\n",
        "print(f\"\\n‚úÖ Deƒüerlendirme tamamlandƒ±!\")\n",
        "print(f\"\\nFinal Metrics (threshold={best_threshold:.4f}):\")\n",
        "print(f\"  Micro F1:    {best_metrics['micro_f1']:.4f}\")\n",
        "print(f\"  Macro F1:    {best_metrics['macro_f1']:.4f}\")\n",
        "print(f\"  Samples F1:  {best_metrics['samples_f1']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4cc1197",
      "metadata": {
        "id": "c4cc1197"
      },
      "source": [
        "## 8. Sonu√ßlarƒ± Kaydet ve ƒ∞ndir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "226f7494",
      "metadata": {
        "id": "226f7494"
      },
      "outputs": [],
      "source": [
        "# √ñzet dosyasƒ± olu≈ütur\n",
        "summary = {\n",
        "    'model': CONFIG['model_name'],\n",
        "    'timestamp': timestamp,\n",
        "    'configuration': CONFIG,\n",
        "    'data': {\n",
        "        'num_labels': num_labels,\n",
        "        'train_samples': len(train_dataset),\n",
        "        'test_samples': len(test_dataset)\n",
        "    },\n",
        "    'training': {\n",
        "        'final_loss': history['train_loss'][-1]\n",
        "    },\n",
        "    'evaluation': metrics_to_save\n",
        "}\n",
        "\n",
        "summary_file = os.path.join(output_dir, \"summary.json\")\n",
        "with open(summary_file, 'w') as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"PIPELINE COMPLETE!\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nSonu√ßlar kaydedildi: {output_dir}\")\n",
        "print(\"\\nDosyalar:\")\n",
        "print(f\"  - labels.json\")\n",
        "print(f\"  - training_history.json\")\n",
        "print(f\"  - evaluation_metrics.json\")\n",
        "print(f\"  - summary.json\")\n",
        "print(f\"  - final_model.pt\")\n",
        "print(f\"  - checkpoint_epoch_*.pt\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b335ef4f",
      "metadata": {
        "id": "b335ef4f"
      },
      "source": [
        "## 9. Colab'da Sonu√ßlarƒ± ƒ∞ndir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44c90d65",
      "metadata": {
        "id": "44c90d65"
      },
      "outputs": [],
      "source": [
        "if IN_COLAB:\n",
        "    import shutil\n",
        "    from google.colab import files\n",
        "\n",
        "    # Sonu√ßlarƒ± ZIP'le\n",
        "    zip_name = f\"{run_name}.zip\"\n",
        "    shutil.make_archive(run_name, 'zip', output_dir)\n",
        "\n",
        "    print(f\"\\nüì¶ Sonu√ßlar sƒ±kƒ±≈ütƒ±rƒ±lƒ±yor: {zip_name}\")\n",
        "    print(f\"   Boyut: {os.path.getsize(zip_name) / (1024*1024):.2f} MB\")\n",
        "\n",
        "    # ƒ∞ndir\n",
        "    print(\"\\n‚¨áÔ∏è  ƒ∞ndirme ba≈ülatƒ±lƒ±yor...\")\n",
        "    files.download(zip_name)\n",
        "    print(\"‚úÖ ƒ∞ndirme tamamlandƒ±!\")\n",
        "else:\n",
        "    print(\"‚ÑπÔ∏è  Yerel ortamdasƒ±nƒ±z, sonu√ßlar zaten bilgisayarƒ±nƒ±zda.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48e7989e",
      "metadata": {
        "id": "48e7989e"
      },
      "source": [
        "## 10. (Opsiyonel) Farklƒ± Modelleri Dene"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d018e9fb",
      "metadata": {
        "id": "d018e9fb"
      },
      "outputs": [],
      "source": [
        "# SecBERT modelini denemek i√ßin bu h√ºcreyi √ßalƒ±≈ütƒ±r\n",
        "# CONFIG['model_name'] = 'jackaduma/SecBERT'\n",
        "\n",
        "# DistilBERT modelini denemek i√ßin bu h√ºcreyi √ßalƒ±≈ütƒ±r\n",
        "# CONFIG['model_name'] = 'distilbert-base-uncased'\n",
        "# CONFIG['batch_size'] = 32  # DistilBERT daha k√º√ß√ºk, batch size artƒ±rƒ±labilir\n",
        "\n",
        "# Sonra yukarƒ±daki h√ºcreleri tekrar √ßalƒ±≈ütƒ±r"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c09370d",
      "metadata": {},
      "source": [
        "## üî¨ Advanced Evaluation - Top-K Strategy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "531c9ef2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# üî¨ Advanced Evaluation with Top-K strategy\n",
        "print(\"üî¨ Testing Top-K prediction strategy...\\n\")\n",
        "\n",
        "# Calculate average number of true labels per sample\n",
        "true_labels_sum = torch.stack([test_dataset[i]['labels'] for i in range(len(test_dataset))]).sum(dim=1)\n",
        "avg_k = int(true_labels_sum.float().mean().item())\n",
        "print(f\"Average true labels per sample: {avg_k:.1f}\")\n",
        "\n",
        "# Test different K values\n",
        "k_values = [1, 3, 5, avg_k, 10, 15, 20]\n",
        "topk_results = []\n",
        "\n",
        "for k in k_values:\n",
        "    print(f\"\\nTesting Top-K with k={k}...\")\n",
        "    \n",
        "    model.eval()\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "    \n",
        "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            outputs = model(\n",
        "                input_ids=batch['input_ids'].to(CONFIG['device']),\n",
        "                attention_mask=batch['attention_mask'].to(CONFIG['device'])\n",
        "            )\n",
        "            probs = torch.sigmoid(outputs['logits'])\n",
        "            \n",
        "            # Select top-k\n",
        "            batch_preds = torch.zeros_like(probs)\n",
        "            topk_values, topk_indices = torch.topk(probs, k=min(k, probs.size(1)), dim=1)\n",
        "            batch_preds.scatter_(1, topk_indices, 1.0)\n",
        "            \n",
        "            all_predictions.append(batch_preds.cpu().numpy())\n",
        "            all_labels.append(batch['labels'].cpu().numpy())\n",
        "    \n",
        "    predictions = np.vstack(all_predictions)\n",
        "    labels = np.vstack(all_labels)\n",
        "    \n",
        "    # Compute metrics\n",
        "    from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "    \n",
        "    micro_f1 = f1_score(labels, predictions, average='micro', zero_division=0)\n",
        "    macro_f1 = f1_score(labels, predictions, average='macro', zero_division=0)\n",
        "    samples_f1 = f1_score(labels, predictions, average='samples', zero_division=0)\n",
        "    precision = precision_score(labels, predictions, average='micro', zero_division=0)\n",
        "    recall = recall_score(labels, predictions, average='micro', zero_division=0)\n",
        "    \n",
        "    topk_results.append({\n",
        "        'k': k,\n",
        "        'micro_f1': micro_f1,\n",
        "        'macro_f1': macro_f1,\n",
        "        'samples_f1': samples_f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    })\n",
        "    \n",
        "    print(f\"  Micro F1: {micro_f1:.4f} | Precision: {precision:.4f} | Recall: {recall:.4f}\")\n",
        "\n",
        "# Display results table\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"TOP-K STRATEGY RESULTS\")\n",
        "print(f\"{'='*80}\\n\")\n",
        "print(f\"{'K':>5} {'Micro F1':>10} {'Macro F1':>10} {'Samples F1':>12} {'Precision':>11} {'Recall':>10}\")\n",
        "print(\"-\" * 80)\n",
        "for result in topk_results:\n",
        "    print(f\"{result['k']:>5} {result['micro_f1']:>10.4f} {result['macro_f1']:>10.4f} \"\n",
        "          f\"{result['samples_f1']:>12.4f} {result['precision']:>11.4f} {result['recall']:>10.4f}\")\n",
        "\n",
        "# Find best K\n",
        "best_result = max(topk_results, key=lambda x: x['micro_f1'])\n",
        "print(f\"\\nüèÜ Best K: {best_result['k']} with Micro F1: {best_result['micro_f1']:.4f}\")\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Plot F1 scores\n",
        "k_vals = [r['k'] for r in topk_results]\n",
        "axes[0].plot(k_vals, [r['micro_f1'] for r in topk_results], marker='o', label='Micro F1')\n",
        "axes[0].plot(k_vals, [r['macro_f1'] for r in topk_results], marker='s', label='Macro F1')\n",
        "axes[0].plot(k_vals, [r['samples_f1'] for r in topk_results], marker='^', label='Samples F1')\n",
        "axes[0].axvline(x=best_result['k'], color='r', linestyle='--', alpha=0.5, label=f'Best K={best_result[\"k\"]}')\n",
        "axes[0].set_xlabel('K')\n",
        "axes[0].set_ylabel('F1 Score')\n",
        "axes[0].set_title('F1 Scores vs K')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot Precision-Recall\n",
        "axes[1].plot(k_vals, [r['precision'] for r in topk_results], marker='o', label='Precision')\n",
        "axes[1].plot(k_vals, [r['recall'] for r in topk_results], marker='s', label='Recall')\n",
        "axes[1].axvline(x=best_result['k'], color='r', linestyle='--', alpha=0.5, label=f'Best K={best_result[\"k\"]}')\n",
        "axes[1].set_xlabel('K')\n",
        "axes[1].set_ylabel('Score')\n",
        "axes[1].set_title('Precision & Recall vs K')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(output_dir, 'topk_analysis.png'), dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nüìä Top-K analiz grafiƒüi kaydedildi: topk_analysis.png\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
