{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fe9927e",
   "metadata": {},
   "source": [
    "# üß™ CTI - MITRE ATTA&CK TTP MAPPƒ∞NG PROJECT\n",
    "\n",
    "## ü§ñ Model: CTI-BERT (IBM Research)\n",
    "- **ibm-research/CTI-BERT**: Cyber Threat Intelligence verisiyle √∂nceden eƒüitilmi≈ü domain-specific BERT\n",
    "- **Avantaj**: G√ºvenlik ve CTI metinlerini anlamada genel BERT'ten daha iyi\n",
    "- **Reference**: https://huggingface.co/ibm-research/CTI-BERT\n",
    "\n",
    "## üìä Dataset: Single Source\n",
    "- **tumeteor/Security-TTP-Mapping** (14.9k train + 2.6k test)\n",
    "- **√ñzellik**: MITRE ATT&CK technique ID'leri (T-codes)\n",
    "- **Avantaj**: Tutarlƒ± label format, y√ºksek kalite\n",
    "- **Reference**: https://huggingface.co/datasets/tumeteor/Security-TTP-Mapping "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6204197",
   "metadata": {},
   "source": [
    "### üîß SETUP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4016e0e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:cd:1: no such file or directory: /content/Mitre_Attack_TTP_Mapping\n",
      "\n",
      "‚ö° OPTIMIZATION APPLIED:\n",
      "   - ExtraTreesClassifier: 50 trees (was 100)\n",
      "   - RandomForestClassifier: 50 trees (was 100)\n",
      "   - max_features='sqrt' (~28 features per split instead of 768)\n",
      "   - min_samples_split=20 (faster splits)\n",
      "   - Expected speedup: ~4x faster training!\n",
      "\n",
      "   If training was stuck, Runtime > Interrupt execution, then re-run from Strategy cell\n",
      "\n",
      "‚ÑπÔ∏è  Yerel ortamda √ßalƒ±≈üƒ±yorsunuz\n"
     ]
    }
   ],
   "source": [
    "# Update repository to latest version (get optimized tree classifiers)\n",
    "!cd /content/Mitre_Attack_TTP_Mapping && git pull origin main\n",
    "\n",
    "print(\"\\n‚ö° OPTIMIZATION APPLIED:\")\n",
    "print(\"   - ExtraTreesClassifier: 50 trees (was 100)\")\n",
    "print(\"   - RandomForestClassifier: 50 trees (was 100)\")\n",
    "print(\"   - max_features='sqrt' (~28 features per split instead of 768)\")\n",
    "print(\"   - min_samples_split=20 (faster splits)\")\n",
    "print(\"   - Expected speedup: ~4x faster training!\")\n",
    "print(\"\\n   If training was stuck, Runtime > Interrupt execution, then re-run from Strategy cell\\n\")\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"‚úÖ Google Colab ortamƒ± tespit edildi\")\n",
    "    \n",
    "    import torch\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"‚úÖ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"   VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  GPU bulunamadƒ±! Runtime > Change runtime type > GPU se√ßin\")\n",
    "    \n",
    "    print(\"\\nüì• Proje indiriliyor...\")\n",
    "    !rm -rf Mitre_Attack_TTP_Mapping\n",
    "    !git clone https://github.com/Aliekinozcetin/Mitre_Attack_TTP_Mapping.git\n",
    "    os.chdir('Mitre_Attack_TTP_Mapping')\n",
    "    print(f\"‚úÖ √áalƒ±≈üma dizini: {os.getcwd()}\")\n",
    "    \n",
    "    print(\"\\nüì¶ Paketler y√ºkleniyor...\")\n",
    "    !pip install -q torch transformers datasets scikit-learn pandas tqdm matplotlib seaborn\n",
    "    print(\"‚úÖ T√ºm paketler y√ºklendi\")\n",
    "    \n",
    "    # HuggingFace baƒülantƒ± optimizasyonu\n",
    "    print(\"\\nüîß HuggingFace cache ayarlarƒ±...\")\n",
    "    \n",
    "    # Create cache directory\n",
    "    cache_dir = '/content/hf_cache'\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    \n",
    "    # Set environment variables\n",
    "    os.environ['HF_HOME'] = cache_dir\n",
    "    os.environ['TRANSFORMERS_CACHE'] = cache_dir\n",
    "    os.environ['HF_DATASETS_CACHE'] = cache_dir\n",
    "    os.environ['HF_HUB_DOWNLOAD_TIMEOUT'] = '600'  # 10 minutes\n",
    "    os.environ['CURL_CA_BUNDLE'] = ''\n",
    "    os.environ['HF_ENDPOINT'] = 'https://huggingface.co'\n",
    "    os.environ['HF_HUB_ENABLE_HF_TRANSFER'] = '1'  # Faster downloads\n",
    "    \n",
    "    print(f\"‚úÖ Cache dizini olu≈üturuldu: {cache_dir}\")\n",
    "    print(f\"   Timeout: 10 dakika\")\n",
    "    \n",
    "    # Test HuggingFace connection\n",
    "    try:\n",
    "        from huggingface_hub import HfApi\n",
    "        api = HfApi()\n",
    "        print(\"\\nüì° HuggingFace baƒülantƒ± testi...\")\n",
    "        info = api.model_info(\"ibm-research/CTI-BERT\", timeout=30)\n",
    "        print(f\"‚úÖ Model eri≈üilebilir: {info.modelId}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Baƒülantƒ± uyarƒ±sƒ±: {str(e)[:100]}\")\n",
    "        print(\"   Model indirme denemeye devam edilecek...\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è  Yerel ortamda √ßalƒ±≈üƒ±yorsunuz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3044287e",
   "metadata": {},
   "source": [
    "### üì¶ Import Modules & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb4696cb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msrc.strategies\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mmodules:\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mmodules[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msrc.strategies\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mjson\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "# Clear import cache\n",
    "import sys\n",
    "if 'src.data_loader' in sys.modules:\n",
    "    del sys.modules['src.data_loader']\n",
    "if 'src.model' in sys.modules:\n",
    "    del sys.modules['src.model']\n",
    "if 'src.train' in sys.modules:\n",
    "    del sys.modules['src.train']\n",
    "if 'src.evaluate' in sys.modules:\n",
    "    del sys.modules['src.evaluate']\n",
    "if 'src.strategies' in sys.modules:\n",
    "    del sys.modules['src.strategies']\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Import prepare_data first\n",
    "from src.data_loader import prepare_data\n",
    "\n",
    "# CRITICAL FIX: Define the function HERE if import fails\n",
    "try:\n",
    "    from src.data_loader import load_datasets_and_prepare_dataloaders\n",
    "    print(\"‚úÖ Fonksiyon GitHub'dan import edildi\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è  GitHub import ba≈üarƒ±sƒ±z, fonksiyon notebook'ta tanƒ±mlanƒ±yor...\")\n",
    "    \n",
    "    def load_datasets_and_prepare_dataloaders(\n",
    "        model_name: str = \"ibm-research/CTI-BERT\",\n",
    "        batch_size: int = 16,\n",
    "        max_length: int = 512,\n",
    "        use_hybrid: bool = True,\n",
    "        dataset_name: str = \"tumeteor/Security-TTP-Mapping\"\n",
    "    ):\n",
    "        \"\"\"Wrapper for prepare_data - notebook fallback version.\"\"\"\n",
    "        data = prepare_data(\n",
    "            model_name=model_name,\n",
    "            max_length=max_length,\n",
    "            use_hybrid=use_hybrid,\n",
    "            dataset_name=dataset_name\n",
    "        )\n",
    "        \n",
    "        train_loader = DataLoader(\n",
    "            data['train_dataset'],\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True\n",
    "        )\n",
    "        \n",
    "        return (\n",
    "            train_loader,\n",
    "            None,\n",
    "            data['test_dataset'],\n",
    "            data['label_list']\n",
    "        )\n",
    "    \n",
    "    print(\"‚úÖ Fallback fonksiyon tanƒ±mlandƒ± (CTI-BERT)\")\n",
    "\n",
    "from src.model import load_model\n",
    "from src.train import train_model\n",
    "from src.evaluate import evaluate_model\n",
    "from src.strategies import get_strategy_config\n",
    "\n",
    "print(\"‚úÖ Mod√ºller y√ºklendi\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b82a7c4",
   "metadata": {},
   "source": [
    "### üîß CONFIGURATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58aaecde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base training configuration\n",
    "BASE_CONFIG = {\n",
    "    'model_name': 'ibm-research/CTI-BERT',  # CTI domain-specific BERT\n",
    "    'batch_size': 16,\n",
    "    'learning_rate': 2e-5,\n",
    "    'num_epochs': 3,\n",
    "    'max_length': 128,\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "}\n",
    "\n",
    "# Output directory\n",
    "OUTPUT_DIR = Path('outputs')\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Store results from all strategies\n",
    "all_test_results = {}\n",
    "\n",
    "print(\"‚úÖ Konfig√ºrasyon ayarlandƒ±\")\n",
    "print(f\"Model: {BASE_CONFIG['model_name']}\")\n",
    "print(f\"Device: {BASE_CONFIG['device']}\")\n",
    "print(f\"Output: {OUTPUT_DIR.absolute()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c082e5",
   "metadata": {},
   "source": [
    "### üìä DATA LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3310ac99",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üì• Veri y√ºkleniyor...\")\n",
    "print(\"üì¶ Dataset: tumeteor/Security-TTP-Mapping (Single Source)\")\n",
    "print(f\"ü§ñ Model: {BASE_CONFIG['model_name']}\")\n",
    "print(\"\")\n",
    "\n",
    "# Use single dataset: tumeteor only\n",
    "train_dataloader, val_dataloader, test_dataset, label_names = load_datasets_and_prepare_dataloaders(\n",
    "    model_name=BASE_CONFIG['model_name'],\n",
    "    batch_size=BASE_CONFIG['batch_size'],\n",
    "    max_length=BASE_CONFIG['max_length'],\n",
    "    use_hybrid=False,  # Single dataset: tumeteor only\n",
    "    dataset_name=\"tumeteor/Security-TTP-Mapping\"\n",
    ")\n",
    "\n",
    "# Get train_dataset from dataloader for strategies\n",
    "train_dataset = train_dataloader.dataset\n",
    "\n",
    "# Create test dataloader\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BASE_CONFIG['batch_size'],\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "num_labels = len(label_names)\n",
    "\n",
    "# Create data dict for backward compatibility\n",
    "data = {\n",
    "    'train_dataset': train_dataset,\n",
    "    'test_dataset': test_dataset,\n",
    "    'label_list': label_names,\n",
    "    'num_labels': num_labels\n",
    "}\n",
    "\n",
    "print(f\"‚úÖ Veri y√ºklendi\")\n",
    "print(f\"   Train batches: {len(train_dataloader)}\")\n",
    "print(f\"   Test batches: {len(test_dataloader)}\")\n",
    "print(f\"   Toplam label sayƒ±sƒ±: {num_labels}\")\n",
    "print(f\"   ƒ∞lk 5 label: {label_names[:5]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de26a69",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä EXPERIMENT STRUCTURE\n",
    "\n",
    "**Execution Order (√ñnerilen Sƒ±ra):**\n",
    "1. **PART A**: Data Augmentation (AUG-1 ‚Üí AUG-4) - Veri kalitesini artƒ±r\n",
    "2. **PART B**: Loss Function Strategies (STR-1 ‚Üí STR-4) - En iyi augmented data ile test et\n",
    "3. **PART C**: Multi-label Classification Techniques - En iyi kombinasyonu bul"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a7ad4a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîÑ PART A: DATA AUGMENTATION EXPERIMENTS\n",
    "\n",
    "**√ñncelik:** Bu b√∂l√ºm√º PART B ve C'den √ñNCE √ßalƒ±≈ütƒ±rƒ±n!\n",
    "\n",
    "Bu b√∂l√ºm **tail TTP augmentation** stratejilerini test eder.\n",
    "\n",
    "### 3 Augmentation Y√∂ntemi:\n",
    "1. **IoC Replacement** - IP, domain, hash, file path deƒüi≈ütirme (overfitting'i √∂nler)\n",
    "2. **Back-translation** - EN‚ÜíDE‚ÜíEN paraphrasing (semantic variation)\n",
    "3. **Tail Oversampling** - Rare TTP'leri 3x-10x √ßoƒüaltma\n",
    "\n",
    "### 5 Test Stratejisi:\n",
    "- **A-1:** Baseline (No Augmentation)\n",
    "- **A-2:** IoC Replacement Only\n",
    "- **A-3:** Back-translation Only\n",
    "- **A-4:** Oversampling Only\n",
    "- **A-5:** Combined (All 3 methods)\n",
    "\n",
    "### Beklenen ƒ∞yile≈ütirme:\n",
    "- **Tail TTP Recall:** +40-60%\n",
    "- **Overall mAP:** +20-30%\n",
    "- **Micro F1:** +30-50%\n",
    "\n",
    "**Not:** Her augmentation stratejisi baƒüƒ±msƒ±z olarak test edilir, sonra kar≈üƒ±la≈ütƒ±rƒ±lƒ±r."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7821b172",
   "metadata": {},
   "source": [
    "### Augmentation Setup\n",
    "\n",
    "Augmentation mod√ºl√ºn√º import et ve test et."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d977d03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import augmentation module\n",
    "from src.augmentation import replace_iocs, back_translate, augment_tail_samples\n",
    "\n",
    "# Test IoC replacement\n",
    "test_text = \"\"\"\n",
    "The attacker used PowerShell to connect to 192.168.1.10 and downloaded malware from\n",
    "http://malicious.com/payload.exe. The file was saved to C:\\\\Users\\\\Alice\\\\AppData\\\\mal.dll\n",
    "with MD5 hash 5d41402abc4b2a76b9719d911017c592. Registry key HKLM\\\\SOFTWARE\\\\Test was modified.\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üß™ TEST 1: IoC REPLACEMENT\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nüìÑ Original Text:\")\n",
    "print(test_text)\n",
    "print(\"\\nüîÑ IoC Replaced:\")\n",
    "print(replace_iocs(test_text))\n",
    "\n",
    "# Test back-translation (small example for speed)\n",
    "simple_text = \"The attacker used PowerShell to execute malicious commands and escalate privileges.\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üß™ TEST 2: BACK-TRANSLATION\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nüìÑ Original Text:\")\n",
    "print(simple_text)\n",
    "print(\"\\nüîÑ Back-translated (EN‚ÜíDE‚ÜíEN):\")\n",
    "print(back_translate(simple_text, device=BASE_CONFIG['device']))\n",
    "\n",
    "print(\"\\n‚úÖ Augmentation module loaded successfully!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476f4abc",
   "metadata": {},
   "source": [
    "### Strategy AUG-1: Baseline (No Augmentation)\n",
    "\n",
    "**A√ßƒ±klama:** Referans performans i√ßin augmentation olmadan Weighted BCE.\n",
    "\n",
    "**S√ºre:** ~30-40 dakika"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26356171",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "strategy_name = \"aug_baseline\"\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"üß™ AUG-1: Baseline (No Augmentation)\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# Start timing\n",
    "strategy_start_time = time.time()\n",
    "\n",
    "# Use weighted BCE (best performing strategy)\n",
    "strategy_config = get_strategy_config(\n",
    "    strategy_name='weighted',\n",
    "    train_dataset=data['train_dataset'],\n",
    "    num_labels=num_labels,\n",
    "    label_list=label_names\n",
    ")\n",
    "\n",
    "# Create DataLoader\n",
    "strategy_train_dataloader = DataLoader(\n",
    "    strategy_config['dataset'],\n",
    "    batch_size=BASE_CONFIG['batch_size'],\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "print(\"üìã Konfig√ºrasyon:\")\n",
    "print(f\"   Strategy: Weighted BCE (Baseline for comparison)\")\n",
    "print(f\"   Augmentation: NONE\")\n",
    "print(f\"   Num labels: {strategy_config['num_labels']}\")\n",
    "\n",
    "# Load model\n",
    "print(\"\\nüîß Model y√ºkleniyor...\")\n",
    "model = load_model(\n",
    "    model_name=BASE_CONFIG['model_name'],\n",
    "    num_labels=strategy_config['num_labels'],\n",
    "    device=BASE_CONFIG['device'],\n",
    "    use_focal_loss=False,\n",
    "    pos_weight=strategy_config['pos_weight']\n",
    ")\n",
    "\n",
    "# Train model\n",
    "print(\"\\nüöÄ Eƒüitim ba≈ülƒ±yor...\")\n",
    "training_history = train_model(\n",
    "    model=model,\n",
    "    train_dataloader=strategy_train_dataloader,\n",
    "    num_epochs=BASE_CONFIG['num_epochs'],\n",
    "    learning_rate=BASE_CONFIG['learning_rate'],\n",
    "    device=BASE_CONFIG['device']\n",
    ")\n",
    "\n",
    "# Evaluate model\n",
    "print(\"\\nüìä Test seti deƒüerlendiriliyor...\")\n",
    "test_results = evaluate_model(\n",
    "    model=model,\n",
    "    test_dataset=data['test_dataset'],\n",
    "    batch_size=BASE_CONFIG['batch_size'],\n",
    "    device=BASE_CONFIG['device'],\n",
    "    label_names=label_names\n",
    ")\n",
    "\n",
    "# Calculate training time\n",
    "training_time_min = (time.time() - strategy_start_time) / 60\n",
    "\n",
    "# Store results\n",
    "all_test_results[strategy_name] = {\n",
    "    'config': 'Weighted BCE (No Augmentation)',\n",
    "    'description': 'Baseline for augmentation comparison',\n",
    "    'training_time_min': training_time_min,\n",
    "    'results': test_results\n",
    "}\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"‚úÖ AUG-1 TAMAMLANDI: {strategy_name}\")\n",
    "print(f\"‚è±Ô∏è  Training Time: {training_time_min:.2f} minutes\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nüìà Sonu√ßlar:\")\n",
    "for metric, value in test_results.items():\n",
    "    if isinstance(value, (int, float)):\n",
    "        print(f\"   {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca01e99",
   "metadata": {},
   "source": [
    "### Strategy AUG-2: IoC Replacement Only\n",
    "\n",
    "**A√ßƒ±klama:** Sadece IoC replacement (IP, domain, hash, path deƒüi≈ütirme).\n",
    "\n",
    "**Avantaj:** √áok hƒ±zlƒ±, overfitting'i √∂nler.\n",
    "\n",
    "**S√ºre:** ~30-40 dakika"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98db8339",
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy_name = \"aug_ioc_replacement\"\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"üß™ AUG-2: IoC Replacement\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# Start timing\n",
    "strategy_start_time = time.time()\n",
    "\n",
    "# Load raw dataset for text augmentation\n",
    "print(\"üîÑ Loading raw dataset for IoC replacement...\")\n",
    "from datasets import load_dataset\n",
    "import ast\n",
    "\n",
    "raw_dataset = load_dataset(\"tumeteor/Security-TTP-Mapping\")\n",
    "train_df = raw_dataset['train'].to_pandas()\n",
    "\n",
    "# Find text column\n",
    "print(f\"Available columns: {train_df.columns.tolist()}\")\n",
    "possible_text_cols = ['text1', 'description', 'text', 'content', 'sentence']\n",
    "text_column = None\n",
    "for col in possible_text_cols:\n",
    "    if col in train_df.columns:\n",
    "        text_column = col\n",
    "        break\n",
    "\n",
    "if text_column is None:\n",
    "    raise ValueError(f\"No text column found! Available: {train_df.columns.tolist()}\")\n",
    "\n",
    "print(f\"Using text column: {text_column}\")\n",
    "\n",
    "# Apply IoC replacement to texts\n",
    "print(\"Replacing IoCs in training texts...\")\n",
    "augmented_train_texts = []\n",
    "for text in train_df[text_column].fillna('').tolist():\n",
    "    # Original + 2 IoC-replaced versions\n",
    "    augmented_train_texts.append(text)  # Original\n",
    "    augmented_train_texts.append(replace_iocs(text, seed=42))  # Aug 1\n",
    "    augmented_train_texts.append(replace_iocs(text, seed=123))  # Aug 2\n",
    "\n",
    "# Replicate labels accordingly\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import ast\n",
    "\n",
    "# Parse labels\n",
    "train_labels_raw = train_df['labels'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x).tolist()\n",
    "\n",
    "# Expand labels to match augmented texts\n",
    "augmented_train_labels = []\n",
    "for labels in train_labels_raw:\n",
    "    augmented_train_labels.append(labels)  # Original\n",
    "    augmented_train_labels.append(labels)  # Aug 1\n",
    "    augmented_train_labels.append(labels)  # Aug 2\n",
    "\n",
    "print(f\"‚úÖ Original samples: {len(train_df)}\")\n",
    "print(f\"‚úÖ Augmented samples: {len(augmented_train_texts)} (3x augmentation)\")\n",
    "\n",
    "# Prepare augmented data\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Create custom dataset\n",
    "class AugmentedCTIDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Get unique labels\n",
    "        all_labels = set()\n",
    "        for label_list in labels:\n",
    "            all_labels.update(label_list)\n",
    "        self.label_list = sorted(list(all_labels))\n",
    "        self.label_to_idx = {label: idx for idx, label in enumerate(self.label_list)}\n",
    "        \n",
    "        # Tokenize all texts\n",
    "        self.encodings = tokenizer(\n",
    "            texts,\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors=None\n",
    "        )\n",
    "        \n",
    "        # Encode labels\n",
    "        self.encoded_labels = []\n",
    "        for label_list in labels:\n",
    "            encoded = [0] * len(self.label_list)\n",
    "            for label in label_list:\n",
    "                if label in self.label_to_idx:\n",
    "                    encoded[self.label_to_idx[label]] = 1\n",
    "            self.encoded_labels.append(encoded)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.encoded_labels[idx], dtype=torch.float)\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "# Create augmented dataset\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_CONFIG['model_name'])\n",
    "aug_train_dataset = AugmentedCTIDataset(\n",
    "    augmented_train_texts,\n",
    "    augmented_train_labels,\n",
    "    tokenizer,\n",
    "    BASE_CONFIG['max_length']\n",
    ")\n",
    "\n",
    "# Create dataloader\n",
    "aug_train_dataloader = DataLoader(\n",
    "    aug_train_dataset,\n",
    "    batch_size=BASE_CONFIG['batch_size'],\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Augmented dataset created!\")\n",
    "print(f\"   Num labels: {len(aug_train_dataset.label_list)}\")\n",
    "\n",
    "# Get strategy config for weighted BCE\n",
    "strategy_config = get_strategy_config(\n",
    "    strategy_name='weighted',\n",
    "    train_dataset=data['train_dataset'],\n",
    "    num_labels=num_labels,\n",
    "    label_list=label_names\n",
    ")\n",
    "\n",
    "# Load model\n",
    "print(\"\\nüîß Model y√ºkleniyor...\")\n",
    "model = load_model(\n",
    "    model_name=BASE_CONFIG['model_name'],\n",
    "    num_labels=len(aug_train_dataset.label_list),\n",
    "    device=BASE_CONFIG['device'],\n",
    "    use_focal_loss=False,\n",
    "    pos_weight=strategy_config['pos_weight']\n",
    ")\n",
    "\n",
    "# Train model\n",
    "print(\"\\nüöÄ Eƒüitim ba≈ülƒ±yor...\")\n",
    "training_history = train_model(\n",
    "    model=model,\n",
    "    train_dataloader=aug_train_dataloader,\n",
    "    num_epochs=BASE_CONFIG['num_epochs'],\n",
    "    learning_rate=BASE_CONFIG['learning_rate'],\n",
    "    device=BASE_CONFIG['device']\n",
    ")\n",
    "\n",
    "# Evaluate model\n",
    "print(\"\\nüìä Test seti deƒüerlendiriliyor...\")\n",
    "test_results = evaluate_model(\n",
    "    model=model,\n",
    "    test_dataset=data['test_dataset'],\n",
    "    batch_size=BASE_CONFIG['batch_size'],\n",
    "    device=BASE_CONFIG['device'],\n",
    "    label_names=label_names\n",
    ")\n",
    "\n",
    "# Calculate training time\n",
    "training_time_min = (time.time() - strategy_start_time) / 60\n",
    "\n",
    "# Store results\n",
    "all_test_results[strategy_name] = {\n",
    "    'config': 'Weighted BCE + IoC Replacement (3x)',\n",
    "    'description': 'Training data augmented with IoC replacement only',\n",
    "    'training_time_min': training_time_min,\n",
    "    'results': test_results\n",
    "}\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"‚úÖ AUG-2 TAMAMLANDI: {strategy_name}\")\n",
    "print(f\"‚è±Ô∏è  Training Time: {training_time_min:.2f} minutes\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nüìà Sonu√ßlar:\")\n",
    "for metric, value in test_results.items():\n",
    "    if isinstance(value, (int, float)):\n",
    "        print(f\"   {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7e4284",
   "metadata": {},
   "source": [
    "### Strategy AUG-3: Back-translation Only\n",
    "\n",
    "**Augmentation Method:** Back-translation (EN‚ÜíDE‚ÜíEN)\n",
    "- Apply back-translation to 15% of tail TTP samples (frequency < 10)\n",
    "- Expected improvement: +15-30% tail recall, +10-20% mAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4823bf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy_name = \"aug_back_translation\"\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"üß™ AUG-3: Back-translation\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# Start timing\n",
    "strategy_start_time = time.time()\n",
    "\n",
    "# Load raw data\n",
    "from datasets import load_dataset\n",
    "raw_dataset = load_dataset(\"tumeteor/Security-TTP-Mapping\")\n",
    "train_df = raw_dataset['train'].to_pandas()\n",
    "\n",
    "# Parse labels\n",
    "import ast\n",
    "train_labels_raw = train_df['labels'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x).tolist()\n",
    "\n",
    "# Calculate label frequencies\n",
    "from collections import Counter\n",
    "label_counter = Counter()\n",
    "for labels in train_labels_raw:\n",
    "    label_counter.update(labels)\n",
    "\n",
    "# Identify tail TTPs (frequency < 10)\n",
    "tail_threshold = 10\n",
    "tail_ttps = {label for label, count in label_counter.items() if count < tail_threshold}\n",
    "print(f\"üìä Tail TTPs detected: {len(tail_ttps)} (frequency < {tail_threshold})\")\n",
    "\n",
    "# Identify samples with tail TTPs\n",
    "tail_sample_indices = []\n",
    "for idx, labels in enumerate(train_labels_raw):\n",
    "    if any(label in tail_ttps for label in labels):\n",
    "        tail_sample_indices.append(idx)\n",
    "\n",
    "print(f\"üìä Samples with tail TTPs: {len(tail_sample_indices)} / {len(train_df)}\")\n",
    "\n",
    "# Apply back-translation to 15% of tail samples (faster)\n",
    "import random\n",
    "random.seed(42)\n",
    "num_to_augment = int(len(tail_sample_indices) * 0.15)\n",
    "samples_to_augment = random.sample(tail_sample_indices, num_to_augment)\n",
    "\n",
    "print(f\"üîÑ Applying back-translation to {num_to_augment} samples...\")\n",
    "\n",
    "# Create augmented dataset\n",
    "augmented_train_texts = train_df[text_column].fillna('').tolist()\n",
    "augmented_train_labels = train_labels_raw.copy()\n",
    "\n",
    "# Load translation models (lazy loading)\n",
    "back_translate_cached = {}\n",
    "\n",
    "for idx in samples_to_augment:\n",
    "    original_text = train_df.iloc[idx][text_column]\n",
    "    if pd.isna(original_text) or len(original_text.strip()) < 10:\n",
    "        continue\n",
    "    \n",
    "    # Back-translate\n",
    "    try:\n",
    "        bt_text = back_translate(original_text, pivot_lang='de')\n",
    "        if bt_text and bt_text != original_text:\n",
    "            # Add augmented sample\n",
    "            augmented_train_texts.append(bt_text)\n",
    "            augmented_train_labels.append(train_labels_raw[idx])\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Back-translation failed for sample {idx}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"Original samples: {len(train_df)}\")\n",
    "print(f\"Augmented samples: {len(augmented_train_texts)} (+{len(augmented_train_texts) - len(train_df)} from back-translation)\")\n",
    "\n",
    "# Create custom dataset\n",
    "from src.data_loader import prepare_data\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class AugmentedCTIDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Get unique labels\n",
    "        all_labels = set()\n",
    "        for label_list in labels:\n",
    "            all_labels.update(label_list)\n",
    "        self.label_list = sorted(list(all_labels))\n",
    "        self.label_to_idx = {label: idx for idx, label in enumerate(self.label_list)}\n",
    "        \n",
    "        # Tokenize all texts\n",
    "        self.encodings = tokenizer(\n",
    "            texts,\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors=None\n",
    "        )\n",
    "        \n",
    "        # Encode labels\n",
    "        self.encoded_labels = []\n",
    "        for label_list in labels:\n",
    "            encoded = [0] * len(self.label_list)\n",
    "            for label in label_list:\n",
    "                if label in self.label_to_idx:\n",
    "                    encoded[self.label_to_idx[label]] = 1\n",
    "            self.encoded_labels.append(encoded)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.encoded_labels[idx], dtype=torch.float)\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "# Create augmented dataset\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_CONFIG['model_name'])\n",
    "aug_train_dataset = AugmentedCTIDataset(\n",
    "    augmented_train_texts,\n",
    "    augmented_train_labels,\n",
    "    tokenizer,\n",
    "    BASE_CONFIG['max_length']\n",
    ")\n",
    "\n",
    "# Create dataloader\n",
    "aug_train_dataloader = DataLoader(\n",
    "    aug_train_dataset,\n",
    "    batch_size=BASE_CONFIG['batch_size'],\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Augmented dataset created!\")\n",
    "print(f\"   Num labels: {len(aug_train_dataset.label_list)}\")\n",
    "\n",
    "# Get strategy config for weighted BCE\n",
    "strategy_config = get_strategy_config(\n",
    "    strategy_name='weighted',\n",
    "    train_dataset=data['train_dataset'],\n",
    "    num_labels=num_labels,\n",
    "    label_list=label_names\n",
    ")\n",
    "\n",
    "# Load model\n",
    "print(\"\\nüîß Model y√ºkleniyor...\")\n",
    "model = load_model(\n",
    "    model_name=BASE_CONFIG['model_name'],\n",
    "    num_labels=len(aug_train_dataset.label_list),\n",
    "    device=BASE_CONFIG['device'],\n",
    "    use_focal_loss=False,\n",
    "    pos_weight=strategy_config['pos_weight']\n",
    ")\n",
    "\n",
    "# Train model\n",
    "print(\"\\nüöÄ Eƒüitim ba≈ülƒ±yor...\")\n",
    "training_history = train_model(\n",
    "    model=model,\n",
    "    train_dataloader=aug_train_dataloader,\n",
    "    num_epochs=BASE_CONFIG['num_epochs'],\n",
    "    learning_rate=BASE_CONFIG['learning_rate'],\n",
    "    device=BASE_CONFIG['device']\n",
    ")\n",
    "\n",
    "# Evaluate model\n",
    "print(\"\\nüìä Test seti deƒüerlendiriliyor...\")\n",
    "test_results = evaluate_model(\n",
    "    model=model,\n",
    "    test_dataset=data['test_dataset'],\n",
    "    batch_size=BASE_CONFIG['batch_size'],\n",
    "    device=BASE_CONFIG['device'],\n",
    "    label_names=label_names\n",
    ")\n",
    "\n",
    "# Calculate training time\n",
    "training_time_min = (time.time() - strategy_start_time) / 60\n",
    "\n",
    "# Store results\n",
    "all_test_results[strategy_name] = {\n",
    "    'config': 'Weighted BCE + Back-translation (30% tail)',\n",
    "    'description': 'Training data augmented with back-translation for tail TTPs',\n",
    "    'training_time_min': training_time_min,\n",
    "    'results': test_results\n",
    "}\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"‚úÖ AUG-3 TAMAMLANDI: {strategy_name}\")\n",
    "print(f\"‚è±Ô∏è  Training Time: {training_time_min:.2f} minutes\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nüìà Sonu√ßlar:\")\n",
    "for metric, value in test_results.items():\n",
    "    if isinstance(value, (int, float)):\n",
    "        print(f\"   {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee08d69",
   "metadata": {},
   "source": [
    "### Strategy AUG-4: Oversampling Only\n",
    "\n",
    "**A√ßƒ±klama:** Sadece tail TTP'leri 3x-10x √ßoƒüaltma (oversampling).\n",
    "\n",
    "**Avantaj:** En hƒ±zlƒ± augmentation, frequency dengesizliƒüini giderir.\n",
    "\n",
    "**S√ºre:** ~30-40 dakika"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ad16bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy_name = \"aug_oversampling\"\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"üß™ AUG-4: Oversampling Only\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# Start timing\n",
    "strategy_start_time = time.time()\n",
    "\n",
    "\n",
    "# Load raw data\n",
    "from datasets import load_dataset\n",
    "raw_dataset = load_dataset(\"tumeteor/Security-TTP-Mapping\")\n",
    "train_df = raw_dataset['train'].to_pandas()\n",
    "\n",
    "# Find text column\n",
    "possible_text_cols = ['text1', 'description', 'text', 'content', 'sentence']\n",
    "text_column = None\n",
    "for col in possible_text_cols:\n",
    "    if col in train_df.columns:\n",
    "        text_column = col\n",
    "        break\n",
    "\n",
    "if text_column is None:\n",
    "    raise ValueError(f\"No text column found! Available: {train_df.columns.tolist()}\")\n",
    "\n",
    "print(f\"Using text column: {text_column}\")\n",
    "\n",
    "# Parse labels\n",
    "import ast\n",
    "train_labels_raw = train_df['labels'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x).tolist()\n",
    "\n",
    "# Calculate label frequencies\n",
    "from collections import Counter\n",
    "label_counter = Counter()\n",
    "for labels in train_labels_raw:\n",
    "    label_counter.update(labels)\n",
    "\n",
    "# Identify tail TTPs (frequency < 10)\n",
    "tail_threshold = 10\n",
    "tail_ttps = {label for label, count in label_counter.items() if count < tail_threshold}\n",
    "print(f\"üìä Tail TTPs detected: {len(tail_ttps)} (frequency < {tail_threshold})\")\n",
    "\n",
    "# Get training texts\n",
    "train_texts = train_df[text_column].fillna('').tolist()\n",
    "\n",
    "# Apply oversampling only\n",
    "augmented_texts = train_texts.copy()\n",
    "augmented_labels = train_labels_raw.copy()\n",
    "\n",
    "print(f\"üîÑ Applying oversampling to tail TTPs...\")\n",
    "\n",
    "for idx, labels in enumerate(train_labels_raw):\n",
    "    # Check if sample has tail TTPs\n",
    "    if any(label in tail_ttps for label in labels):\n",
    "        # Calculate oversample factor based on min frequency\n",
    "        min_freq = min([label_counter[label] for label in labels if label in tail_ttps])\n",
    "        oversample_factor = max(3, min(10, 100 // min_freq))  # 3x-10x based on frequency\n",
    "        \n",
    "        # Oversample\n",
    "        for _ in range(oversample_factor - 1):  # -1 because original is already in list\n",
    "            augmented_texts.append(train_texts[idx])\n",
    "            augmented_labels.append(labels)\n",
    "\n",
    "print(f\"Original samples: {len(train_texts)}\")\n",
    "print(f\"Augmented samples: {len(augmented_texts)}\")\n",
    "print(f\"Augmentation ratio: {len(augmented_texts) / len(train_texts):.2f}x\")\n",
    "\n",
    "# Create custom dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class AugmentedCTIDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Get unique labels\n",
    "        all_labels = set()\n",
    "        for label_list in labels:\n",
    "            all_labels.update(label_list)\n",
    "        self.label_list = sorted(list(all_labels))\n",
    "        self.label_to_idx = {label: idx for idx, label in enumerate(self.label_list)}\n",
    "        \n",
    "        # Tokenize all texts\n",
    "        self.encodings = tokenizer(\n",
    "            texts,\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors=None\n",
    "        )\n",
    "        \n",
    "        # Encode labels\n",
    "        self.encoded_labels = []\n",
    "        for label_list in labels:\n",
    "            encoded = [0] * len(self.label_list)\n",
    "            for label in label_list:\n",
    "                if label in self.label_to_idx:\n",
    "                    encoded[self.label_to_idx[label]] = 1\n",
    "            self.encoded_labels.append(encoded)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.encoded_labels[idx], dtype=torch.float)\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "# Create augmented dataset\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_CONFIG['model_name'])\n",
    "aug_train_dataset = AugmentedCTIDataset(\n",
    "    augmented_texts,\n",
    "    augmented_labels,\n",
    "    tokenizer,\n",
    "    BASE_CONFIG['max_length']\n",
    ")\n",
    "\n",
    "# Create dataloader\n",
    "aug_train_dataloader = DataLoader(\n",
    "    aug_train_dataset,\n",
    "    batch_size=BASE_CONFIG['batch_size'],\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Augmented dataset created!\")\n",
    "print(f\"   Num labels: {len(aug_train_dataset.label_list)}\")\n",
    "\n",
    "# Get strategy config for weighted BCE\n",
    "strategy_config = get_strategy_config(\n",
    "    strategy_name='weighted',\n",
    "    train_dataset=data['train_dataset'],\n",
    "    num_labels=num_labels,\n",
    "    label_list=label_names\n",
    ")\n",
    "\n",
    "# Load model\n",
    "print(\"\\nüîß Model y√ºkleniyor...\")\n",
    "model = load_model(\n",
    "    model_name=BASE_CONFIG['model_name'],\n",
    "    num_labels=len(aug_train_dataset.label_list),\n",
    "    device=BASE_CONFIG['device'],\n",
    "    use_focal_loss=False,\n",
    "    pos_weight=strategy_config['pos_weight']\n",
    ")\n",
    "\n",
    "# Train model\n",
    "print(\"\\nüöÄ Eƒüitim ba≈ülƒ±yor...\")\n",
    "training_history = train_model(\n",
    "    model=model,\n",
    "    train_dataloader=aug_train_dataloader,\n",
    "    num_epochs=BASE_CONFIG['num_epochs'],\n",
    "    learning_rate=BASE_CONFIG['learning_rate'],\n",
    "    device=BASE_CONFIG['device']\n",
    ")\n",
    "\n",
    "# Evaluate model\n",
    "print(\"\\nüìä Test seti deƒüerlendiriliyor...\")\n",
    "test_results = evaluate_model(\n",
    "    model=model,\n",
    "    test_dataset=data['test_dataset'],\n",
    "    batch_size=BASE_CONFIG['batch_size'],\n",
    "    device=BASE_CONFIG['device']\n",
    ")\n",
    "\n",
    "# Store results\n",
    "# Calculate training time\n",
    "training_time_min = (time.time() - strategy_start_time) / 60\n",
    "\n",
    "all_test_results[strategy_name] = {\n",
    "    'config': 'Weighted BCE + Oversampling Only',\n",
    "    'description': 'Training data augmented with tail TTP oversampling',\n",
    "    'training_time_min': training_time_min,\n",
    "    'results': test_results\n",
    "}\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"‚úÖ AUG-4 TAMAMLANDI: {strategy_name}\")\n",
    "print(\"=\"*60)\n",
    "print(f\"‚è±Ô∏è  Training Time: {training_time_min:.2f} minutes\")\n",
    "print(f\"\\nüìà Sonu√ßlar:\")\n",
    "for metric, value in test_results.items():\n",
    "    if isinstance(value, (int, float)):\n",
    "        print(f\"   {metric}: {value:.4f}\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9351506a",
   "metadata": {},
   "source": [
    "### Strategy AUG-5: Combined (IoC + Back-translation + Oversampling)\n",
    "\n",
    "**Yapƒ±landƒ±rma:**\n",
    "- **Augmentation**: ƒ∞yile≈ütirme Kombinasyonu\n",
    "  - IoC Replacement (100% olasƒ±lƒ±k)\n",
    "  - Back-translation (15% olasƒ±lƒ±k)\n",
    "  - Tail TTP Oversampling (3x-10x)\n",
    "- **Loss Function**: Weighted BCE\n",
    "- **Classification**: CTI-BERT\n",
    "\n",
    "**Beklenen ƒ∞yile≈ütirmeler:**\n",
    "- +40-60% Tail TTP recall\n",
    "- +20-30% mAP (ranking kalitesi)\n",
    "- Genel F1 ve Hamming Loss iyile≈ütirmesi\n",
    "\n",
    "**S√ºre:** ~50-60 dakika"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60c6ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy_name = \"aug_combined\"\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"üß™ AUG-5: Combined Augmentation (All Methods)\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# Start timing\n",
    "strategy_start_time = time.time()\n",
    "\n",
    "\n",
    "# Load raw data\n",
    "from datasets import load_dataset\n",
    "raw_dataset = load_dataset(\"tumeteor/Security-TTP-Mapping\")\n",
    "train_df = raw_dataset['train'].to_pandas()\n",
    "\n",
    "# Find text column\n",
    "possible_text_cols = ['text1', 'description', 'text', 'content', 'sentence']\n",
    "text_column = None\n",
    "for col in possible_text_cols:\n",
    "    if col in train_df.columns:\n",
    "        text_column = col\n",
    "        break\n",
    "\n",
    "if text_column is None:\n",
    "    raise ValueError(f\"No text column found! Available: {train_df.columns.tolist()}\")\n",
    "\n",
    "print(f\"Using text column: {text_column}\")\n",
    "\n",
    "# Parse labels\n",
    "import ast\n",
    "train_labels_raw = train_df['labels'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x).tolist()\n",
    "\n",
    "# Calculate label frequencies\n",
    "from collections import Counter\n",
    "label_counter = Counter()\n",
    "for labels in train_labels_raw:\n",
    "    label_counter.update(labels)\n",
    "\n",
    "# Identify tail TTPs (frequency < 10)\n",
    "tail_threshold = 10\n",
    "tail_ttps = {label for label, count in label_counter.items() if count < tail_threshold}\n",
    "print(f\"üìä Tail TTPs detected: {len(tail_ttps)} (frequency < {tail_threshold})\")\n",
    "\n",
    "# Get training texts\n",
    "train_texts = train_df[text_column].fillna('').tolist()\n",
    "\n",
    "print(f\"üîÑ Applying COMBINED augmentation...\")\n",
    "print(f\"   - IoC Replacement\")\n",
    "print(f\"   - Back-translation (15% probability)\")\n",
    "print(f\"   - Tail Oversampling (3x-10x)\")\n",
    "\n",
    "# Apply combined augmentation\n",
    "augmented_texts = train_texts.copy()\n",
    "augmented_labels = train_labels_raw.copy()\n",
    "\n",
    "for idx, labels in enumerate(train_labels_raw):\n",
    "    # Check if sample has tail TTPs\n",
    "    if any(label in tail_ttps for label in labels):\n",
    "        # Calculate oversample factor based on min frequency\n",
    "        min_freq = min([label_counter[label] for label in labels if label in tail_ttps])\n",
    "        oversample_factor = max(3, min(10, 100 // min_freq))  # 3x-10x based on frequency\n",
    "        \n",
    "        # Oversample with augmentation\n",
    "        for _ in range(oversample_factor - 1):  # -1 because original is already in list\n",
    "            augmented_text = train_texts[idx]\n",
    "            \n",
    "            # Apply IoC replacement\n",
    "            augmented_text = replace_iocs(augmented_text)\n",
    "            \n",
    "            # Apply back-translation with 15% probability\n",
    "            import random\n",
    "            if random.random() < 0.15:\n",
    "                augmented_text = back_translate(augmented_text, device=BASE_CONFIG['device'])\n",
    "            \n",
    "            augmented_texts.append(augmented_text)\n",
    "            augmented_labels.append(labels)\n",
    "\n",
    "print(f\"\\nOriginal samples: {len(train_texts)}\")\n",
    "print(f\"Augmented samples: {len(augmented_texts)}\")\n",
    "print(f\"Augmentation ratio: {len(augmented_texts) / len(train_texts):.2f}x\")\n",
    "\n",
    "# Create custom dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class AugmentedCTIDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Get unique labels\n",
    "        all_labels = set()\n",
    "        for label_list in labels:\n",
    "            all_labels.update(label_list)\n",
    "        self.label_list = sorted(list(all_labels))\n",
    "        self.label_to_idx = {label: idx for idx, label in enumerate(self.label_list)}\n",
    "        \n",
    "        # Tokenize all texts\n",
    "        self.encodings = tokenizer(\n",
    "            texts,\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors=None\n",
    "        )\n",
    "        \n",
    "        # Encode labels\n",
    "        self.encoded_labels = []\n",
    "        for label_list in labels:\n",
    "            encoded = [0] * len(self.label_list)\n",
    "            for label in label_list:\n",
    "                if label in self.label_to_idx:\n",
    "                    encoded[self.label_to_idx[label]] = 1\n",
    "            self.encoded_labels.append(encoded)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.encoded_labels[idx], dtype=torch.float)\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "# Create augmented dataset\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_CONFIG['model_name'])\n",
    "aug_train_dataset = AugmentedCTIDataset(\n",
    "    augmented_texts,\n",
    "    augmented_labels,\n",
    "    tokenizer,\n",
    "    BASE_CONFIG['max_length']\n",
    ")\n",
    "\n",
    "# Create dataloader\n",
    "aug_train_dataloader = DataLoader(\n",
    "    aug_train_dataset,\n",
    "    batch_size=BASE_CONFIG['batch_size'],\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Augmented dataset created!\")\n",
    "print(f\"   Num labels: {len(aug_train_dataset.label_list)}\")\n",
    "\n",
    "# Get strategy config for weighted BCE\n",
    "strategy_config = get_strategy_config(\n",
    "    strategy_name='weighted',\n",
    "    train_dataset=data['train_dataset'],\n",
    "    num_labels=num_labels,\n",
    "    label_list=label_names\n",
    ")\n",
    "\n",
    "# Load model\n",
    "print(\"\\nüîß Model y√ºkleniyor...\")\n",
    "model = load_model(\n",
    "    model_name=BASE_CONFIG['model_name'],\n",
    "    num_labels=len(aug_train_dataset.label_list),\n",
    "    device=BASE_CONFIG['device'],\n",
    "    use_focal_loss=False,\n",
    "    pos_weight=strategy_config['pos_weight']\n",
    ")\n",
    "\n",
    "# Train model\n",
    "print(\"\\nüöÄ Eƒüitim ba≈ülƒ±yor...\")\n",
    "training_history = train_model(\n",
    "    model=model,\n",
    "    train_dataloader=aug_train_dataloader,\n",
    "    num_epochs=BASE_CONFIG['num_epochs'],\n",
    "    learning_rate=BASE_CONFIG['learning_rate'],\n",
    "    device=BASE_CONFIG['device']\n",
    ")\n",
    "\n",
    "# Evaluate model\n",
    "print(\"\\nüìä Test seti deƒüerlendiriliyor...\")\n",
    "test_results = evaluate_model(\n",
    "    model=model,\n",
    "    test_dataset=data['test_dataset'],\n",
    "    batch_size=BASE_CONFIG['batch_size'],\n",
    "    device=BASE_CONFIG['device']\n",
    ")\n",
    "\n",
    "# Store results\n",
    "# Calculate training time\n",
    "training_time_min = (time.time() - strategy_start_time) / 60\n",
    "\n",
    "all_test_results[strategy_name] = {\n",
    "    'config': 'Weighted BCE + Combined Augmentation',\n",
    "    'description': 'Training data augmented with IoC replacement, back-translation, and tail oversampling',\n",
    "    'training_time_min': training_time_min,\n",
    "    'results': test_results\n",
    "}\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"‚úÖ AUG-5 TAMAMLANDI: {strategy_name}\")\n",
    "print(\"=\"*60)\n",
    "print(f\"‚è±Ô∏è  Training Time: {training_time_min:.2f} minutes\")\n",
    "print(f\"\\nüìà Sonu√ßlar:\")\n",
    "for metric, value in test_results.items():\n",
    "    if isinstance(value, (int, float)):\n",
    "        print(f\"   {metric}: {value:.4f}\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e18e9a8",
   "metadata": {},
   "source": [
    "## üìä Augmentation Results Comparison\n",
    "\n",
    "Compare all augmentation strategies and identify the best performer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4899f42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract augmentation results\n",
    "aug_strategies = ['aug_baseline', 'aug_ioc_replacement', 'aug_back_translation', 'aug_oversampling', 'aug_combined']\n",
    "\n",
    "# Create comparison dataframe\n",
    "aug_comparison = []\n",
    "for strategy_name in aug_strategies:\n",
    "    if strategy_name in all_test_results:\n",
    "        result_dict = all_test_results[strategy_name]\n",
    "        row = {\n",
    "            'Strategy': strategy_name,\n",
    "            'Config': result_dict['config'],\n",
    "            'Description': result_dict['description'],\n",
    "            'Training_Time_min': result_dict.get('training_time_min', 0)\n",
    "        }\n",
    "        # Add metrics\n",
    "        for metric, value in result_dict['results'].items():\n",
    "            if isinstance(value, (int, float)):\n",
    "                row[metric] = value\n",
    "        aug_comparison.append(row)\n",
    "\n",
    "aug_df = pd.DataFrame(aug_comparison)\n",
    "\n",
    "# Sort by mAP (descending)\n",
    "if 'mean_average_precision' in aug_df.columns:\n",
    "    aug_df = aug_df.sort_values('mean_average_precision', ascending=False)\n",
    "\n",
    "# Export to CSV (without delta columns)\n",
    "import os\n",
    "csv_path = 'outputs/augmentation_comparison.csv'\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "aug_df.to_csv(csv_path, index=False)\n",
    "print(f\"‚úÖ CSV exported to: {csv_path}\\n\")\n",
    "\n",
    "# Calculate improvements over baseline (for display only, not in CSV)\n",
    "aug_df_display = aug_df.copy()\n",
    "if len(aug_df_display) > 0 and 'aug_baseline' in aug_df_display['Strategy'].values:\n",
    "    baseline_idx = aug_df_display[aug_df_display['Strategy'] == 'aug_baseline'].index[0]\n",
    "    \n",
    "    # Calculate deltas for display\n",
    "    for col in aug_df_display.columns:\n",
    "        if col not in ['Strategy', 'Config', 'Description', 'Training_Time_min'] and aug_df_display[col].dtype in ['float64', 'int64']:\n",
    "            baseline_val = aug_df_display.loc[baseline_idx, col]\n",
    "            aug_df_display[f'{col}_delta'] = ((aug_df_display[col] - baseline_val) / baseline_val * 100).round(2)\n",
    "\n",
    "# Display comparison\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä AUGMENTATION STRATEGIES COMPARISON\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Key metrics to display\n",
    "key_metrics = ['micro_f1', 'mean_average_precision', 'recall_at_5', 'recall_at_10', 'Training_Time_min']\n",
    "\n",
    "for idx, row in aug_df_display.iterrows():\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"üéØ {row['Strategy'].upper()}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Config: {row['Config']}\")\n",
    "    print(f\"Description: {row['Description']}\\n\")\n",
    "    \n",
    "    print(\"Metrics:\")\n",
    "    for metric in key_metrics:\n",
    "        if metric in row:\n",
    "            value = row[metric]\n",
    "            delta_col = f'{metric}_delta'\n",
    "            if delta_col in row and pd.notna(row[delta_col]):\n",
    "                delta = row[delta_col]\n",
    "                delta_str = f\" ({'+' if delta > 0 else ''}{delta:.2f}%)\"\n",
    "                if metric == 'Training_Time_min':\n",
    "                    print(f\"  {metric}: {value:.2f} min\")\n",
    "                else:\n",
    "                    print(f\"  {metric}: {value:.4f}{delta_str}\")\n",
    "            else:\n",
    "                if metric == 'Training_Time_min':\n",
    "                    print(f\"  {metric}: {value:.2f} min\")\n",
    "                else:\n",
    "                    print(f\"  {metric}: {value:.4f}\")\n",
    "\n",
    "# Create visualization (use original aug_df without deltas)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if len(aug_df) > 0:\n",
    "    # Create output directory for plots\n",
    "    plots_dir = 'outputs/augmentation_plots'\n",
    "    os.makedirs(plots_dir, exist_ok=True)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Plot 1: Micro F1\n",
    "    ax = axes[0, 0]\n",
    "    strategies = aug_df['Strategy'].tolist()\n",
    "    micro_f1 = aug_df['micro_f1'].tolist()\n",
    "    bars = ax.bar(range(len(strategies)), micro_f1, color=['gray' if 'baseline' in s else 'skyblue' for s in strategies])\n",
    "    ax.set_xticks(range(len(strategies)))\n",
    "    ax.set_xticklabels(strategies, rotation=45, ha='right')\n",
    "    ax.set_ylabel('Micro F1')\n",
    "    ax.set_title('Micro F1 Comparison')\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Annotate bars\n",
    "    for i, (bar, val) in enumerate(zip(bars, micro_f1)):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005, \n",
    "                f'{val:.4f}', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    # Plot 2: mAP\n",
    "    ax = axes[0, 1]\n",
    "    if 'mean_average_precision' in aug_df.columns:\n",
    "        map_values = aug_df['mean_average_precision'].tolist()\n",
    "        bars = ax.bar(range(len(strategies)), map_values, color=['gray' if 'baseline' in s else 'lightcoral' for s in strategies])\n",
    "        ax.set_xticks(range(len(strategies)))\n",
    "        ax.set_xticklabels(strategies, rotation=45, ha='right')\n",
    "        ax.set_ylabel('mAP')\n",
    "        ax.set_title('Mean Average Precision (mAP) Comparison')\n",
    "        ax.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        for i, (bar, val) in enumerate(zip(bars, map_values)):\n",
    "            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005, \n",
    "                    f'{val:.4f}', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    # Plot 3: Recall@5 and Recall@10\n",
    "    ax = axes[1, 0]\n",
    "    x = np.arange(len(strategies))\n",
    "    width = 0.35\n",
    "    if 'recall_at_5' in aug_df.columns and 'recall_at_10' in aug_df.columns:\n",
    "        r5 = aug_df['recall_at_5'].tolist()\n",
    "        r10 = aug_df['recall_at_10'].tolist()\n",
    "        ax.bar(x - width/2, r5, width, label='Recall@5', color='lightgreen')\n",
    "        ax.bar(x + width/2, r10, width, label='Recall@10', color='darkgreen')\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(strategies, rotation=45, ha='right')\n",
    "        ax.set_ylabel('Recall')\n",
    "        ax.set_title('Recall@K Comparison')\n",
    "        ax.legend()\n",
    "        ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Improvement over baseline (%) - use display dataframe with deltas\n",
    "    ax = axes[1, 1]\n",
    "    if 'micro_f1_delta' in aug_df_display.columns:\n",
    "        # Filter out baseline\n",
    "        aug_df_filtered = aug_df_display[aug_df_display['Strategy'] != 'aug_baseline']\n",
    "        if len(aug_df_filtered) > 0:\n",
    "            strategies_filtered = aug_df_filtered['Strategy'].tolist()\n",
    "            f1_delta = aug_df_filtered['micro_f1_delta'].tolist()\n",
    "            map_delta = aug_df_filtered['mean_average_precision_delta'].tolist() if 'mean_average_precision_delta' in aug_df_filtered.columns else [0] * len(strategies_filtered)\n",
    "            \n",
    "            x = np.arange(len(strategies_filtered))\n",
    "            width = 0.35\n",
    "            ax.bar(x - width/2, f1_delta, width, label='Micro F1 Œî%', color='skyblue')\n",
    "            ax.bar(x + width/2, map_delta, width, label='mAP Œî%', color='lightcoral')\n",
    "            ax.set_xticks(x)\n",
    "            ax.set_xticklabels(strategies_filtered, rotation=45, ha='right')\n",
    "            ax.set_ylabel('Improvement over Baseline (%)')\n",
    "            ax.set_title('Relative Improvement over Baseline')\n",
    "            ax.axhline(y=0, color='black', linestyle='--', linewidth=0.8)\n",
    "            ax.legend()\n",
    "            ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save combined plot\n",
    "    combined_plot_path = f'{plots_dir}/augmentation_comparison_all.png'\n",
    "    plt.savefig(combined_plot_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"‚úÖ Combined plot saved: {combined_plot_path}\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Save individual plots\n",
    "    print(\"\\nüìä Saving individual plots...\")\n",
    "    \n",
    "    # Individual Plot 1: Micro F1\n",
    "    fig1, ax1 = plt.subplots(figsize=(10, 6))\n",
    "    bars = ax1.bar(range(len(strategies)), micro_f1, color=['gray' if 'baseline' in s else 'skyblue' for s in strategies])\n",
    "    ax1.set_xticks(range(len(strategies)))\n",
    "    ax1.set_xticklabels(strategies, rotation=45, ha='right')\n",
    "    ax1.set_ylabel('Micro F1', fontsize=12)\n",
    "    ax1.set_title('Augmentation Strategies: Micro F1 Comparison', fontsize=14, fontweight='bold')\n",
    "    ax1.grid(axis='y', alpha=0.3)\n",
    "    for i, (bar, val) in enumerate(zip(bars, micro_f1)):\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005, \n",
    "                f'{val:.4f}', ha='center', va='bottom', fontsize=10)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{plots_dir}/micro_f1_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"  ‚úì Micro F1 plot saved\")\n",
    "    \n",
    "    # Individual Plot 2: mAP\n",
    "    if 'mean_average_precision' in aug_df.columns:\n",
    "        fig2, ax2 = plt.subplots(figsize=(10, 6))\n",
    "        map_values = aug_df['mean_average_precision'].tolist()\n",
    "        bars = ax2.bar(range(len(strategies)), map_values, color=['gray' if 'baseline' in s else 'lightcoral' for s in strategies])\n",
    "        ax2.set_xticks(range(len(strategies)))\n",
    "        ax2.set_xticklabels(strategies, rotation=45, ha='right')\n",
    "        ax2.set_ylabel('Mean Average Precision (mAP)', fontsize=12)\n",
    "        ax2.set_title('Augmentation Strategies: mAP Comparison', fontsize=14, fontweight='bold')\n",
    "        ax2.grid(axis='y', alpha=0.3)\n",
    "        for i, (bar, val) in enumerate(zip(bars, map_values)):\n",
    "            ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005, \n",
    "                    f'{val:.4f}', ha='center', va='bottom', fontsize=10)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{plots_dir}/map_comparison.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"  ‚úì mAP plot saved\")\n",
    "    \n",
    "    # Individual Plot 3: Recall@K\n",
    "    if 'recall_at_5' in aug_df.columns and 'recall_at_10' in aug_df.columns:\n",
    "        fig3, ax3 = plt.subplots(figsize=(10, 6))\n",
    "        x = np.arange(len(strategies))\n",
    "        width = 0.35\n",
    "        r5 = aug_df['recall_at_5'].tolist()\n",
    "        r10 = aug_df['recall_at_10'].tolist()\n",
    "        ax3.bar(x - width/2, r5, width, label='Recall@5', color='lightgreen')\n",
    "        ax3.bar(x + width/2, r10, width, label='Recall@10', color='darkgreen')\n",
    "        ax3.set_xticks(x)\n",
    "        ax3.set_xticklabels(strategies, rotation=45, ha='right')\n",
    "        ax3.set_ylabel('Recall', fontsize=12)\n",
    "        ax3.set_title('Augmentation Strategies: Recall@K Comparison', fontsize=14, fontweight='bold')\n",
    "        ax3.legend(fontsize=11)\n",
    "        ax3.grid(axis='y', alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{plots_dir}/recall_k_comparison.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"  ‚úì Recall@K plot saved\")\n",
    "    \n",
    "    # Individual Plot 4: Improvement over baseline\n",
    "    if 'micro_f1_delta' in aug_df_display.columns:\n",
    "        aug_df_filtered = aug_df_display[aug_df_display['Strategy'] != 'aug_baseline']\n",
    "        if len(aug_df_filtered) > 0:\n",
    "            fig4, ax4 = plt.subplots(figsize=(10, 6))\n",
    "            strategies_filtered = aug_df_filtered['Strategy'].tolist()\n",
    "            f1_delta = aug_df_filtered['micro_f1_delta'].tolist()\n",
    "            map_delta = aug_df_filtered['mean_average_precision_delta'].tolist() if 'mean_average_precision_delta' in aug_df_filtered.columns else [0] * len(strategies_filtered)\n",
    "            \n",
    "            x = np.arange(len(strategies_filtered))\n",
    "            width = 0.35\n",
    "            ax4.bar(x - width/2, f1_delta, width, label='Micro F1 Œî%', color='skyblue')\n",
    "            ax4.bar(x + width/2, map_delta, width, label='mAP Œî%', color='lightcoral')\n",
    "            ax4.set_xticks(x)\n",
    "            ax4.set_xticklabels(strategies_filtered, rotation=45, ha='right')\n",
    "            ax4.set_ylabel('Improvement over Baseline (%)', fontsize=12)\n",
    "            ax4.set_title('Augmentation Strategies: Relative Improvement', fontsize=14, fontweight='bold')\n",
    "            ax4.axhline(y=0, color='black', linestyle='--', linewidth=0.8)\n",
    "            ax4.legend(fontsize=11)\n",
    "            ax4.grid(axis='y', alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f'{plots_dir}/improvement_over_baseline.png', dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            print(f\"  ‚úì Improvement plot saved\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ All plots saved to: {plots_dir}/\")\n",
    "\n",
    "# Summary recommendation\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üèÜ RECOMMENDATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if len(aug_df) > 1:\n",
    "    # Exclude baseline\n",
    "    aug_df_filtered_rec = aug_df[aug_df['Strategy'] != 'aug_baseline']\n",
    "    if len(aug_df_filtered_rec) > 0:\n",
    "        best_strategy = aug_df_filtered_rec.iloc[0]['Strategy']\n",
    "        best_map = aug_df_filtered_rec.iloc[0]['mean_average_precision'] if 'mean_average_precision' in aug_df_filtered_rec.columns else 0\n",
    "        best_f1 = aug_df_filtered_rec.iloc[0]['micro_f1']\n",
    "        \n",
    "        print(f\"\\n‚úÖ Best performing strategy: {best_strategy.upper()}\")\n",
    "        print(f\"   - mAP: {best_map:.4f}\")\n",
    "        print(f\"   - Micro F1: {best_f1:.4f}\")\n",
    "        \n",
    "        # Calculate deltas for recommendation\n",
    "        if 'aug_baseline' in aug_df['Strategy'].values:\n",
    "            baseline_data = aug_df[aug_df['Strategy'] == 'aug_baseline'].iloc[0]\n",
    "            baseline_map = baseline_data['mean_average_precision'] if 'mean_average_precision' in baseline_data else 0\n",
    "            baseline_f1 = baseline_data['micro_f1']\n",
    "            \n",
    "            map_delta = ((best_map - baseline_map) / baseline_map * 100) if baseline_map > 0 else 0\n",
    "            f1_delta = ((best_f1 - baseline_f1) / baseline_f1 * 100) if baseline_f1 > 0 else 0\n",
    "            \n",
    "            print(f\"\\n   Improvement over baseline:\")\n",
    "            print(f\"   - mAP: +{map_delta:.2f}%\")\n",
    "            print(f\"   - Micro F1: +{f1_delta:.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìÅ EXPORTED FILES:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"  ‚Ä¢ CSV: {csv_path}\")\n",
    "print(f\"  ‚Ä¢ Plots: {plots_dir}/\")\n",
    "print(\"    - augmentation_comparison_all.png (combined)\")\n",
    "print(\"    - micro_f1_comparison.png\")\n",
    "print(\"    - map_comparison.png\")\n",
    "print(\"    - recall_k_comparison.png\")\n",
    "print(\"    - improvement_over_baseline.png\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f52529",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîÑ PART B: LOSS FUNCTION STRATEGIES\n",
    "\n",
    "**Not:** Bu stratejileri PART A'dan sonra, en iyi augmentation y√∂ntemi ile √ßalƒ±≈ütƒ±rƒ±n.\n",
    "\n",
    "Her strateji baƒüƒ±msƒ±z olarak test edilebilir. ƒ∞stediƒüiniz h√ºcreyi √ßalƒ±≈ütƒ±rƒ±n."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990c7ec3",
   "metadata": {},
   "source": [
    "### üîπ Strategy B-1: Baseline (Standard BCE Loss)\n",
    "\n",
    "**A√ßƒ±klama:** Standart Binary Cross-Entropy loss kullanƒ±r. Referans performans i√ßin baseline.\n",
    "\n",
    "**Part B Baseline:** PART A'nƒ±n en iyi augmentation sonucu ile kar≈üƒ±la≈ütƒ±rma yapmak i√ßin √ßalƒ±≈ütƒ±rƒ±n (30-45 dakika)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfc0ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy_name = \"baseline\"\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"üß™ STRATEGY 1: Baseline BCE\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# Start timing\n",
    "strategy_start_time = time.time()\n",
    "\n",
    "\n",
    "# Get strategy configuration\n",
    "strategy_config = get_strategy_config(\n",
    "    strategy_name=strategy_name,\n",
    "    train_dataset=data['train_dataset'],\n",
    "    num_labels=num_labels,\n",
    "    label_list=label_names\n",
    ")\n",
    "\n",
    "# Create DataLoader\n",
    "if strategy_config['custom_dataloader'] is not None:\n",
    "    strategy_train_dataloader = strategy_config['custom_dataloader'](BASE_CONFIG['batch_size'])\n",
    "else:\n",
    "    strategy_train_dataloader = DataLoader(\n",
    "        strategy_config['dataset'],\n",
    "        batch_size=BASE_CONFIG['batch_size'],\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "print(\"üìã Konfig√ºrasyon:\")\n",
    "print(f\"   Strategy: {strategy_config['name']}\")\n",
    "print(f\"   Description: {strategy_config['description']}\")\n",
    "print(f\"   Num labels: {strategy_config['num_labels']}\")\n",
    "print(f\"   Focal loss: {strategy_config['use_focal_loss']}\")\n",
    "if strategy_config['pos_weight'] is not None:\n",
    "    print(f\"   Pos weight: min={strategy_config['pos_weight'].min():.2f}, max={strategy_config['pos_weight'].max():.2f}\")\n",
    "\n",
    "# Load model\n",
    "print(\"\\nüîß Model y√ºkleniyor...\")\n",
    "model = load_model(\n",
    "    model_name=BASE_CONFIG['model_name'],\n",
    "    num_labels=strategy_config['num_labels'],\n",
    "    device=BASE_CONFIG['device'],\n",
    "    use_focal_loss=strategy_config['use_focal_loss'],\n",
    "    focal_alpha=strategy_config.get('focal_alpha', 0.25),\n",
    "    focal_gamma=strategy_config.get('focal_gamma', 2.0),\n",
    "    pos_weight=strategy_config['pos_weight']\n",
    ")\n",
    "\n",
    "# Train model\n",
    "print(\"\\nüöÄ Eƒüitim ba≈ülƒ±yor...\")\n",
    "training_history = train_model(\n",
    "    model=model,\n",
    "    train_dataloader=strategy_train_dataloader,\n",
    "    num_epochs=BASE_CONFIG['num_epochs'],\n",
    "    learning_rate=BASE_CONFIG['learning_rate'],\n",
    "    device=BASE_CONFIG['device']\n",
    ")\n",
    "\n",
    "# Evaluate model\n",
    "print(\"\\nüìä Test seti deƒüerlendiriliyor...\")\n",
    "test_results = evaluate_model(\n",
    "    model=model,\n",
    "    test_dataset=data['test_dataset'],\n",
    "    batch_size=BASE_CONFIG['batch_size'],\n",
    "    device=BASE_CONFIG['device']\n",
    ")\n",
    "\n",
    "# Store results\n",
    "# Calculate training time\n",
    "training_time_min = (time.time() - strategy_start_time) / 60\n",
    "\n",
    "all_test_results[strategy_name] = {\n",
    "    'config': strategy_config['name'],\n",
    "    'description': strategy_config['description'],\n",
    "    'training_time_min': training_time_min,\n",
    "    'results': test_results\n",
    "}\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"‚úÖ STRATEGY 1 TAMAMLANDI: {strategy_name}\")\n",
    "print(\"=\"*60)\n",
    "print(f\"‚è±Ô∏è  Training Time: {training_time_min:.2f} minutes\")\n",
    "print(f\"\\nüìà Sonu√ßlar:\")\n",
    "for metric, value in test_results.items():\n",
    "    if isinstance(value, (int, float)):\n",
    "        print(f\"   {metric}: {value:.4f}\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bd655b",
   "metadata": {},
   "source": [
    "### üîπ Strategy B-2: Weighted BCE Loss\n",
    "\n",
    "**A√ßƒ±klama:** Her label i√ßin frekans bazlƒ± aƒüƒ±rlƒ±k hesaplar (pos_weight=458 for rare labels). Class imbalance i√ßin en etkili y√∂ntem.\n",
    "\n",
    "**√ñnerilen Kullanƒ±m:** Baseline'dan sonra bu stratejiyi test edin. F1 > 0.15 ise, diƒüer stratejileri test etmeye gerek yok."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a054e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy_name = \"weighted\"\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"üß™ STRATEGY 2: Weighted BCE\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# Start timing\n",
    "strategy_start_time = time.time()\n",
    "\n",
    "\n",
    "# Get strategy configuration\n",
    "strategy_config = get_strategy_config(\n",
    "    strategy_name=strategy_name,\n",
    "    train_dataset=data['train_dataset'],\n",
    "    num_labels=num_labels,\n",
    "    label_list=label_names\n",
    ")\n",
    "\n",
    "# Create DataLoader\n",
    "if strategy_config['custom_dataloader'] is not None:\n",
    "    strategy_train_dataloader = strategy_config['custom_dataloader'](BASE_CONFIG['batch_size'])\n",
    "else:\n",
    "    strategy_train_dataloader = DataLoader(\n",
    "        strategy_config['dataset'],\n",
    "        batch_size=BASE_CONFIG['batch_size'],\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "print(\"üìã Konfig√ºrasyon:\")\n",
    "print(f\"   Strategy: {strategy_config['name']}\")\n",
    "print(f\"   Description: {strategy_config['description']}\")\n",
    "print(f\"   Num labels: {strategy_config['num_labels']}\")\n",
    "print(f\"   Focal loss: {strategy_config['use_focal_loss']}\")\n",
    "if strategy_config['pos_weight'] is not None:\n",
    "    print(f\"   Pos weight: min={strategy_config['pos_weight'].min():.2f}, max={strategy_config['pos_weight'].max():.2f}\")\n",
    "\n",
    "# Load model\n",
    "print(\"\\nüîß Model y√ºkleniyor...\")\n",
    "model = load_model(\n",
    "    model_name=BASE_CONFIG['model_name'],\n",
    "    num_labels=strategy_config['num_labels'],\n",
    "    device=BASE_CONFIG['device'],\n",
    "    use_focal_loss=strategy_config['use_focal_loss'],\n",
    "    focal_alpha=strategy_config.get('focal_alpha', 0.25),\n",
    "    focal_gamma=strategy_config.get('focal_gamma', 2.0),\n",
    "    pos_weight=strategy_config['pos_weight']\n",
    ")\n",
    "\n",
    "# Train model\n",
    "print(\"\\nüöÄ Eƒüitim ba≈ülƒ±yor...\")\n",
    "training_history = train_model(\n",
    "    model=model,\n",
    "    train_dataloader=strategy_train_dataloader,\n",
    "    num_epochs=BASE_CONFIG['num_epochs'],\n",
    "    learning_rate=BASE_CONFIG['learning_rate'],\n",
    "    device=BASE_CONFIG['device']\n",
    ")\n",
    "\n",
    "# Evaluate model\n",
    "print(\"\\nüìä Test seti deƒüerlendiriliyor...\")\n",
    "test_results = evaluate_model(\n",
    "    model=model,\n",
    "    test_dataset=data['test_dataset'],\n",
    "    batch_size=BASE_CONFIG['batch_size'],\n",
    "    device=BASE_CONFIG['device']\n",
    ")\n",
    "\n",
    "# Store results\n",
    "# Calculate training time\n",
    "training_time_min = (time.time() - strategy_start_time) / 60\n",
    "\n",
    "all_test_results[strategy_name] = {\n",
    "    'config': strategy_config['name'],\n",
    "    'description': strategy_config['description'],\n",
    "    'training_time_min': training_time_min,\n",
    "    'results': test_results\n",
    "}\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"‚úÖ STRATEGY 2 TAMAMLANDI: {strategy_name}\")\n",
    "print(\"=\"*60)\n",
    "print(f\"‚è±Ô∏è  Training Time: {training_time_min:.2f} minutes\")\n",
    "print(f\"\\nüìà Sonu√ßlar:\")\n",
    "for metric, value in test_results.items():\n",
    "    if isinstance(value, (int, float)):\n",
    "        print(f\"   {metric}: {value:.4f}\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0028aa98",
   "metadata": {},
   "source": [
    "### üîπ Strategy B-3: Focal Loss (Œ≥=2, Œ±=0.25)\n",
    "\n",
    "**A√ßƒ±klama:** Focal Loss with moderate focusing (Œ≥=2). Hard √∂rneklere odaklanƒ±r.\n",
    "\n",
    "**√ñnerilen Kullanƒ±m:** Weighted BCE ba≈üarƒ±sƒ±z olursa deneyin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acce9512",
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy_name = \"focal_weak\"\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"üß™ STRATEGY 3: Focal Loss (Œ≥=2)\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# Start timing\n",
    "strategy_start_time = time.time()\n",
    "\n",
    "\n",
    "# Get strategy configuration\n",
    "strategy_config = get_strategy_config(\n",
    "    strategy_name=strategy_name,\n",
    "    train_dataset=data['train_dataset'],\n",
    "    num_labels=num_labels,\n",
    "    label_list=label_names\n",
    ")\n",
    "\n",
    "# Create DataLoader\n",
    "if strategy_config['custom_dataloader'] is not None:\n",
    "    strategy_train_dataloader = strategy_config['custom_dataloader'](BASE_CONFIG['batch_size'])\n",
    "else:\n",
    "    strategy_train_dataloader = DataLoader(\n",
    "        strategy_config['dataset'],\n",
    "        batch_size=BASE_CONFIG['batch_size'],\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "print(\"üìã Konfig√ºrasyon:\")\n",
    "print(f\"   Strategy: {strategy_config['name']}\")\n",
    "print(f\"   Description: {strategy_config['description']}\")\n",
    "print(f\"   Num labels: {strategy_config['num_labels']}\")\n",
    "print(f\"   Focal loss: {strategy_config['use_focal_loss']}\")\n",
    "if strategy_config['use_focal_loss']:\n",
    "    print(f\"   Focal alpha: {strategy_config.get('focal_alpha')}\")\n",
    "    print(f\"   Focal gamma: {strategy_config.get('focal_gamma')}\")\n",
    "\n",
    "# Load model\n",
    "print(\"\\nüîß Model y√ºkleniyor...\")\n",
    "model = load_model(\n",
    "    model_name=BASE_CONFIG['model_name'],\n",
    "    num_labels=strategy_config['num_labels'],\n",
    "    device=BASE_CONFIG['device'],\n",
    "    use_focal_loss=strategy_config['use_focal_loss'],\n",
    "    focal_alpha=strategy_config.get('focal_alpha', 0.25),\n",
    "    focal_gamma=strategy_config.get('focal_gamma', 2.0),\n",
    "    pos_weight=strategy_config['pos_weight']\n",
    ")\n",
    "\n",
    "# Train model\n",
    "print(\"\\nüöÄ Eƒüitim ba≈ülƒ±yor...\")\n",
    "training_history = train_model(\n",
    "    model=model,\n",
    "    train_dataloader=strategy_train_dataloader,\n",
    "    num_epochs=BASE_CONFIG['num_epochs'],\n",
    "    learning_rate=BASE_CONFIG['learning_rate'],\n",
    "    device=BASE_CONFIG['device']\n",
    ")\n",
    "\n",
    "# Evaluate model\n",
    "print(\"\\nüìä Test seti deƒüerlendiriliyor...\")\n",
    "test_results = evaluate_model(\n",
    "    model=model,\n",
    "    test_dataset=data['test_dataset'],\n",
    "    batch_size=BASE_CONFIG['batch_size'],\n",
    "    device=BASE_CONFIG['device']\n",
    ")\n",
    "\n",
    "# Store results\n",
    "# Calculate training time\n",
    "training_time_min = (time.time() - strategy_start_time) / 60\n",
    "\n",
    "all_test_results[strategy_name] = {\n",
    "    'config': strategy_config['name'],\n",
    "    'description': strategy_config['description'],\n",
    "    'training_time_min': training_time_min,\n",
    "    'results': test_results\n",
    "}\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"‚úÖ STRATEGY 3 TAMAMLANDI: {strategy_name}\")\n",
    "print(\"=\"*60)\n",
    "print(f\"‚è±Ô∏è  Training Time: {training_time_min:.2f} minutes\")\n",
    "print(f\"\\nüìà Sonu√ßlar:\")\n",
    "for metric, value in test_results.items():\n",
    "    if isinstance(value, (int, float)):\n",
    "        print(f\"   {metric}: {value:.4f}\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8b7263",
   "metadata": {},
   "source": [
    "### Strategy 4: Focal Loss (Œ≥=5, Œ±=0.25)\n",
    "\n",
    "**A√ßƒ±klama:** Focal Loss with strong focusing (Œ≥=5). √áok hard √∂rneklere odaklanƒ±r.\n",
    "\n",
    "**√ñnerilen Kullanƒ±m:** Focal Œ≥=2 ba≈üarƒ±lƒ± olursa daha g√º√ßl√º versiyon i√ßin deneyin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135e8330",
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy_name = \"focal_strong\"\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"üß™ STRATEGY 4: Focal Loss (Œ≥=5)\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# Start timing\n",
    "strategy_start_time = time.time()\n",
    "\n",
    "\n",
    "# Get strategy configuration\n",
    "strategy_config = get_strategy_config(\n",
    "    strategy_name=strategy_name,\n",
    "    train_dataset=data['train_dataset'],\n",
    "    num_labels=num_labels,\n",
    "    label_list=label_names\n",
    ")\n",
    "\n",
    "# Create DataLoader\n",
    "if strategy_config['custom_dataloader'] is not None:\n",
    "    strategy_train_dataloader = strategy_config['custom_dataloader'](BASE_CONFIG['batch_size'])\n",
    "else:\n",
    "    strategy_train_dataloader = DataLoader(\n",
    "        strategy_config['dataset'],\n",
    "        batch_size=BASE_CONFIG['batch_size'],\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "print(\"üìã Konfig√ºrasyon:\")\n",
    "print(f\"   Strategy: {strategy_config['name']}\")\n",
    "print(f\"   Description: {strategy_config['description']}\")\n",
    "print(f\"   Num labels: {strategy_config['num_labels']}\")\n",
    "print(f\"   Focal loss: {strategy_config['use_focal_loss']}\")\n",
    "if strategy_config['use_focal_loss']:\n",
    "    print(f\"   Focal alpha: {strategy_config.get('focal_alpha')}\")\n",
    "    print(f\"   Focal gamma: {strategy_config.get('focal_gamma')}\")\n",
    "\n",
    "# Load model\n",
    "print(\"\\nüîß Model y√ºkleniyor...\")\n",
    "model = load_model(\n",
    "    model_name=BASE_CONFIG['model_name'],\n",
    "    num_labels=strategy_config['num_labels'],\n",
    "    device=BASE_CONFIG['device'],\n",
    "    use_focal_loss=strategy_config['use_focal_loss'],\n",
    "    focal_alpha=strategy_config.get('focal_alpha', 0.25),\n",
    "    focal_gamma=strategy_config.get('focal_gamma', 2.0),\n",
    "    pos_weight=strategy_config['pos_weight']\n",
    ")\n",
    "\n",
    "# Train model\n",
    "print(\"\\nüöÄ Eƒüitim ba≈ülƒ±yor...\")\n",
    "training_history = train_model(\n",
    "    model=model,\n",
    "    train_dataloader=strategy_train_dataloader,\n",
    "    num_epochs=BASE_CONFIG['num_epochs'],\n",
    "    learning_rate=BASE_CONFIG['learning_rate'],\n",
    "    device=BASE_CONFIG['device']\n",
    ")\n",
    "\n",
    "# Evaluate model\n",
    "print(\"\\nüìä Test seti deƒüerlendiriliyor...\")\n",
    "test_results = evaluate_model(\n",
    "    model=model,\n",
    "    test_dataset=data['test_dataset'],\n",
    "    batch_size=BASE_CONFIG['batch_size'],\n",
    "    device=BASE_CONFIG['device']\n",
    ")\n",
    "\n",
    "# Store results\n",
    "# Calculate training time\n",
    "training_time_min = (time.time() - strategy_start_time) / 60\n",
    "\n",
    "all_test_results[strategy_name] = {\n",
    "    'config': strategy_config['name'],\n",
    "    'description': strategy_config['description'],\n",
    "    'training_time_min': training_time_min,\n",
    "    'results': test_results\n",
    "}\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"‚úÖ STRATEGY 4 TAMAMLANDI: {strategy_name}\")\n",
    "print(\"=\"*60)\n",
    "print(f\"‚è±Ô∏è  Training Time: {training_time_min:.2f} minutes\")\n",
    "print(f\"\\nüìà Sonu√ßlar:\")\n",
    "for metric, value in test_results.items():\n",
    "    if isinstance(value, (int, float)):\n",
    "        print(f\"   {metric}: {value:.4f}\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da24503a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä PART B Section 1: Loss Function Comparison\n",
    "\n",
    "Compare the 4 loss function strategies (B-1 to B-4) to identify the best performers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5dcd2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract loss function results for comparison\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"üìä LOSS FUNCTION STRATEGIES COMPARISON\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "loss_strategies = ['baseline', 'weighted', 'focal_weak', 'focal_strong']\n",
    "loss_comparison_data = []\n",
    "\n",
    "for strategy_name in loss_strategies:\n",
    "    if strategy_name in all_test_results:\n",
    "        data = all_test_results[strategy_name]\n",
    "        results = data['results']\n",
    "        loss_comparison_data.append({\n",
    "            'Strategy': data['config'],\n",
    "            'Training_Time_min': data.get('training_time_min', 0),\n",
    "            'mAP': results.get('mean_average_precision', 0),\n",
    "            'Micro_F1': results.get('micro_f1', 0),\n",
    "            'Recall@5': results.get('recall_at_5', 0),\n",
    "            'Precision@5': results.get('precision_at_5', 0),\n",
    "            'Recall@10': results.get('recall_at_10', 0),\n",
    "            'Hamming_Loss': results.get('hamming_loss', 0),\n",
    "            'Micro_Precision': results.get('micro_precision', 0),\n",
    "            'Micro_Recall': results.get('micro_recall', 0)\n",
    "        })\n",
    "\n",
    "if len(loss_comparison_data) > 0:\n",
    "    df_loss_comparison = pd.DataFrame(loss_comparison_data)\n",
    "    \n",
    "    # Export CSV\n",
    "    import os\n",
    "    loss_csv_path = 'outputs/loss_function_comparison.csv'\n",
    "    os.makedirs('outputs', exist_ok=True)\n",
    "    df_loss_comparison.to_csv(loss_csv_path, index=False)\n",
    "    print(f\"‚úÖ CSV exported to: {loss_csv_path}\\n\")\n",
    "    \n",
    "    # Display comparison table\n",
    "    print(\"\\nüìã Loss Function Performance Comparison:\")\n",
    "    print(df_loss_comparison.to_string(index=False))\n",
    "    \n",
    "    # Find best strategies\n",
    "    print(\"\\nüèÜ Best Performers:\")\n",
    "    print(f\"   Best mAP: {df_loss_comparison.loc[df_loss_comparison['mAP'].idxmax(), 'Strategy']} ({df_loss_comparison['mAP'].max():.4f})\")\n",
    "    print(f\"   Best Micro F1: {df_loss_comparison.loc[df_loss_comparison['Micro_F1'].idxmax(), 'Strategy']} ({df_loss_comparison['Micro_F1'].max():.4f})\")\n",
    "    print(f\"   Best Recall@5: {df_loss_comparison.loc[df_loss_comparison['Recall@5'].idxmax(), 'Strategy']} ({df_loss_comparison['Recall@5'].max():.4f})\")\n",
    "    print(f\"   Lowest Hamming Loss: {df_loss_comparison.loc[df_loss_comparison['Hamming_Loss'].idxmin(), 'Strategy']} ({df_loss_comparison['Hamming_Loss'].min():.4f})\")\n",
    "    \n",
    "    # Create output directory for plots\n",
    "    loss_plots_dir = 'outputs/loss_function_plots'\n",
    "    os.makedirs(loss_plots_dir, exist_ok=True)\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Plot 1: mAP comparison\n",
    "    ax = axes[0, 0]\n",
    "    ax.bar(range(len(df_loss_comparison)), df_loss_comparison['mAP'], color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'], alpha=0.8)\n",
    "    ax.set_xticks(range(len(df_loss_comparison)))\n",
    "    ax.set_xticklabels(df_loss_comparison['Strategy'], rotation=45, ha='right')\n",
    "    ax.set_ylabel('Mean Average Precision (mAP)', fontsize=12)\n",
    "    ax.set_title('Loss Functions: mAP Comparison', fontsize=14, fontweight='bold')\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    for i, v in enumerate(df_loss_comparison['mAP']):\n",
    "        ax.text(i, v + 0.01, f'{v:.3f}', ha='center', fontsize=10)\n",
    "    \n",
    "    # Plot 2: Micro F1 comparison\n",
    "    ax = axes[0, 1]\n",
    "    ax.bar(range(len(df_loss_comparison)), df_loss_comparison['Micro_F1'], color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'], alpha=0.8)\n",
    "    ax.set_xticks(range(len(df_loss_comparison)))\n",
    "    ax.set_xticklabels(df_loss_comparison['Strategy'], rotation=45, ha='right')\n",
    "    ax.set_ylabel('Micro F1 Score', fontsize=12)\n",
    "    ax.set_title('Loss Functions: Micro F1 Comparison', fontsize=14, fontweight='bold')\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    for i, v in enumerate(df_loss_comparison['Micro_F1']):\n",
    "        ax.text(i, v + 0.01, f'{v:.3f}', ha='center', fontsize=10)\n",
    "    \n",
    "    # Plot 3: Recall@5 and Recall@10\n",
    "    ax = axes[1, 0]\n",
    "    x = range(len(df_loss_comparison))\n",
    "    width = 0.35\n",
    "    ax.bar([i - width/2 for i in x], df_loss_comparison['Recall@5'], width, label='Recall@5', alpha=0.8)\n",
    "    ax.bar([i + width/2 for i in x], df_loss_comparison['Recall@10'], width, label='Recall@10', alpha=0.8)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(df_loss_comparison['Strategy'], rotation=45, ha='right')\n",
    "    ax.set_ylabel('Recall Score', fontsize=12)\n",
    "    ax.set_title('Loss Functions: Recall@5 and Recall@10', fontsize=14, fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Precision vs Recall\n",
    "    ax = axes[1, 1]\n",
    "    ax.scatter(df_loss_comparison['Micro_Recall'], df_loss_comparison['Micro_Precision'], \n",
    "               s=200, c=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'], alpha=0.6)\n",
    "    for i, strategy in enumerate(df_loss_comparison['Strategy']):\n",
    "        ax.annotate(strategy, \n",
    "                   (df_loss_comparison.loc[i, 'Micro_Recall'], df_loss_comparison.loc[i, 'Micro_Precision']),\n",
    "                   xytext=(5, 5), textcoords='offset points', fontsize=10)\n",
    "    ax.set_xlabel('Micro Recall', fontsize=12)\n",
    "    ax.set_ylabel('Micro Precision', fontsize=12)\n",
    "    ax.set_title('Loss Functions: Precision vs Recall', fontsize=14, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save combined plot\n",
    "    combined_plot_path = f'{loss_plots_dir}/loss_function_comparison_all.png'\n",
    "    plt.savefig(combined_plot_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"\\n‚úÖ Combined plot saved: {combined_plot_path}\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Save individual plots\n",
    "    print(\"\\nüìä Saving individual plots...\")\n",
    "    \n",
    "    # Individual Plot 1: mAP\n",
    "    fig1, ax1 = plt.subplots(figsize=(10, 6))\n",
    "    ax1.bar(range(len(df_loss_comparison)), df_loss_comparison['mAP'], color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'], alpha=0.8)\n",
    "    ax1.set_xticks(range(len(df_loss_comparison)))\n",
    "    ax1.set_xticklabels(df_loss_comparison['Strategy'], rotation=45, ha='right')\n",
    "    ax1.set_ylabel('Mean Average Precision (mAP)', fontsize=12)\n",
    "    ax1.set_title('Loss Functions: mAP Comparison', fontsize=14, fontweight='bold')\n",
    "    ax1.grid(axis='y', alpha=0.3)\n",
    "    for i, v in enumerate(df_loss_comparison['mAP']):\n",
    "        ax1.text(i, v + 0.01, f'{v:.3f}', ha='center', fontsize=10)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{loss_plots_dir}/map_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"  ‚úì mAP plot saved\")\n",
    "    \n",
    "    # Individual Plot 2: Micro F1\n",
    "    fig2, ax2 = plt.subplots(figsize=(10, 6))\n",
    "    ax2.bar(range(len(df_loss_comparison)), df_loss_comparison['Micro_F1'], color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'], alpha=0.8)\n",
    "    ax2.set_xticks(range(len(df_loss_comparison)))\n",
    "    ax2.set_xticklabels(df_loss_comparison['Strategy'], rotation=45, ha='right')\n",
    "    ax2.set_ylabel('Micro F1 Score', fontsize=12)\n",
    "    ax2.set_title('Loss Functions: Micro F1 Comparison', fontsize=14, fontweight='bold')\n",
    "    ax2.grid(axis='y', alpha=0.3)\n",
    "    for i, v in enumerate(df_loss_comparison['Micro_F1']):\n",
    "        ax2.text(i, v + 0.01, f'{v:.3f}', ha='center', fontsize=10)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{loss_plots_dir}/micro_f1_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"  ‚úì Micro F1 plot saved\")\n",
    "    \n",
    "    # Individual Plot 3: Recall@K\n",
    "    fig3, ax3 = plt.subplots(figsize=(10, 6))\n",
    "    x = range(len(df_loss_comparison))\n",
    "    width = 0.35\n",
    "    ax3.bar([i - width/2 for i in x], df_loss_comparison['Recall@5'], width, label='Recall@5', alpha=0.8, color='lightgreen')\n",
    "    ax3.bar([i + width/2 for i in x], df_loss_comparison['Recall@10'], width, label='Recall@10', alpha=0.8, color='darkgreen')\n",
    "    ax3.set_xticks(x)\n",
    "    ax3.set_xticklabels(df_loss_comparison['Strategy'], rotation=45, ha='right')\n",
    "    ax3.set_ylabel('Recall Score', fontsize=12)\n",
    "    ax3.set_title('Loss Functions: Recall@5 and Recall@10', fontsize=14, fontweight='bold')\n",
    "    ax3.legend(fontsize=11)\n",
    "    ax3.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{loss_plots_dir}/recall_k_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"  ‚úì Recall@K plot saved\")\n",
    "    \n",
    "    # Individual Plot 4: Precision vs Recall\n",
    "    fig4, ax4 = plt.subplots(figsize=(10, 6))\n",
    "    ax4.scatter(df_loss_comparison['Micro_Recall'], df_loss_comparison['Micro_Precision'], \n",
    "               s=200, c=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'], alpha=0.6)\n",
    "    for i, strategy in enumerate(df_loss_comparison['Strategy']):\n",
    "        ax4.annotate(strategy, \n",
    "                   (df_loss_comparison.loc[i, 'Micro_Recall'], df_loss_comparison.loc[i, 'Micro_Precision']),\n",
    "                   xytext=(5, 5), textcoords='offset points', fontsize=10)\n",
    "    ax4.set_xlabel('Micro Recall', fontsize=12)\n",
    "    ax4.set_ylabel('Micro Precision', fontsize=12)\n",
    "    ax4.set_title('Loss Functions: Precision vs Recall', fontsize=14, fontweight='bold')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{loss_plots_dir}/precision_recall_scatter.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"  ‚úì Precision-Recall scatter plot saved\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ All plots saved to: {loss_plots_dir}/\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üìÅ EXPORTED FILES:\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"  ‚Ä¢ CSV: {loss_csv_path}\")\n",
    "    print(f\"  ‚Ä¢ Plots: {loss_plots_dir}/\")\n",
    "    print(\"    - loss_function_comparison_all.png (combined)\")\n",
    "    print(\"    - map_comparison.png\")\n",
    "    print(\"    - micro_f1_comparison.png\")\n",
    "    print(\"    - recall_k_comparison.png\")\n",
    "    print(\"    - precision_recall_scatter.png\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No loss function results found. Please run strategies B-1 to B-4 first.\")\n",
    "\n",
    "print(f\"\\n{'='*80}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb40a253",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üî¨ PART B Section 2: Capacity Testing (Top-K Analysis)\n",
    "\n",
    "Test model capacity with different label subset sizes to understand learning behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af275ab",
   "metadata": {},
   "source": [
    "### Strategy 5: Top-K Label Analysis (Capacity Test)\n",
    "\n",
    "**A√ßƒ±klama:** 5 farklƒ± K deƒüeri ile model kapasitesini test eder. Her K i√ßin baseline BCE kullanƒ±r.\n",
    "\n",
    "**Test Edilen K Deƒüerleri:**\n",
    "- Top-100: Geni≈ü label seti\n",
    "- Top-50: Orta seviye\n",
    "- Top-20: K√º√ß√ºk label seti\n",
    "- Top-10: Minimal label seti\n",
    "- Top-5: En k√º√ß√ºk subset\n",
    "\n",
    "**√áƒ±ktƒ±lar:**\n",
    "- CSV dosyasƒ±: T√ºm metriklerin kar≈üƒ±la≈ütƒ±rmasƒ±\n",
    "- 4 grafikli histogram: F1 vs K, Hamming Loss vs K, Recall@5, All Metrics\n",
    "\n",
    "**√ñnerilen Kullanƒ±m:** Model'in farklƒ± label sayƒ±larƒ±ndaki performansƒ±nƒ± g√∂rmek i√ßin √ßalƒ±≈ütƒ±rƒ±n (~2-2.5 saat)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c188f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top-K Label Analysis: Test model capacity with different label subset sizes\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"üß™ TOP-K LABEL ANALYSIS\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "from src.strategies import filter_top_k_labels\n",
    "\n",
    "# Test different K values\n",
    "k_values = [100, 50, 20, 10, 5]\n",
    "topk_results = {}\n",
    "\n",
    "for k in k_values:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üî¨ Testing Top-{k} Labels\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Filter dataset to top-k labels\n",
    "    filtered_train_ds, filtered_label_list, label_mapping = filter_top_k_labels(\n",
    "        train_dataset, \n",
    "        label_names, \n",
    "        k=k\n",
    "    )\n",
    "    filtered_test_ds, _, _ = filter_top_k_labels(\n",
    "        test_dataset, \n",
    "        label_names, \n",
    "        k=k\n",
    "    )\n",
    "    \n",
    "    print(f\"üìä Dataset Statistics:\")\n",
    "    print(f\"   Top-{k} labels selected\")\n",
    "    print(f\"   Train samples: {len(filtered_train_ds)}\")\n",
    "    print(f\"   Test samples: {len(filtered_test_ds)}\")\n",
    "    print(f\"   Labels: {filtered_label_list[:5]}...\")\n",
    "    \n",
    "    # Create dataloaders\n",
    "    topk_train_loader = DataLoader(\n",
    "        filtered_train_ds,\n",
    "        batch_size=BASE_CONFIG['batch_size'],\n",
    "        shuffle=True\n",
    "    )\n",
    "    topk_test_loader = DataLoader(\n",
    "        filtered_test_ds,\n",
    "        batch_size=BASE_CONFIG['batch_size'],\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    # Load model for this K\n",
    "    print(f\"\\nüîß Loading model for {k} labels...\")\n",
    "    topk_model = load_model(\n",
    "        model_name=BASE_CONFIG['model_name'],\n",
    "        num_labels=k,\n",
    "        device=BASE_CONFIG['device'],\n",
    "        use_focal_loss=False,\n",
    "        pos_weight=None\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    print(f\"\\nüöÄ Training on Top-{k}...\")\n",
    "    topk_history = train_model(\n",
    "        model=topk_model,\n",
    "        train_dataloader=topk_train_loader,\n",
    "        num_epochs=BASE_CONFIG['num_epochs'],\n",
    "        learning_rate=BASE_CONFIG['learning_rate'],\n",
    "        device=BASE_CONFIG['device']\n",
    "    )\n",
    "    \n",
    "    # Evaluate model\n",
    "    print(f\"\\nüìä Evaluating Top-{k}...\")\n",
    "    topk_test_results = evaluate_model(\n",
    "        model=topk_model,\n",
    "        test_dataloader=topk_test_loader,\n",
    "        label_names=filtered_label_list,\n",
    "        device=BASE_CONFIG['device']\n",
    "    )\n",
    "    \n",
    "    # Store results\n",
    "    topk_results[f'top_{k}'] = {\n",
    "        'k': k,\n",
    "        'num_train': len(filtered_train_ds),\n",
    "        'num_test': len(filtered_test_ds),\n",
    "        'metrics': topk_test_results,\n",
    "        'labels': filtered_label_list\n",
    "    }\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"\\n‚úÖ Top-{k} Results:\")\n",
    "    for metric, value in topk_test_results.items():\n",
    "        if isinstance(value, (int, float)):\n",
    "            print(f\"   {metric}: {value:.4f}\")\n",
    "\n",
    "# Create comparison DataFrame\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"üìä TOP-K COMPARISON TABLE\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "topk_comparison = []\n",
    "for key, data in topk_results.items():\n",
    "    metrics = data['metrics']\n",
    "    topk_comparison.append({\n",
    "        'K': data['k'],\n",
    "        'Train Samples': data['num_train'],\n",
    "        'Test Samples': data['num_test'],\n",
    "        'Micro F1': metrics.get('micro_f1', 0),\n",
    "        'Hamming Loss': metrics.get('hamming_loss', 0),\n",
    "        'Micro Precision': metrics.get('micro_precision', 0),\n",
    "        'Micro Recall': metrics.get('micro_recall', 0),\n",
    "        'Recall@5': metrics.get('recall_at_5', 0),\n",
    "        'Precision@5': metrics.get('precision_at_5', 0)\n",
    "    })\n",
    "\n",
    "df_topk = pd.DataFrame(topk_comparison)\n",
    "df_topk = df_topk.sort_values('K', ascending=False)\n",
    "print(df_topk.to_string(index=False))\n",
    "\n",
    "# Save CSV\n",
    "import os\n",
    "topk_csv_path = 'outputs/topk_analysis.csv'\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "df_topk.to_csv(topk_csv_path, index=False)\n",
    "print(f\"\\n‚úÖ CSV saved: {topk_csv_path}\")\n",
    "\n",
    "# Create output directory for plots\n",
    "topk_plots_dir = 'outputs/topk_analysis_plots'\n",
    "os.makedirs(topk_plots_dir, exist_ok=True)\n",
    "\n",
    "# Create visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Plot 1: F1 Score vs K\n",
    "ax = axes[0, 0]\n",
    "ax.plot(df_topk['K'], df_topk['Micro F1'], marker='o', linewidth=2, markersize=8, label='Micro F1', color='blue')\n",
    "ax.set_xlabel('Number of Labels (K)', fontsize=12)\n",
    "ax.set_ylabel('Micro F1 Score', fontsize=12)\n",
    "ax.set_title('Model Performance vs Label Count', fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend()\n",
    "ax.invert_xaxis()  # Higher K on left\n",
    "\n",
    "# Plot 2: Hamming Loss vs K\n",
    "ax = axes[0, 1]\n",
    "ax.plot(df_topk['K'], df_topk['Hamming Loss'], marker='s', linewidth=2, markersize=8, label='Hamming Loss', color='red')\n",
    "ax.set_xlabel('Number of Labels (K)', fontsize=12)\n",
    "ax.set_ylabel('Hamming Loss (lower is better)', fontsize=12)\n",
    "ax.set_title('Hamming Loss vs Label Count', fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend()\n",
    "ax.invert_xaxis()  # Higher K on left\n",
    "\n",
    "# Plot 3: Recall@5 vs K\n",
    "ax = axes[1, 0]\n",
    "ax.bar(range(len(df_topk)), df_topk['Recall@5'], alpha=0.7, color='coral')\n",
    "ax.set_xticks(range(len(df_topk)))\n",
    "ax.set_xticklabels([f'Top-{k}' for k in df_topk['K']])\n",
    "ax.set_ylabel('Recall@5', fontsize=12)\n",
    "ax.set_title('Top-5 Recall by Label Count', fontsize=14, fontweight='bold')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "for i, v in enumerate(df_topk['Recall@5']):\n",
    "    ax.text(i, v + 0.02, f'{v:.3f}', ha='center', fontsize=10)\n",
    "\n",
    "# Plot 4: All metrics comparison\n",
    "ax = axes[1, 1]\n",
    "x = range(len(df_topk))\n",
    "width = 0.2\n",
    "ax.bar([i - width*1.5 for i in x], df_topk['Micro F1'], width, label='F1', alpha=0.8)\n",
    "ax.bar([i - width*0.5 for i in x], df_topk['Micro Precision'], width, label='Precision', alpha=0.8)\n",
    "ax.bar([i + width*0.5 for i in x], df_topk['Micro Recall'], width, label='Recall', alpha=0.8)\n",
    "ax.bar([i + width*1.5 for i in x], df_topk['Recall@5'], width, label='Recall@5', alpha=0.8)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([f'Top-{k}' for k in df_topk['K']])\n",
    "ax.set_ylabel('Score', fontsize=12)\n",
    "ax.set_title('All Metrics Comparison', fontsize=14, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save combined plot\n",
    "combined_plot_path = f'{topk_plots_dir}/topk_analysis_all.png'\n",
    "plt.savefig(combined_plot_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"\\n‚úÖ Combined plot saved: {combined_plot_path}\")\n",
    "plt.show()\n",
    "\n",
    "# Save individual plots\n",
    "print(\"\\nüìä Saving individual plots...\")\n",
    "\n",
    "# Individual Plot 1: F1 Score vs K\n",
    "fig1, ax1 = plt.subplots(figsize=(10, 6))\n",
    "ax1.plot(df_topk['K'], df_topk['Micro F1'], marker='o', linewidth=2, markersize=8, label='Micro F1', color='blue')\n",
    "ax1.set_xlabel('Number of Labels (K)', fontsize=12)\n",
    "ax1.set_ylabel('Micro F1 Score', fontsize=12)\n",
    "ax1.set_title('Top-K Analysis: Model Performance vs Label Count', fontsize=14, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.invert_xaxis()\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{topk_plots_dir}/f1_vs_k.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(f\"  ‚úì F1 vs K plot saved\")\n",
    "\n",
    "# Individual Plot 2: Hamming Loss vs K\n",
    "fig2, ax2 = plt.subplots(figsize=(10, 6))\n",
    "ax2.plot(df_topk['K'], df_topk['Hamming Loss'], marker='s', linewidth=2, markersize=8, label='Hamming Loss', color='red')\n",
    "ax2.set_xlabel('Number of Labels (K)', fontsize=12)\n",
    "ax2.set_ylabel('Hamming Loss (lower is better)', fontsize=12)\n",
    "ax2.set_title('Top-K Analysis: Hamming Loss vs Label Count', fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.invert_xaxis()\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{topk_plots_dir}/hamming_loss_vs_k.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(f\"  ‚úì Hamming Loss vs K plot saved\")\n",
    "\n",
    "# Individual Plot 3: Recall@5 vs K\n",
    "fig3, ax3 = plt.subplots(figsize=(10, 6))\n",
    "ax3.bar(range(len(df_topk)), df_topk['Recall@5'], alpha=0.7, color='coral')\n",
    "ax3.set_xticks(range(len(df_topk)))\n",
    "ax3.set_xticklabels([f'Top-{k}' for k in df_topk['K']])\n",
    "ax3.set_ylabel('Recall@5', fontsize=12)\n",
    "ax3.set_title('Top-K Analysis: Recall@5 by Label Count', fontsize=14, fontweight='bold')\n",
    "ax3.grid(axis='y', alpha=0.3)\n",
    "for i, v in enumerate(df_topk['Recall@5']):\n",
    "    ax3.text(i, v + 0.02, f'{v:.3f}', ha='center', fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{topk_plots_dir}/recall5_vs_k.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(f\"  ‚úì Recall@5 plot saved\")\n",
    "\n",
    "# Individual Plot 4: All Metrics Comparison\n",
    "fig4, ax4 = plt.subplots(figsize=(12, 6))\n",
    "x = range(len(df_topk))\n",
    "width = 0.2\n",
    "ax4.bar([i - width*1.5 for i in x], df_topk['Micro F1'], width, label='F1', alpha=0.8)\n",
    "ax4.bar([i - width*0.5 for i in x], df_topk['Micro Precision'], width, label='Precision', alpha=0.8)\n",
    "ax4.bar([i + width*0.5 for i in x], df_topk['Micro Recall'], width, label='Recall', alpha=0.8)\n",
    "ax4.bar([i + width*1.5 for i in x], df_topk['Recall@5'], width, label='Recall@5', alpha=0.8)\n",
    "ax4.set_xticks(x)\n",
    "ax4.set_xticklabels([f'Top-{k}' for k in df_topk['K']])\n",
    "ax4.set_ylabel('Score', fontsize=12)\n",
    "ax4.set_title('Top-K Analysis: All Metrics Comparison', fontsize=14, fontweight='bold')\n",
    "ax4.legend(fontsize=11)\n",
    "ax4.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{topk_plots_dir}/all_metrics_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(f\"  ‚úì All metrics comparison plot saved\")\n",
    "\n",
    "print(f\"\\n‚úÖ All plots saved to: {topk_plots_dir}/\")\n",
    "\n",
    "# Store in all_test_results for comparison\n",
    "for key, data in topk_results.items():\n",
    "    all_test_results[key] = {\n",
    "        'config': f\"Top-{data['k']} Labels\",\n",
    "        'description': f\"Baseline BCE with {data['k']} most frequent labels\",\n",
    "        'results': data['metrics']\n",
    "    }\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìÅ EXPORTED FILES:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"  ‚Ä¢ CSV: {topk_csv_path}\")\n",
    "print(f\"  ‚Ä¢ Plots: {topk_plots_dir}/\")\n",
    "print(\"    - topk_analysis_all.png (combined)\")\n",
    "print(\"    - f1_vs_k.png\")\n",
    "print(\"    - hamming_loss_vs_k.png\")\n",
    "print(\"    - recall5_vs_k.png\")\n",
    "print(\"    - all_metrics_comparison.png\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"‚úÖ TOP-K ANALYSIS COMPLETE\")\n",
    "print(f\"{'='*60}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4409327",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîó PART C: HYBRID STRATEGIES (Loss √ó Classification)\n",
    "\n",
    "**Not:** Bu b√∂l√ºm√º PART A ve B'den SONRA √ßalƒ±≈ütƒ±rƒ±n!\n",
    "\n",
    "**Comprehensive Testing:** 2 Loss √ó 5 Methods = **10 Strateji**\n",
    "\n",
    "Bu b√∂l√ºm **t√ºm loss fonksiyonlarƒ±nƒ± t√ºm classification y√∂ntemleriyle** test eder:\n",
    "\n",
    "### Loss Functions (Part B'den se√ßildi):\n",
    "1. **Weighted BCE** - Frequency-based weights (pos_weight) - En ba≈üarƒ±lƒ± baseline\n",
    "2. **Focal Loss Œ≥=5** - Strong hard example focusing - En g√º√ßl√º focal loss\n",
    "\n",
    "### Classification Methods (5 y√∂ntem):\n",
    "1. **ClassifierChain** - Label dependencies (sequential) - Label ili≈ükileri i√ßin\n",
    "2. **ExtraTreesClassifier** - Extremely randomized trees - Hƒ±z ve √ße≈üitlilik i√ßin\n",
    "3. **RandomForestClassifier** - Ensemble of decision trees - Y√ºksek doƒüruluk i√ßin\n",
    "4. **AttentionXML** (NeurIPS 2019) - Multi-label attention mechanism\n",
    "5. **LightXML** (AAAI 2021) - Dynamic negative sampling + label embeddings\n",
    "\n",
    "### Strategy Matrix:\n",
    "```\n",
    "                    Chain  ExtraTrees  RandomForest  AttentionXML  LightXML\n",
    "Weighted BCE        C-1    C-2         C-3           C-4           C-5\n",
    "Focal Loss Œ≥=5      C-6    C-7         C-8           C-9           C-10\n",
    "```\n",
    "\n",
    "**Method √ñzellikleri:**\n",
    "- **ClassifierChain**: Sequential label modeling, captures dependencies\n",
    "- **ExtraTrees**: Faster, more diverse, less overfitting (random splits)\n",
    "- **RandomForest**: Optimal splits, slightly better accuracy\n",
    "- **AttentionXML**: Her label i√ßin √∂zel attention weights ‚Üí label-specific features\n",
    "- **LightXML**: Two-stage ranking + dynamic negative sampling ‚Üí efficiency\n",
    "\n",
    "**Loss Function Etkisi:**\n",
    "- **Weighted BCE**: Standard approach, class imbalance i√ßin weights\n",
    "- **Focal Loss Œ≥=5**: Hard examples'a odaklanƒ±r, tail TTPs i√ßin potansiyel iyile≈ütirme\n",
    "\n",
    "**Toplam S√ºre:** ~7.5-10 saat (10 strateji, ~45-60 dakika each)\n",
    "\n",
    "**Kullanƒ±m:** A≈üaƒüƒ±daki h√ºcreyi √ßalƒ±≈ütƒ±rarak t√ºm kombinasyonlarƒ± otomatik olarak test edin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e104bd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"üîÑ HYBRID STRATEGIES: LOSS FUNCTIONS √ó CLASSIFICATION METHODS\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "from src.classifier_chain import train_classifier_chain, evaluate_classifier_chain\n",
    "from src.classifier_chain import train_multi_output_classifier, evaluate_multi_output_classifier\n",
    "\n",
    "# Define loss configurations (2 most promising from PART B experiments)\n",
    "# Format: (name, use_focal_loss, pos_weight_config, focal_alpha, focal_gamma)\n",
    "loss_configs = []\n",
    "\n",
    "# 1. Weighted BCE (best baseline)\n",
    "strategy_config_weighted = get_strategy_config(\n",
    "    strategy_name='weighted',\n",
    "    train_dataset=train_dataset,\n",
    "    num_labels=num_labels,\n",
    "    label_list=label_names\n",
    ")\n",
    "loss_configs.append({\n",
    "    'name': 'weighted',\n",
    "    'display_name': 'Weighted BCE',\n",
    "    'use_focal_loss': False,\n",
    "    'pos_weight': strategy_config_weighted['pos_weight'],\n",
    "    'focal_alpha': None,\n",
    "    'focal_gamma': None\n",
    "})\n",
    "\n",
    "# 2. Focal Loss Œ≥=5 (strongest focal)\n",
    "loss_configs.append({\n",
    "    'name': 'focal_gamma5',\n",
    "    'display_name': 'Focal Loss (Œ≥=5)',\n",
    "    'use_focal_loss': True,\n",
    "    'pos_weight': None,\n",
    "    'focal_alpha': 0.25,\n",
    "    'focal_gamma': 5.0\n",
    "})\n",
    "\n",
    "# Define classification methods (5 methods: 3 traditional + 2 XMC)\n",
    "classification_methods = [\n",
    "    {\n",
    "        'name': 'classifier_chain',\n",
    "        'display_name': 'ClassifierChain',\n",
    "        'base_estimator': 'logistic',\n",
    "        'train_func': train_classifier_chain,\n",
    "        'eval_func': evaluate_classifier_chain\n",
    "    },\n",
    "    {\n",
    "        'name': 'extra_trees',\n",
    "        'display_name': 'ExtraTreesClassifier',\n",
    "        'base_estimator': 'extra_trees',\n",
    "        'train_func': train_multi_output_classifier,\n",
    "        'eval_func': evaluate_multi_output_classifier\n",
    "    },\n",
    "    {\n",
    "        'name': 'random_forest',\n",
    "        'display_name': 'RandomForestClassifier',\n",
    "        'base_estimator': 'random_forest',\n",
    "        'train_func': train_multi_output_classifier,\n",
    "        'eval_func': evaluate_multi_output_classifier\n",
    "    },\n",
    "    {\n",
    "        'name': 'attentionxml',\n",
    "        'display_name': 'AttentionXML',\n",
    "        'train_func': 'attention_xml',\n",
    "        'eval_func': 'attention_xml'\n",
    "    },\n",
    "    {\n",
    "        'name': 'lightxml',\n",
    "        'display_name': 'LightXML',\n",
    "        'train_func': 'light_xml',\n",
    "        'eval_func': 'light_xml'\n",
    "    }\n",
    "]\n",
    "\n",
    "# Counter for strategy numbering (starting from 1 for Part C)\n",
    "strategy_counter = 1\n",
    "\n",
    "# Test all combinations\n",
    "print(f\"üß™ Testing {len(loss_configs)} loss functions √ó {len(classification_methods)} classification methods\")\n",
    "print(f\"   Total combinations: {len(loss_configs) * len(classification_methods)}\")\n",
    "print(f\"   Estimated time: ~{len(loss_configs) * len(classification_methods) * 50} minutes\")\n",
    "print(f\"\\n{'='*80}\\n\")\n",
    "\n",
    "for loss_config in loss_configs:\n",
    "    for clf_method in classification_methods:\n",
    "        \n",
    "        strategy_name = f\"hybrid_{loss_config['name']}_{clf_method['name']}\"\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"üß™ PART C STRATEGY {strategy_counter}: {loss_config['display_name']} + {clf_method['display_name']}\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "        \n",
    "        # Start timing\n",
    "        strategy_start_time = time.time()\n",
    "        \n",
    "        # STEP 1: Configuration\n",
    "        print(f\"üìã Configuration:\")\n",
    "        print(f\"   Loss Function: {loss_config['display_name']}\")\n",
    "        print(f\"   Classification Method: {clf_method['display_name']}\")\n",
    "        print(f\"   Use Focal Loss: {loss_config['use_focal_loss']}\")\n",
    "        if loss_config['pos_weight'] is not None:\n",
    "            print(f\"   Pos Weight: min={loss_config['pos_weight'].min():.2f}, max={loss_config['pos_weight'].max():.2f}\")\n",
    "        if loss_config['use_focal_loss']:\n",
    "            print(f\"   Focal Alpha: {loss_config['focal_alpha']}\")\n",
    "            print(f\"   Focal Gamma: {loss_config['focal_gamma']}\")\n",
    "        \n",
    "        # Check if this is AttentionXML or LightXML (end-to-end training)\n",
    "        if clf_method['name'] in ['attentionxml', 'lightxml']:\n",
    "            # Import XML utilities\n",
    "            from src.xml_utils import train_attention_xml, evaluate_attention_xml\n",
    "            from src.xml_utils import train_light_xml, evaluate_light_xml\n",
    "            \n",
    "            if clf_method['name'] == 'attentionxml':\n",
    "                from src.attention_xml import load_attention_xml_model\n",
    "                \n",
    "                print(f\"\\nüîß Training {clf_method['display_name']} (end-to-end with {loss_config['display_name']})...\")\n",
    "                model = load_attention_xml_model(\n",
    "                    model_name=BASE_CONFIG['model_name'],\n",
    "                    num_labels=num_labels,\n",
    "                    device=BASE_CONFIG['device'],\n",
    "                    dropout=0.1\n",
    "                )\n",
    "                \n",
    "                training_history = train_attention_xml(\n",
    "                    model=model,\n",
    "                    train_dataloader=train_dataloader,\n",
    "                    num_epochs=BASE_CONFIG['num_epochs'],\n",
    "                    learning_rate=BASE_CONFIG['learning_rate'],\n",
    "                    device=BASE_CONFIG['device'],\n",
    "                    use_focal_loss=loss_config['use_focal_loss'],\n",
    "                    pos_weight=loss_config['pos_weight'],\n",
    "                    focal_alpha=loss_config['focal_alpha'],\n",
    "                    focal_gamma=loss_config['focal_gamma']\n",
    "                )\n",
    "                \n",
    "                print(f\"\\nüìä Evaluating...\")\n",
    "                test_results = evaluate_attention_xml(\n",
    "                    model=model,\n",
    "                    test_dataloader=test_dataloader,\n",
    "                    device=BASE_CONFIG['device'],\n",
    "                    label_names=label_names\n",
    "                )\n",
    "                \n",
    "            else:  # lightxml\n",
    "                from src.light_xml import load_light_xml_model\n",
    "                \n",
    "                print(f\"\\nüîß Training {clf_method['display_name']} (end-to-end with {loss_config['display_name']})...\")\n",
    "                model = load_light_xml_model(\n",
    "                    model_name=BASE_CONFIG['model_name'],\n",
    "                    num_labels=num_labels,\n",
    "                    device=BASE_CONFIG['device'],\n",
    "                    num_label_groups=50,\n",
    "                    label_emb_dim=128,\n",
    "                    dropout=0.1\n",
    "                )\n",
    "                \n",
    "                training_history = train_light_xml(\n",
    "                    model=model,\n",
    "                    train_dataloader=train_dataloader,\n",
    "                    num_epochs=BASE_CONFIG['num_epochs'],\n",
    "                    learning_rate=BASE_CONFIG['learning_rate'],\n",
    "                    device=BASE_CONFIG['device'],\n",
    "                    use_focal_loss=loss_config['use_focal_loss'],\n",
    "                    pos_weight=loss_config['pos_weight'],\n",
    "                    focal_alpha=loss_config['focal_alpha'],\n",
    "                    focal_gamma=loss_config['focal_gamma']\n",
    "                )\n",
    "                \n",
    "                print(f\"\\nüìä Evaluating...\")\n",
    "                test_results = evaluate_light_xml(\n",
    "                    model=model,\n",
    "                    test_dataloader=test_dataloader,\n",
    "                    device=BASE_CONFIG['device'],\n",
    "                    label_names=label_names\n",
    "                )\n",
    "        \n",
    "        else:\n",
    "            # Traditional two-stage approach: BERT + Classification\n",
    "            # STEP 1: Train BERT with specific loss function\n",
    "            print(f\"\\nüîß Step 1/3: Training BERT with {loss_config['display_name']}...\")\n",
    "            bert_model = load_model(\n",
    "                model_name=BASE_CONFIG['model_name'],\n",
    "                num_labels=num_labels,\n",
    "                device=BASE_CONFIG['device'],\n",
    "                use_focal_loss=loss_config['use_focal_loss'],\n",
    "                focal_alpha=loss_config['focal_alpha'],\n",
    "                focal_gamma=loss_config['focal_gamma'],\n",
    "                pos_weight=loss_config['pos_weight']\n",
    "            )\n",
    "            \n",
    "            training_history = train_model(\n",
    "                model=bert_model,\n",
    "                train_dataloader=train_dataloader,\n",
    "                num_epochs=BASE_CONFIG['num_epochs'],\n",
    "                learning_rate=BASE_CONFIG['learning_rate'],\n",
    "                device=BASE_CONFIG['device']\n",
    "            )\n",
    "            \n",
    "            # STEP 2: Train classifier on top of BERT embeddings\n",
    "            print(f\"\\nüîß Step 2/3: Training {clf_method['display_name']}...\")\n",
    "            \n",
    "            if clf_method['name'] == 'classifier_chain':\n",
    "                # ClassifierChain: Sequential label modeling\n",
    "                chain_model = train_classifier_chain(\n",
    "                    bert_model=bert_model,\n",
    "                    train_dataloader=train_dataloader,\n",
    "                    device=BASE_CONFIG['device'],\n",
    "                    base_estimator=clf_method['base_estimator'],\n",
    "                    order='random',\n",
    "                    random_state=42\n",
    "                )\n",
    "                \n",
    "                print(f\"\\nüìä Step 3/3: Evaluating...\")\n",
    "                test_results = evaluate_classifier_chain(\n",
    "                    model=chain_model,\n",
    "                    test_dataloader=test_dataloader,\n",
    "                    label_names=label_names\n",
    "                )\n",
    "            \n",
    "            else:  # MultiOutputClassifier (RandomForest or ExtraTrees)\n",
    "                # Multi-output: Independent binary classifiers\n",
    "                multi_output_model = train_multi_output_classifier(\n",
    "                    bert_model=bert_model,\n",
    "                    train_dataloader=train_dataloader,\n",
    "                    device=BASE_CONFIG['device'],\n",
    "                    base_estimator=clf_method['base_estimator'],\n",
    "                    n_jobs=-1,\n",
    "                    random_state=42\n",
    "                )\n",
    "                \n",
    "                print(f\"\\nüìä Step 3/3: Evaluating...\")\n",
    "                test_results = evaluate_multi_output_classifier(\n",
    "                    model=multi_output_model,\n",
    "                    test_dataloader=test_dataloader,\n",
    "                    label_names=label_names\n",
    "                )\n",
    "        \n",
    "        # Calculate training time\n",
    "        training_time_min = (time.time() - strategy_start_time) / 60\n",
    "        \n",
    "        # Store results\n",
    "        all_test_results[strategy_name] = {\n",
    "            'config': f\"{loss_config['display_name']} + {clf_method['display_name']}\",\n",
    "            'description': f\"Hybrid strategy: {loss_config['display_name']} loss with {clf_method['display_name']}\",\n",
    "            'training_time_min': training_time_min,\n",
    "            'results': test_results\n",
    "        }\n",
    "        \n",
    "        # Display results\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"‚úÖ PART C STRATEGY {strategy_counter} COMPLETE: {strategy_name}\")\n",
    "        print(f\"‚è±Ô∏è  Training Time: {training_time_min:.2f} minutes\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        print(f\"\\nüìà Results:\")\n",
    "        for metric, value in test_results.items():\n",
    "            if isinstance(value, (int, float)):\n",
    "                print(f\"   {metric}: {value:.4f}\")\n",
    "        \n",
    "        print(f\"\\n{'='*80}\\n\")\n",
    "        \n",
    "        # Increment counter\n",
    "        strategy_counter += 1\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"‚úÖ ALL HYBRID STRATEGIES COMPLETE\")\n",
    "print(f\"   Total strategies tested: {len(loss_configs) * len(classification_methods)}\")\n",
    "print(f\"{'='*80}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe3c589",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä RESULTS COMPARISON\n",
    "\n",
    "√áalƒ±≈ütƒ±rdƒ±ƒüƒ±nƒ±z stratejilerin sonu√ßlarƒ±nƒ± kar≈üƒ±la≈ütƒ±rmak i√ßin bu h√ºcreyi √ßalƒ±≈ütƒ±rƒ±n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8301b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(all_test_results) == 0:\n",
    "    print(\"‚ö†Ô∏è Hen√ºz hi√ßbir strateji test edilmedi!\")\n",
    "else:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üìä STRATEGY COMPARISON RESULTS\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    # Create comparison table\n",
    "    comparison_data = []\n",
    "    for strategy_name, data in all_test_results.items():\n",
    "        results = data['results']\n",
    "        comparison_data.append({\n",
    "            'Strategy': strategy_name,\n",
    "            'Training Time (min)': data.get('training_time_min', 0),\n",
    "            'Micro Precision': results.get('micro_precision', 0),\n",
    "            'Micro Recall': results.get('micro_recall', 0),\n",
    "            'Micro F1': results.get('micro_f1', 0),\n",
    "            'mAP': results.get('mean_average_precision', 0),\n",
    "            'Hamming Loss': results.get('hamming_loss', 0),\n",
    "            'Example-based Accuracy': results.get('example_based_accuracy', 0),\n",
    "            'Precision@5': results.get('precision_at_5', 0),\n",
    "            'Recall@5': results.get('recall_at_5', 0),\n",
    "            'Precision@10': results.get('precision_at_10', 0),\n",
    "            'Recall@10': results.get('recall_at_10', 0)\n",
    "        })\n",
    "    \n",
    "    df_comparison = pd.DataFrame(comparison_data)\n",
    "    df_comparison = df_comparison.sort_values('mAP', ascending=False)  # Sort by mAP\n",
    "    \n",
    "    print(df_comparison.to_string(index=False))\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    \n",
    "    # Find best strategy\n",
    "    best_strategy = df_comparison.iloc[0]['Strategy']\n",
    "    best_map = df_comparison.iloc[0]['mAP']\n",
    "    best_f1 = df_comparison.iloc[0]['Micro F1']\n",
    "    \n",
    "    print(f\"\\nüèÜ EN ƒ∞Yƒ∞ STRATEJƒ∞: {best_strategy}\")\n",
    "    print(f\"   mAP (Ranking Quality): {best_map:.4f}\")\n",
    "    print(f\"   Micro F1 Score: {best_f1:.4f}\")\n",
    "    print(f\"   Micro Precision: {df_comparison.iloc[0]['Micro Precision']:.4f}\")\n",
    "    print(f\"   Micro Recall: {df_comparison.iloc[0]['Micro Recall']:.4f}\")\n",
    "    \n",
    "    # Compare to baseline if exists\n",
    "    if 'baseline' in all_test_results:\n",
    "        baseline_map = all_test_results['baseline']['results'].get('mean_average_precision', 0)\n",
    "        improvement = ((best_map - baseline_map) / baseline_map * 100) if baseline_map > 0 else 0\n",
    "        print(f\"   Baseline'a g√∂re mAP iyile≈ütirme: {improvement:+.2f}%\")\n",
    "    \n",
    "    # Save comparison to CSV\n",
    "    csv_path = 'outputs/hybrid_strategies_comparison.csv'\n",
    "    os.makedirs('outputs', exist_ok=True)\n",
    "    df_comparison.to_csv(csv_path, index=False)\n",
    "    print(f\"\\nüíæ CSV saved: {csv_path}\")\n",
    "    \n",
    "    # Plot comparison - individual plots\n",
    "    if len(all_test_results) > 1:\n",
    "        plots_dir = 'outputs/hybrid_strategies_plots'\n",
    "        os.makedirs(plots_dir, exist_ok=True)\n",
    "        \n",
    "        strategies = df_comparison['Strategy'].tolist()\n",
    "        x = np.arange(len(strategies))\n",
    "        \n",
    "        # 1. Micro Precision\n",
    "        fig1, ax1 = plt.subplots(figsize=(10, 6))\n",
    "        ax1.bar(x, df_comparison['Micro Precision'].tolist(), alpha=0.8, color='skyblue')\n",
    "        ax1.set_xlabel('Strategy', fontsize=12)\n",
    "        ax1.set_ylabel('Micro Precision', fontsize=12)\n",
    "        ax1.set_title('Hybrid Strategies: Micro Precision Comparison', fontsize=14, fontweight='bold')\n",
    "        ax1.set_xticks(x)\n",
    "        ax1.set_xticklabels(strategies, rotation=45, ha='right')\n",
    "        ax1.grid(axis='y', alpha=0.3)\n",
    "        ax1.set_ylim([0, 1])\n",
    "        for i, v in enumerate(df_comparison['Micro Precision'].tolist()):\n",
    "            ax1.text(i, v + 0.02, f'{v:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{plots_dir}/micro_precision.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # 2. Micro Recall\n",
    "        fig2, ax2 = plt.subplots(figsize=(10, 6))\n",
    "        ax2.bar(x, df_comparison['Micro Recall'].tolist(), alpha=0.8, color='lightcoral')\n",
    "        ax2.set_xlabel('Strategy', fontsize=12)\n",
    "        ax2.set_ylabel('Micro Recall', fontsize=12)\n",
    "        ax2.set_title('Hybrid Strategies: Micro Recall Comparison', fontsize=14, fontweight='bold')\n",
    "        ax2.set_xticks(x)\n",
    "        ax2.set_xticklabels(strategies, rotation=45, ha='right')\n",
    "        ax2.grid(axis='y', alpha=0.3)\n",
    "        ax2.set_ylim([0, 1])\n",
    "        for i, v in enumerate(df_comparison['Micro Recall'].tolist()):\n",
    "            ax2.text(i, v + 0.02, f'{v:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{plots_dir}/micro_recall.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # 3. Hamming Loss\n",
    "        fig3, ax3 = plt.subplots(figsize=(10, 6))\n",
    "        ax3.bar(x, df_comparison['Hamming Loss'].tolist(), alpha=0.8, color='orange')\n",
    "        ax3.set_xlabel('Strategy', fontsize=12)\n",
    "        ax3.set_ylabel('Hamming Loss', fontsize=12)\n",
    "        ax3.set_title('Hybrid Strategies: Hamming Loss Comparison (Lower is Better)', fontsize=14, fontweight='bold')\n",
    "        ax3.set_xticks(x)\n",
    "        ax3.set_xticklabels(strategies, rotation=45, ha='right')\n",
    "        ax3.grid(axis='y', alpha=0.3)\n",
    "        for i, v in enumerate(df_comparison['Hamming Loss'].tolist()):\n",
    "            ax3.text(i, v + 0.001, f'{v:.4f}', ha='center', va='bottom', fontsize=9)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{plots_dir}/hamming_loss.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # 4. mAP (Mean Average Precision)\n",
    "        fig4, ax4 = plt.subplots(figsize=(10, 6))\n",
    "        ax4.bar(x, df_comparison['mAP'].tolist(), alpha=0.8, color='green')\n",
    "        ax4.set_xlabel('Strategy', fontsize=12)\n",
    "        ax4.set_ylabel('mAP', fontsize=12)\n",
    "        ax4.set_title('Hybrid Strategies: Mean Average Precision (mAP) Comparison', fontsize=14, fontweight='bold')\n",
    "        ax4.set_xticks(x)\n",
    "        ax4.set_xticklabels(strategies, rotation=45, ha='right')\n",
    "        ax4.grid(axis='y', alpha=0.3)\n",
    "        ax4.set_ylim([0, 1])\n",
    "        for i, v in enumerate(df_comparison['mAP'].tolist()):\n",
    "            ax4.text(i, v + 0.02, f'{v:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{plots_dir}/map.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # 5. Example-based Accuracy\n",
    "        fig5, ax5 = plt.subplots(figsize=(10, 6))\n",
    "        ax5.bar(x, df_comparison['Example-based Accuracy'].tolist(), alpha=0.8, color='purple')\n",
    "        ax5.set_xlabel('Strategy', fontsize=12)\n",
    "        ax5.set_ylabel('Example-based Accuracy', fontsize=12)\n",
    "        ax5.set_title('Hybrid Strategies: Example-based Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "        ax5.set_xticks(x)\n",
    "        ax5.set_xticklabels(strategies, rotation=45, ha='right')\n",
    "        ax5.grid(axis='y', alpha=0.3)\n",
    "        ax5.set_ylim([0, 1])\n",
    "        for i, v in enumerate(df_comparison['Example-based Accuracy'].tolist()):\n",
    "            ax5.text(i, v + 0.02, f'{v:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{plots_dir}/example_based_accuracy.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # 6. Precision@5 vs Precision@10\n",
    "        fig6, ax6 = plt.subplots(figsize=(10, 6))\n",
    "        width = 0.35\n",
    "        ax6.bar(x - width/2, df_comparison['Precision@5'].tolist(), width, label='Precision@5', alpha=0.8, color='teal')\n",
    "        ax6.bar(x + width/2, df_comparison['Precision@10'].tolist(), width, label='Precision@10', alpha=0.8, color='navy')\n",
    "        ax6.set_xlabel('Strategy', fontsize=12)\n",
    "        ax6.set_ylabel('Precision@K', fontsize=12)\n",
    "        ax6.set_title('Hybrid Strategies: Precision@5 vs Precision@10 Comparison', fontsize=14, fontweight='bold')\n",
    "        ax6.set_xticks(x)\n",
    "        ax6.set_xticklabels(strategies, rotation=45, ha='right')\n",
    "        ax6.legend()\n",
    "        ax6.grid(axis='y', alpha=0.3)\n",
    "        ax6.set_ylim([0, 1])\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{plots_dir}/precision_at_k.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # 7. Recall@5 vs Recall@10\n",
    "        fig7, ax7 = plt.subplots(figsize=(10, 6))\n",
    "        ax7.bar(x - width/2, df_comparison['Recall@5'].tolist(), width, label='Recall@5', alpha=0.8, color='coral')\n",
    "        ax7.bar(x + width/2, df_comparison['Recall@10'].tolist(), width, label='Recall@10', alpha=0.8, color='crimson')\n",
    "        ax7.set_xlabel('Strategy', fontsize=12)\n",
    "        ax7.set_ylabel('Recall@K', fontsize=12)\n",
    "        ax7.set_title('Hybrid Strategies: Recall@5 vs Recall@10 Comparison', fontsize=14, fontweight='bold')\n",
    "        ax7.set_xticks(x)\n",
    "        ax7.set_xticklabels(strategies, rotation=45, ha='right')\n",
    "        ax7.legend()\n",
    "        ax7.grid(axis='y', alpha=0.3)\n",
    "        ax7.set_ylim([0, 1])\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{plots_dir}/recall_at_k.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # Combined plot (6 panels: 3x2 layout)\n",
    "        fig, axes = plt.subplots(3, 2, figsize=(16, 18))\n",
    "        \n",
    "        # Panel 1: Micro Precision & Recall\n",
    "        width = 0.35\n",
    "        axes[0, 0].bar(x - width/2, df_comparison['Micro Precision'].tolist(), width, label='Precision', alpha=0.8, color='skyblue')\n",
    "        axes[0, 0].bar(x + width/2, df_comparison['Micro Recall'].tolist(), width, label='Recall', alpha=0.8, color='lightcoral')\n",
    "        axes[0, 0].set_xlabel('Strategy')\n",
    "        axes[0, 0].set_ylabel('Score')\n",
    "        axes[0, 0].set_title('Micro Precision & Recall Comparison')\n",
    "        axes[0, 0].set_xticks(x)\n",
    "        axes[0, 0].set_xticklabels(strategies, rotation=45, ha='right', fontsize=8)\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "        axes[0, 0].set_ylim([0, 1])\n",
    "        \n",
    "        # Panel 2: Hamming Loss\n",
    "        axes[0, 1].bar(x, df_comparison['Hamming Loss'].tolist(), alpha=0.8, color='orange')\n",
    "        axes[0, 1].set_xlabel('Strategy')\n",
    "        axes[0, 1].set_ylabel('Hamming Loss')\n",
    "        axes[0, 1].set_title('Hamming Loss (Lower is Better)')\n",
    "        axes[0, 1].set_xticks(x)\n",
    "        axes[0, 1].set_xticklabels(strategies, rotation=45, ha='right', fontsize=8)\n",
    "        axes[0, 1].grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # Panel 3: mAP\n",
    "        axes[1, 0].bar(x, df_comparison['mAP'].tolist(), alpha=0.8, color='green')\n",
    "        axes[1, 0].set_xlabel('Strategy')\n",
    "        axes[1, 0].set_ylabel('mAP')\n",
    "        axes[1, 0].set_title('Mean Average Precision (Ranking Quality)')\n",
    "        axes[1, 0].set_xticks(x)\n",
    "        axes[1, 0].set_xticklabels(strategies, rotation=45, ha='right', fontsize=8)\n",
    "        axes[1, 0].grid(axis='y', alpha=0.3)\n",
    "        axes[1, 0].set_ylim([0, 1])\n",
    "        \n",
    "        # Panel 4: Example-based Accuracy\n",
    "        axes[1, 1].bar(x, df_comparison['Example-based Accuracy'].tolist(), alpha=0.8, color='purple')\n",
    "        axes[1, 1].set_xlabel('Strategy')\n",
    "        axes[1, 1].set_ylabel('Example-based Accuracy')\n",
    "        axes[1, 1].set_title('Example-based Accuracy (Exact Match)')\n",
    "        axes[1, 1].set_xticks(x)\n",
    "        axes[1, 1].set_xticklabels(strategies, rotation=45, ha='right', fontsize=8)\n",
    "        axes[1, 1].grid(axis='y', alpha=0.3)\n",
    "        axes[1, 1].set_ylim([0, 1])\n",
    "        \n",
    "        # Panel 5: Precision@5 vs @10\n",
    "        axes[2, 0].bar(x - width/2, df_comparison['Precision@5'].tolist(), width, label='P@5', alpha=0.8, color='teal')\n",
    "        axes[2, 0].bar(x + width/2, df_comparison['Precision@10'].tolist(), width, label='P@10', alpha=0.8, color='navy')\n",
    "        axes[2, 0].set_xlabel('Strategy')\n",
    "        axes[2, 0].set_ylabel('Precision@K')\n",
    "        axes[2, 0].set_title('Precision@5 vs Precision@10')\n",
    "        axes[2, 0].set_xticks(x)\n",
    "        axes[2, 0].set_xticklabels(strategies, rotation=45, ha='right', fontsize=8)\n",
    "        axes[2, 0].legend()\n",
    "        axes[2, 0].grid(axis='y', alpha=0.3)\n",
    "        axes[2, 0].set_ylim([0, 1])\n",
    "        \n",
    "        # Panel 6: Recall@5 vs @10\n",
    "        axes[2, 1].bar(x - width/2, df_comparison['Recall@5'].tolist(), width, label='R@5', alpha=0.8, color='coral')\n",
    "        axes[2, 1].bar(x + width/2, df_comparison['Recall@10'].tolist(), width, label='R@10', alpha=0.8, color='crimson')\n",
    "        axes[2, 1].set_xlabel('Strategy')\n",
    "        axes[2, 1].set_ylabel('Recall@K')\n",
    "        axes[2, 1].set_title('Recall@5 vs Recall@10')\n",
    "        axes[2, 1].set_xticks(x)\n",
    "        axes[2, 1].set_xticklabels(strategies, rotation=45, ha='right', fontsize=8)\n",
    "        axes[2, 1].legend()\n",
    "        axes[2, 1].grid(axis='y', alpha=0.3)\n",
    "        axes[2, 1].set_ylim([0, 1])\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        combined_plot_path = f'{plots_dir}/hybrid_strategies_comparison_all.png'\n",
    "        plt.savefig(combined_plot_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"\\nüìä Plots saved:\")\n",
    "        print(f\"    - {plots_dir}/hybrid_strategies_comparison_all.png (combined 6-panel)\")\n",
    "        print(f\"    - {plots_dir}/micro_precision.png\")\n",
    "        print(f\"    - {plots_dir}/micro_recall.png\")\n",
    "        print(f\"    - {plots_dir}/hamming_loss.png\")\n",
    "        print(f\"    - {plots_dir}/map.png\")\n",
    "        print(f\"    - {plots_dir}/example_based_accuracy.png\")\n",
    "        print(f\"    - {plots_dir}/precision_at_k.png (P@5 vs P@10)\")\n",
    "        print(f\"    - {plots_dir}/recall_at_k.png (R@5 vs R@10)\")\n",
    "    \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c58a4da",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üíæ SAVE RESULTS\n",
    "\n",
    "Sonu√ßlarƒ± kaydetmek i√ßin bu h√ºcreyi √ßalƒ±≈ütƒ±rƒ±n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652cd657",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(all_test_results) == 0:\n",
    "    print(\"‚ö†Ô∏è Kaydedilecek sonu√ß yok!\")\n",
    "else:\n",
    "    # Helper function to convert numpy arrays to lists\n",
    "    def convert_to_serializable(obj):\n",
    "        \"\"\"Recursively convert numpy arrays to lists for JSON serialization.\"\"\"\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        elif isinstance(obj, dict):\n",
    "            return {key: convert_to_serializable(value) for key, value in obj.items()}\n",
    "        elif isinstance(obj, list):\n",
    "            return [convert_to_serializable(item) for item in obj]\n",
    "        elif isinstance(obj, (np.integer, np.floating)):\n",
    "            return float(obj)\n",
    "        else:\n",
    "            return obj\n",
    "    \n",
    "    # Save results to JSON\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    results_file = OUTPUT_DIR / f'strategy_comparison_{timestamp}.json'\n",
    "    \n",
    "    # Convert results to JSON-serializable format\n",
    "    serializable_results = convert_to_serializable(all_test_results)\n",
    "    \n",
    "    with open(results_file, 'w') as f:\n",
    "        json.dump(serializable_results, f, indent=2)\n",
    "    \n",
    "    print(f\"‚úÖ Sonu√ßlar kaydedildi: {results_file}\")\n",
    "    \n",
    "    # Save comparison table to CSV (same format as comparison cell)\n",
    "    comparison_data = []\n",
    "    for strategy_name, data in all_test_results.items():\n",
    "        results = data['results']\n",
    "        comparison_data.append({\n",
    "            'Strategy': strategy_name,\n",
    "            'Training_Time_min': data.get('training_time_min', 0),\n",
    "            'Micro_Precision': results.get('micro_precision', 0),\n",
    "            'Micro_Recall': results.get('micro_recall', 0),\n",
    "            'Micro_F1': results.get('micro_f1', 0),\n",
    "            'mAP': results.get('mean_average_precision', 0),\n",
    "            'Hamming_Loss': results.get('hamming_loss', 0),\n",
    "            'Example_based_Accuracy': results.get('example_based_accuracy', 0),\n",
    "            'Precision_at_5': results.get('precision_at_5', 0),\n",
    "            'Recall_at_5': results.get('recall_at_5', 0),\n",
    "            'Precision_at_10': results.get('precision_at_10', 0),\n",
    "            'Recall_at_10': results.get('recall_at_10', 0)\n",
    "        })\n",
    "    \n",
    "    df_comparison = pd.DataFrame(comparison_data)\n",
    "    csv_file = OUTPUT_DIR / f'strategy_comparison_{timestamp}.csv'\n",
    "    df_comparison.to_csv(csv_file, index=False)\n",
    "    \n",
    "    print(f\"‚úÖ CSV tablosu kaydedildi: {csv_file}\")\n",
    "    \n",
    "    # Collect all plot files to download\n",
    "    files_to_download = [results_file, csv_file]\n",
    "    \n",
    "    # Check if plots exist\n",
    "    plots_dir = Path('outputs/hybrid_strategies_plots')\n",
    "    if plots_dir.exists():\n",
    "        plot_files = [\n",
    "            'hybrid_strategies_comparison_all.png',\n",
    "            'micro_precision.png',\n",
    "            'micro_recall.png',\n",
    "            'hamming_loss.png',\n",
    "            'map.png',\n",
    "            'example_based_accuracy.png',\n",
    "            'precision_at_k.png',\n",
    "            'recall_at_k.png'\n",
    "        ]\n",
    "        \n",
    "        for plot_file in plot_files:\n",
    "            plot_path = plots_dir / plot_file\n",
    "            if plot_path.exists():\n",
    "                files_to_download.append(plot_path)\n",
    "                print(f\"‚úÖ Plot hazƒ±r: {plot_path}\")\n",
    "    \n",
    "    # Download files if on Colab\n",
    "    try:\n",
    "        from google.colab import files\n",
    "        for file_path in files_to_download:\n",
    "            files.download(str(file_path))\n",
    "        print(f\"\\n‚úÖ {len(files_to_download)} dosya indirildi:\")\n",
    "        print(f\"   - 1 JSON (results)\")\n",
    "        print(f\"   - 1 CSV (comparison table)\")\n",
    "        print(f\"   - {len(files_to_download) - 2} PNG (plots)\")\n",
    "    except ImportError:\n",
    "        print(f\"\\nüí° Colab deƒüil, {len(files_to_download)} dosya kaydedildi\")\n",
    "        print(f\"   Dosyalar: outputs/ klas√∂r√ºnde\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93aeaaa",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìù NOTLAR VE √ñNERƒ∞LER\n",
    "\n",
    "**Model:**\n",
    "- **CTI-BERT** (ibm-research/CTI-BERT): Cyber Threat Intelligence'a √∂zel BERT\n",
    "- G√ºvenlik metinlerinde genel BERT'ten daha iyi performans\n",
    "- Pre-trained on security-specific corpus\n",
    "- 768-dimensional embeddings\n",
    "\n",
    "**Dataset:**\n",
    "- Single source: tumeteor/Security-TTP-Mapping (14.9k train + 2.6k test)\n",
    "- 499 unique MITRE ATT&CK technique labels\n",
    "- Tutarlƒ± label format, y√ºksek kalite\n",
    "- Imbalanced distribution (tail TTP'ler az g√∂r√ºl√ºr)\n",
    "\n",
    "---\n",
    "\n",
    "## üìä EXPERIMENT STRUCTURE OVERVIEW\n",
    "\n",
    "### **PART A: Data Augmentation (5 strategies)**\n",
    "Tail TTP'leri g√º√ßlendirmek i√ßin augmentation y√∂ntemleri:\n",
    "- **A-1:** Baseline (No Augmentation)\n",
    "- **A-2:** IoC Replacement Only\n",
    "- **A-3:** Back-translation Only\n",
    "- **A-4:** Oversampling Only (3x-10x)\n",
    "- **A-5:** Combined (All 3 methods)\n",
    "\n",
    "**Beklenen ƒ∞yile≈ütirme:** Tail TTP Recall +40-60%, Overall mAP +20-30%\n",
    "\n",
    "### **PART B Section 1: Loss Functions (4 strategies)**\n",
    "BERT end-to-end training ile farklƒ± loss fonksiyonlarƒ±:\n",
    "- **STR-1:** Baseline BCE (class imbalance handle yok)\n",
    "- **STR-2:** Weighted BCE (pos_weight ile rare label'lara odaklanƒ±r)\n",
    "- **STR-3:** Focal Loss Œ≥=2 (moderate hard example focusing)\n",
    "- **STR-4:** Focal Loss Œ≥=5 (strong hard example focusing)\n",
    "\n",
    "**En Ba≈üarƒ±lƒ±:** Weighted BCE (frequency-based pos_weight)\n",
    "\n",
    "### **PART B Section 2: Capacity Testing (1 strategy, 5 variants)**\n",
    "Model kapasitesini test et:\n",
    "- **STR-5:** Top-K Label Analysis (K=5, 10, 20, 50, 100)\n",
    "- Her K i√ßin baseline BCE kullanƒ±r\n",
    "- Model'in farklƒ± label sayƒ±larƒ±ndaki performansƒ±nƒ± g√∂sterir\n",
    "\n",
    "**S√ºre:** ~2-2.5 saat (5 model)\n",
    "\n",
    "### **PART C: Hybrid Strategies (10 strategies = 2 loss √ó 5 methods)**\n",
    "En iyi loss fonksiyonlarƒ±yla t√ºm classification y√∂ntemlerini test et:\n",
    "\n",
    "**Loss Functions (Part B'den se√ßildi):**\n",
    "1. **Weighted BCE** - Frequency-based weights\n",
    "2. **Focal Loss Œ≥=5** - Strong hard example focusing\n",
    "\n",
    "**Classification Methods:**\n",
    "1. **ClassifierChain** - Label dependencies (sequential)\n",
    "2. **ExtraTreesClassifier** - Extremely randomized trees\n",
    "3. **RandomForestClassifier** - Ensemble of decision trees\n",
    "4. **AttentionXML** - Multi-label attention mechanism (NeurIPS 2019)\n",
    "5. **LightXML** - Dynamic negative sampling + label embeddings (AAAI 2021)\n",
    "\n",
    "**Strategy Matrix:**\n",
    "```\n",
    "                    Chain  ExtraTrees  RandomForest  AttentionXML  LightXML\n",
    "Weighted BCE        C-1    C-2         C-3           C-4           C-5\n",
    "Focal Loss Œ≥=5      C-6    C-7         C-8           C-9           C-10\n",
    "```\n",
    "\n",
    "**Toplam S√ºre:** ~7.5-10 saat (10 strateji √ó 45-60 dakika)\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Method Detaylarƒ±\n",
    "\n",
    "### **ClassifierChain:**\n",
    "- **Nasƒ±l √ßalƒ±≈üƒ±r:** BERT embeddings + 499 zincirleme binary classifier\n",
    "- **Avantaj:** Label dependencies modelleyebilir\n",
    "- **Dezavantaj:** Training s√ºresi uzun, chain order'a baƒüƒ±mlƒ±\n",
    "- **Progress:** Tek satƒ±r progress bar (499 line spam yerine)\n",
    "\n",
    "### **ExtraTrees vs RandomForest:**\n",
    "- **ExtraTrees:** Daha hƒ±zlƒ±, random splits, less overfitting\n",
    "- **RandomForest:** Optimal splits, biraz daha y√ºksek accuracy\n",
    "- **ƒ∞kisi de:** Ensemble methods, class_weight='balanced'\n",
    "\n",
    "### **AttentionXML:**\n",
    "- Her label i√ßin √∂zel attention weights\n",
    "- Label-specific text regions'a odaklanƒ±r\n",
    "- End-to-end BERT fine-tuning\n",
    "- Simplified implementation (499 labels i√ßin)\n",
    "\n",
    "### **LightXML:**\n",
    "- Two-stage: Label grouping (50 groups) ‚Üí Candidate ranking\n",
    "- Dynamic negative sampling\n",
    "- Label embeddings (128-dim semantic space)\n",
    "- Efficient for large label spaces\n",
    "\n",
    "---\n",
    "\n",
    "## üìà Ba≈üarƒ± Kriterleri\n",
    "\n",
    "**SOC Analyst Perspektifi:**\n",
    "- **mAP > 0.20** ‚Üí ƒ∞yi sƒ±ralama kalitesi (EN √ñNEMLƒ∞!)\n",
    "- **Recall@5 > 0.30** ‚Üí Top-5 tahmin i√ßinde doƒüru TTP'leri bulma\n",
    "- **Micro F1 > 0.15** ‚Üí Genel performans (threshold-based)\n",
    "- **Hamming Loss < 0.10** ‚Üí Az yanlƒ±≈ü tahmin\n",
    "\n",
    "**Metrikler:**\n",
    "- **mAP:** Sƒ±ralama kalitesi - doƒüru TTP'leri listenin tepesine koyma ba≈üarƒ±sƒ±\n",
    "- **Recall@5:** Top-5 tahmin i√ßinde ka√ß doƒüru TTP var\n",
    "- **Precision@5:** Top-5 tahminlerin ka√ßƒ± doƒüru\n",
    "- **Micro F1:** T√ºm label'lar √ºzerinden genel performans\n",
    "- **Hamming Loss:** Yanlƒ±≈ü tahmin edilen label oranƒ± (d√º≈ü√ºk=iyi)\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Test √ñnerileri\n",
    "\n",
    "**Sƒ±ralama (Optimal):**\n",
    "1. **PART A** ‚Üí En iyi augmentation'ƒ± bul (tail TTP'ler i√ßin kritik)\n",
    "2. **PART B** ‚Üí En iyi loss function'ƒ± bul (weighted BCE genelde kazanƒ±r)\n",
    "3. **PART C** ‚Üí En iyi kombinasyonu bul (loss √ó classification)\n",
    "\n",
    "**Baƒüƒ±msƒ±z Test (Esnek):**\n",
    "- Her part birbirinden baƒüƒ±msƒ±z √ßalƒ±≈ütƒ±rƒ±labilir\n",
    "- Tek gereksinim: Her part'ƒ±n ba≈üƒ±ndaki import h√ºcresini √ßalƒ±≈ütƒ±r\n",
    "- ƒ∞stediƒüin sƒ±rayla test edebilirsin\n",
    "\n",
    "**Hƒ±zlƒ± Test:**\n",
    "- A-1 (Baseline) + STR-2 (Weighted BCE) ‚Üí ~45 dakika\n",
    "- C-1 (Weighted BCE + Chain) ‚Üí ~50 dakika\n",
    "- Toplam: ~1.5 saat (hƒ±zlƒ± baseline)\n",
    "\n",
    "**Tam Test:**\n",
    "- Part A (5 strateji) ‚Üí ~4-5 saat\n",
    "- Part B (4+5 strateji) ‚Üí ~5-6 saat\n",
    "- Part C (10 strateji) ‚Üí ~7.5-10 saat\n",
    "- Toplam: ~17-21 saat\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Hatƒ±rlatmalar\n",
    "\n",
    "- Her strateji h√ºcresi **baƒüƒ±msƒ±zdƒ±r**, istediƒüiniz sƒ±rada √ßalƒ±≈ütƒ±rabilirsiniz\n",
    "- Sonu√ßlar `all_test_results` dictionary'sinde saklanƒ±r\n",
    "- Comparison h√ºcresini dilediƒüiniz zaman √ßalƒ±≈ütƒ±rƒ±p ara sonu√ßlara bakabilirsiniz\n",
    "- CTI-BERT ilk indirilirken cache'lenir (~500MB), sonraki √ßalƒ±≈ütƒ±rmalar hƒ±zlƒ±dƒ±r\n",
    "- **ClassifierChain verbose output d√ºzeltildi** - 499 line yerine tek progress bar\n",
    "- **AttentionXML ve LightXML eklendi** - XMC (extreme multi-label classification) i√ßin\n",
    "- **Progress bars** t√ºm training loop'larda aktif - ger√ßek zamanlƒ± ilerleme takibi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
