{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fe9927e",
   "metadata": {},
   "source": [
    "# üß™ Class Imbalance Strategies - CTI-BERT ile Adƒ±m Adƒ±m Test\n",
    "\n",
    "Her stratejiyi **baƒüƒ±msƒ±z** olarak test et, sonu√ßlarƒ± kar≈üƒ±la≈ütƒ±r.\n",
    "\n",
    "## ü§ñ Model: CTI-BERT (IBM Research)\n",
    "- **ibm-research/CTI-BERT**: Cyber Threat Intelligence verisiyle √∂nceden eƒüitilmi≈ü domain-specific BERT\n",
    "- **Avantaj**: G√ºvenlik ve CTI metinlerini anlamada genel BERT'ten daha iyi\n",
    "- **Reference**: https://huggingface.co/ibm-research/CTI-BERT\n",
    "\n",
    "## üìä Dataset: Single Source\n",
    "- **tumeteor/Security-TTP-Mapping** (14.9k train + 2.6k test)\n",
    "- **√ñzellik**: MITRE ATT&CK technique ID'leri (T-codes)\n",
    "- **Avantaj**: Tutarlƒ± label format, y√ºksek kalite\n",
    "\n",
    "## üí° Kullanƒ±m:\n",
    "1. Setup h√ºcrelerini √ßalƒ±≈ütƒ±r (1-4)\n",
    "2. ƒ∞stediƒüin stratejiyi se√ß (Strategy 1-5)\n",
    "3. Sadece o stratejiyi √ßalƒ±≈ütƒ±r (~30-45 dk)\n",
    "4. Results Comparison h√ºcresini √ßalƒ±≈ütƒ±r\n",
    "5. Ba≈üka bir strateji test et\n",
    "\n",
    "**Avantaj:** Her stratejiyi ayrƒ± ayrƒ± test edip sonu√ßlarƒ± g√∂rebilirsin!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6204197",
   "metadata": {},
   "source": [
    "## üîß SETUP (Her zaman √ßalƒ±≈ütƒ±r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5620b520",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4dc270e4",
   "metadata": {},
   "source": [
    "### 1. Colab Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fa0e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"‚úÖ Google Colab ortamƒ± tespit edildi\")\n",
    "    \n",
    "    import torch\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"‚úÖ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"   VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  GPU bulunamadƒ±! Runtime > Change runtime type > GPU se√ßin\")\n",
    "    \n",
    "    print(\"\\nüì• Proje indiriliyor...\")\n",
    "    !rm -rf Mitre_Attack_TTP_Mapping\n",
    "    !git clone https://github.com/Aliekinozcetin/Mitre_Attack_TTP_Mapping.git\n",
    "    os.chdir('Mitre_Attack_TTP_Mapping')\n",
    "    print(f\"‚úÖ √áalƒ±≈üma dizini: {os.getcwd()}\")\n",
    "    \n",
    "    print(\"\\nüì¶ Paketler y√ºkleniyor...\")\n",
    "    !pip install -q torch transformers datasets scikit-learn pandas tqdm matplotlib seaborn\n",
    "    print(\"‚úÖ T√ºm paketler y√ºklendi\")\n",
    "    \n",
    "    # HuggingFace baƒülantƒ± optimizasyonu\n",
    "    print(\"\\nüîß HuggingFace cache ayarlarƒ±...\")\n",
    "    \n",
    "    # Create cache directory\n",
    "    cache_dir = '/content/hf_cache'\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    \n",
    "    # Set environment variables\n",
    "    os.environ['HF_HOME'] = cache_dir\n",
    "    os.environ['TRANSFORMERS_CACHE'] = cache_dir\n",
    "    os.environ['HF_DATASETS_CACHE'] = cache_dir\n",
    "    os.environ['HF_HUB_DOWNLOAD_TIMEOUT'] = '600'  # 10 minutes\n",
    "    os.environ['CURL_CA_BUNDLE'] = ''\n",
    "    os.environ['HF_ENDPOINT'] = 'https://huggingface.co'\n",
    "    os.environ['HF_HUB_ENABLE_HF_TRANSFER'] = '1'  # Faster downloads\n",
    "    \n",
    "    print(f\"‚úÖ Cache dizini olu≈üturuldu: {cache_dir}\")\n",
    "    print(f\"   Timeout: 10 dakika\")\n",
    "    \n",
    "    # Test HuggingFace connection\n",
    "    try:\n",
    "        from huggingface_hub import HfApi\n",
    "        api = HfApi()\n",
    "        print(\"\\nüì° HuggingFace baƒülantƒ± testi...\")\n",
    "        info = api.model_info(\"ibm-research/CTI-BERT\", timeout=30)\n",
    "        print(f\"‚úÖ Model eri≈üilebilir: {info.modelId}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Baƒülantƒ± uyarƒ±sƒ±: {str(e)[:100]}\")\n",
    "        print(\"   Model indirme denemeye devam edilecek...\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è  Yerel ortamda √ßalƒ±≈üƒ±yorsunuz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3044287e",
   "metadata": {},
   "source": [
    "### 2. Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4696cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear import cache\n",
    "import sys\n",
    "if 'src.data_loader' in sys.modules:\n",
    "    del sys.modules['src.data_loader']\n",
    "if 'src.model' in sys.modules:\n",
    "    del sys.modules['src.model']\n",
    "if 'src.train' in sys.modules:\n",
    "    del sys.modules['src.train']\n",
    "if 'src.evaluate' in sys.modules:\n",
    "    del sys.modules['src.evaluate']\n",
    "if 'src.strategies' in sys.modules:\n",
    "    del sys.modules['src.strategies']\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Import prepare_data first\n",
    "from src.data_loader import prepare_data\n",
    "\n",
    "# CRITICAL FIX: Define the function HERE if import fails\n",
    "try:\n",
    "    from src.data_loader import load_datasets_and_prepare_dataloaders\n",
    "    print(\"‚úÖ Fonksiyon GitHub'dan import edildi\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è  GitHub import ba≈üarƒ±sƒ±z, fonksiyon notebook'ta tanƒ±mlanƒ±yor...\")\n",
    "    \n",
    "    def load_datasets_and_prepare_dataloaders(\n",
    "        model_name: str = \"ibm-research/CTI-BERT\",\n",
    "        batch_size: int = 16,\n",
    "        max_length: int = 512,\n",
    "        use_hybrid: bool = True,\n",
    "        dataset_name: str = \"tumeteor/Security-TTP-Mapping\"\n",
    "    ):\n",
    "        \"\"\"Wrapper for prepare_data - notebook fallback version.\"\"\"\n",
    "        data = prepare_data(\n",
    "            model_name=model_name,\n",
    "            max_length=max_length,\n",
    "            use_hybrid=use_hybrid,\n",
    "            dataset_name=dataset_name\n",
    "        )\n",
    "        \n",
    "        train_loader = DataLoader(\n",
    "            data['train_dataset'],\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True\n",
    "        )\n",
    "        \n",
    "        return (\n",
    "            train_loader,\n",
    "            None,\n",
    "            data['test_dataset'],\n",
    "            data['label_list']\n",
    "        )\n",
    "    \n",
    "    print(\"‚úÖ Fallback fonksiyon tanƒ±mlandƒ± (CTI-BERT)\")\n",
    "\n",
    "from src.model import load_model\n",
    "from src.train import train_model\n",
    "from src.evaluate import evaluate_model\n",
    "from src.strategies import get_strategy_config\n",
    "\n",
    "print(\"‚úÖ Mod√ºller y√ºklendi\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b82a7c4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîß CONFIGURATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58aaecde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base training configuration\n",
    "BASE_CONFIG = {\n",
    "    'model_name': 'ibm-research/CTI-BERT',  # CTI domain-specific BERT\n",
    "    'batch_size': 16,\n",
    "    'learning_rate': 2e-5,\n",
    "    'num_epochs': 3,\n",
    "    'max_length': 128,\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "}\n",
    "\n",
    "# Output directory\n",
    "OUTPUT_DIR = Path('outputs')\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Store results from all strategies\n",
    "all_test_results = {}\n",
    "\n",
    "print(\"‚úÖ Konfig√ºrasyon ayarlandƒ±\")\n",
    "print(f\"Model: {BASE_CONFIG['model_name']}\")\n",
    "print(f\"Device: {BASE_CONFIG['device']}\")\n",
    "print(f\"Output: {OUTPUT_DIR.absolute()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c082e5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä DATA LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3310ac99",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üì• Veri y√ºkleniyor...\")\n",
    "print(\"üì¶ Dataset: tumeteor/Security-TTP-Mapping (Single Source)\")\n",
    "print(f\"ü§ñ Model: {BASE_CONFIG['model_name']}\")\n",
    "print(\"\")\n",
    "\n",
    "# Use single dataset: tumeteor only\n",
    "train_dataloader, val_dataloader, test_dataset, label_names = load_datasets_and_prepare_dataloaders(\n",
    "    model_name=BASE_CONFIG['model_name'],\n",
    "    batch_size=BASE_CONFIG['batch_size'],\n",
    "    max_length=BASE_CONFIG['max_length'],\n",
    "    use_hybrid=False,  # Single dataset: tumeteor only\n",
    "    dataset_name=\"tumeteor/Security-TTP-Mapping\"\n",
    ")\n",
    "\n",
    "# Get train_dataset from dataloader for strategies\n",
    "train_dataset = train_dataloader.dataset\n",
    "\n",
    "# Create test dataloader\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BASE_CONFIG['batch_size'],\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "num_labels = len(label_names)\n",
    "\n",
    "# Create data dict for backward compatibility\n",
    "data = {\n",
    "    'train_dataset': train_dataset,\n",
    "    'test_dataset': test_dataset,\n",
    "    'label_list': label_names,\n",
    "    'num_labels': num_labels\n",
    "}\n",
    "\n",
    "print(f\"‚úÖ Veri y√ºklendi\")\n",
    "print(f\"   Train batches: {len(train_dataloader)}\")\n",
    "print(f\"   Test batches: {len(test_dataloader)}\")\n",
    "print(f\"   Toplam label sayƒ±sƒ±: {num_labels}\")\n",
    "print(f\"   ƒ∞lk 5 label: {label_names[:5]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de26a69",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üß™ STRATEGY TESTING\n",
    "\n",
    "Her strateji baƒüƒ±msƒ±z olarak test edilebilir. ƒ∞stediƒüiniz h√ºcreyi √ßalƒ±≈ütƒ±rƒ±n."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990c7ec3",
   "metadata": {},
   "source": [
    "### Strategy 1: Baseline (Standard BCE Loss)\n",
    "\n",
    "**A√ßƒ±klama:** Standart Binary Cross-Entropy loss kullanƒ±r. Referans performans i√ßin baseline.\n",
    "\n",
    "**√ñnerilen Kullanƒ±m:** ƒ∞lk olarak bu stratejiyi √ßalƒ±≈ütƒ±rƒ±n (30-45 dakika)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfc0ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy_name = \"baseline\"\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"üß™ STRATEGY 1: Baseline BCE\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# Get strategy configuration\n",
    "strategy_config = get_strategy_config(\n",
    "    strategy_name=strategy_name,\n",
    "    train_dataset=data['train_dataset'],\n",
    "    num_labels=num_labels,\n",
    "    label_list=label_names\n",
    ")\n",
    "\n",
    "# Create DataLoader\n",
    "if strategy_config['custom_dataloader'] is not None:\n",
    "    strategy_train_dataloader = strategy_config['custom_dataloader'](BASE_CONFIG['batch_size'])\n",
    "else:\n",
    "    strategy_train_dataloader = DataLoader(\n",
    "        strategy_config['dataset'],\n",
    "        batch_size=BASE_CONFIG['batch_size'],\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "print(\"üìã Konfig√ºrasyon:\")\n",
    "print(f\"   Strategy: {strategy_config['name']}\")\n",
    "print(f\"   Description: {strategy_config['description']}\")\n",
    "print(f\"   Num labels: {strategy_config['num_labels']}\")\n",
    "print(f\"   Focal loss: {strategy_config['use_focal_loss']}\")\n",
    "if strategy_config['pos_weight'] is not None:\n",
    "    print(f\"   Pos weight: min={strategy_config['pos_weight'].min():.2f}, max={strategy_config['pos_weight'].max():.2f}\")\n",
    "\n",
    "# Load model\n",
    "print(\"\\nüîß Model y√ºkleniyor...\")\n",
    "model = load_model(\n",
    "    model_name=BASE_CONFIG['model_name'],\n",
    "    num_labels=strategy_config['num_labels'],\n",
    "    device=BASE_CONFIG['device'],\n",
    "    use_focal_loss=strategy_config['use_focal_loss'],\n",
    "    focal_alpha=strategy_config.get('focal_alpha', 0.25),\n",
    "    focal_gamma=strategy_config.get('focal_gamma', 2.0),\n",
    "    pos_weight=strategy_config['pos_weight']\n",
    ")\n",
    "\n",
    "# Train model\n",
    "print(\"\\nüöÄ Eƒüitim ba≈ülƒ±yor...\")\n",
    "training_history = train_model(\n",
    "    model=model,\n",
    "    train_dataloader=strategy_train_dataloader,\n",
    "    num_epochs=BASE_CONFIG['num_epochs'],\n",
    "    learning_rate=BASE_CONFIG['learning_rate'],\n",
    "    device=BASE_CONFIG['device']\n",
    ")\n",
    "\n",
    "# Evaluate model\n",
    "print(\"\\nüìä Test seti deƒüerlendiriliyor...\")\n",
    "test_results = evaluate_model(\n",
    "    model=model,\n",
    "    test_dataset=data['test_dataset'],\n",
    "    batch_size=BASE_CONFIG['batch_size'],\n",
    "    device=BASE_CONFIG['device']\n",
    ")\n",
    "\n",
    "# Store results\n",
    "all_test_results[strategy_name] = {\n",
    "    'config': strategy_config['name'],\n",
    "    'description': strategy_config['description'],\n",
    "    'results': test_results\n",
    "}\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"‚úÖ STRATEGY 1 TAMAMLANDI: {strategy_name}\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nüìà Sonu√ßlar:\")\n",
    "for metric, value in test_results.items():\n",
    "    if isinstance(value, (int, float)):\n",
    "        print(f\"   {metric}: {value:.4f}\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bd655b",
   "metadata": {},
   "source": [
    "### Strategy 2: Weighted BCE Loss\n",
    "\n",
    "**A√ßƒ±klama:** Her label i√ßin frekans bazlƒ± aƒüƒ±rlƒ±k hesaplar (pos_weight=458 for rare labels). Class imbalance i√ßin en etkili y√∂ntem.\n",
    "\n",
    "**√ñnerilen Kullanƒ±m:** Baseline'dan sonra bu stratejiyi test edin. F1 > 0.15 ise, diƒüer stratejileri test etmeye gerek yok."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a054e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy_name = \"weighted\"\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"üß™ STRATEGY 2: Weighted BCE\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# Get strategy configuration\n",
    "strategy_config = get_strategy_config(\n",
    "    strategy_name=strategy_name,\n",
    "    train_dataset=data['train_dataset'],\n",
    "    num_labels=num_labels,\n",
    "    label_list=label_names\n",
    ")\n",
    "\n",
    "# Create DataLoader\n",
    "if strategy_config['custom_dataloader'] is not None:\n",
    "    strategy_train_dataloader = strategy_config['custom_dataloader'](BASE_CONFIG['batch_size'])\n",
    "else:\n",
    "    strategy_train_dataloader = DataLoader(\n",
    "        strategy_config['dataset'],\n",
    "        batch_size=BASE_CONFIG['batch_size'],\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "print(\"üìã Konfig√ºrasyon:\")\n",
    "print(f\"   Strategy: {strategy_config['name']}\")\n",
    "print(f\"   Description: {strategy_config['description']}\")\n",
    "print(f\"   Num labels: {strategy_config['num_labels']}\")\n",
    "print(f\"   Focal loss: {strategy_config['use_focal_loss']}\")\n",
    "if strategy_config['pos_weight'] is not None:\n",
    "    print(f\"   Pos weight: min={strategy_config['pos_weight'].min():.2f}, max={strategy_config['pos_weight'].max():.2f}\")\n",
    "\n",
    "# Load model\n",
    "print(\"\\nüîß Model y√ºkleniyor...\")\n",
    "model = load_model(\n",
    "    model_name=BASE_CONFIG['model_name'],\n",
    "    num_labels=strategy_config['num_labels'],\n",
    "    device=BASE_CONFIG['device'],\n",
    "    use_focal_loss=strategy_config['use_focal_loss'],\n",
    "    focal_alpha=strategy_config.get('focal_alpha', 0.25),\n",
    "    focal_gamma=strategy_config.get('focal_gamma', 2.0),\n",
    "    pos_weight=strategy_config['pos_weight']\n",
    ")\n",
    "\n",
    "# Train model\n",
    "print(\"\\nüöÄ Eƒüitim ba≈ülƒ±yor...\")\n",
    "training_history = train_model(\n",
    "    model=model,\n",
    "    train_dataloader=strategy_train_dataloader,\n",
    "    num_epochs=BASE_CONFIG['num_epochs'],\n",
    "    learning_rate=BASE_CONFIG['learning_rate'],\n",
    "    device=BASE_CONFIG['device']\n",
    ")\n",
    "\n",
    "# Evaluate model\n",
    "print(\"\\nüìä Test seti deƒüerlendiriliyor...\")\n",
    "test_results = evaluate_model(\n",
    "    model=model,\n",
    "    test_dataset=data['test_dataset'],\n",
    "    batch_size=BASE_CONFIG['batch_size'],\n",
    "    device=BASE_CONFIG['device']\n",
    ")\n",
    "\n",
    "# Store results\n",
    "all_test_results[strategy_name] = {\n",
    "    'config': strategy_config['name'],\n",
    "    'description': strategy_config['description'],\n",
    "    'results': test_results\n",
    "}\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"‚úÖ STRATEGY 2 TAMAMLANDI: {strategy_name}\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nüìà Sonu√ßlar:\")\n",
    "for metric, value in test_results.items():\n",
    "    if isinstance(value, (int, float)):\n",
    "        print(f\"   {metric}: {value:.4f}\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0028aa98",
   "metadata": {},
   "source": [
    "### Strategy 3: Focal Loss (Œ≥=2, Œ±=0.25)\n",
    "\n",
    "**A√ßƒ±klama:** Focal Loss with moderate focusing (Œ≥=2). Hard √∂rneklere odaklanƒ±r.\n",
    "\n",
    "**√ñnerilen Kullanƒ±m:** Weighted BCE ba≈üarƒ±sƒ±z olursa deneyin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acce9512",
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy_name = \"focal_weak\"\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"üß™ STRATEGY 3: Focal Loss (Œ≥=2)\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# Get strategy configuration\n",
    "strategy_config = get_strategy_config(\n",
    "    strategy_name=strategy_name,\n",
    "    train_dataset=data['train_dataset'],\n",
    "    num_labels=num_labels,\n",
    "    label_list=label_names\n",
    ")\n",
    "\n",
    "# Create DataLoader\n",
    "if strategy_config['custom_dataloader'] is not None:\n",
    "    strategy_train_dataloader = strategy_config['custom_dataloader'](BASE_CONFIG['batch_size'])\n",
    "else:\n",
    "    strategy_train_dataloader = DataLoader(\n",
    "        strategy_config['dataset'],\n",
    "        batch_size=BASE_CONFIG['batch_size'],\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "print(\"üìã Konfig√ºrasyon:\")\n",
    "print(f\"   Strategy: {strategy_config['name']}\")\n",
    "print(f\"   Description: {strategy_config['description']}\")\n",
    "print(f\"   Num labels: {strategy_config['num_labels']}\")\n",
    "print(f\"   Focal loss: {strategy_config['use_focal_loss']}\")\n",
    "if strategy_config['use_focal_loss']:\n",
    "    print(f\"   Focal alpha: {strategy_config.get('focal_alpha')}\")\n",
    "    print(f\"   Focal gamma: {strategy_config.get('focal_gamma')}\")\n",
    "\n",
    "# Load model\n",
    "print(\"\\nüîß Model y√ºkleniyor...\")\n",
    "model = load_model(\n",
    "    model_name=BASE_CONFIG['model_name'],\n",
    "    num_labels=strategy_config['num_labels'],\n",
    "    device=BASE_CONFIG['device'],\n",
    "    use_focal_loss=strategy_config['use_focal_loss'],\n",
    "    focal_alpha=strategy_config.get('focal_alpha', 0.25),\n",
    "    focal_gamma=strategy_config.get('focal_gamma', 2.0),\n",
    "    pos_weight=strategy_config['pos_weight']\n",
    ")\n",
    "\n",
    "# Train model\n",
    "print(\"\\nüöÄ Eƒüitim ba≈ülƒ±yor...\")\n",
    "training_history = train_model(\n",
    "    model=model,\n",
    "    train_dataloader=strategy_train_dataloader,\n",
    "    num_epochs=BASE_CONFIG['num_epochs'],\n",
    "    learning_rate=BASE_CONFIG['learning_rate'],\n",
    "    device=BASE_CONFIG['device']\n",
    ")\n",
    "\n",
    "# Evaluate model\n",
    "print(\"\\nüìä Test seti deƒüerlendiriliyor...\")\n",
    "test_results = evaluate_model(\n",
    "    model=model,\n",
    "    test_dataset=data['test_dataset'],\n",
    "    batch_size=BASE_CONFIG['batch_size'],\n",
    "    device=BASE_CONFIG['device']\n",
    ")\n",
    "\n",
    "# Store results\n",
    "all_test_results[strategy_name] = {\n",
    "    'config': strategy_config['name'],\n",
    "    'description': strategy_config['description'],\n",
    "    'results': test_results\n",
    "}\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"‚úÖ STRATEGY 3 TAMAMLANDI: {strategy_name}\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nüìà Sonu√ßlar:\")\n",
    "for metric, value in test_results.items():\n",
    "    if isinstance(value, (int, float)):\n",
    "        print(f\"   {metric}: {value:.4f}\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8b7263",
   "metadata": {},
   "source": [
    "### Strategy 4: Focal Loss (Œ≥=5, Œ±=0.25)\n",
    "\n",
    "**A√ßƒ±klama:** Focal Loss with strong focusing (Œ≥=5). √áok hard √∂rneklere odaklanƒ±r.\n",
    "\n",
    "**√ñnerilen Kullanƒ±m:** Focal Œ≥=2 ba≈üarƒ±lƒ± olursa daha g√º√ßl√º versiyon i√ßin deneyin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e88621",
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy_name = \"focal_strong\"\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"üß™ STRATEGY 4: Focal Loss (Œ≥=5)\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# Get strategy configuration\n",
    "strategy_config = get_strategy_config(\n",
    "    strategy_name=strategy_name,\n",
    "    train_dataset=data['train_dataset'],\n",
    "    num_labels=num_labels,\n",
    "    label_list=label_names\n",
    ")\n",
    "\n",
    "# Create DataLoader\n",
    "if strategy_config['custom_dataloader'] is not None:\n",
    "    strategy_train_dataloader = strategy_config['custom_dataloader'](BASE_CONFIG['batch_size'])\n",
    "else:\n",
    "    strategy_train_dataloader = DataLoader(\n",
    "        strategy_config['dataset'],\n",
    "        batch_size=BASE_CONFIG['batch_size'],\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "print(\"üìã Konfig√ºrasyon:\")\n",
    "print(f\"   Strategy: {strategy_config['name']}\")\n",
    "print(f\"   Description: {strategy_config['description']}\")\n",
    "print(f\"   Num labels: {strategy_config['num_labels']}\")\n",
    "print(f\"   Focal loss: {strategy_config['use_focal_loss']}\")\n",
    "if strategy_config['use_focal_loss']:\n",
    "    print(f\"   Focal alpha: {strategy_config.get('focal_alpha')}\")\n",
    "    print(f\"   Focal gamma: {strategy_config.get('focal_gamma')}\")\n",
    "\n",
    "# Load model\n",
    "print(\"\\nüîß Model y√ºkleniyor...\")\n",
    "model = load_model(\n",
    "    model_name=BASE_CONFIG['model_name'],\n",
    "    num_labels=strategy_config['num_labels'],\n",
    "    device=BASE_CONFIG['device'],\n",
    "    use_focal_loss=strategy_config['use_focal_loss'],\n",
    "    focal_alpha=strategy_config.get('focal_alpha', 0.25),\n",
    "    focal_gamma=strategy_config.get('focal_gamma', 2.0),\n",
    "    pos_weight=strategy_config['pos_weight']\n",
    ")\n",
    "\n",
    "# Train model\n",
    "print(\"\\nüöÄ Eƒüitim ba≈ülƒ±yor...\")\n",
    "training_history = train_model(\n",
    "    model=model,\n",
    "    train_dataloader=strategy_train_dataloader,\n",
    "    num_epochs=BASE_CONFIG['num_epochs'],\n",
    "    learning_rate=BASE_CONFIG['learning_rate'],\n",
    "    device=BASE_CONFIG['device']\n",
    ")\n",
    "\n",
    "# Evaluate model\n",
    "print(\"\\nüìä Test seti deƒüerlendiriliyor...\")\n",
    "test_results = evaluate_model(\n",
    "    model=model,\n",
    "    test_dataset=data['test_dataset'],\n",
    "    batch_size=BASE_CONFIG['batch_size'],\n",
    "    device=BASE_CONFIG['device']\n",
    ")\n",
    "\n",
    "# Store results\n",
    "all_test_results[strategy_name] = {\n",
    "    'config': strategy_config['name'],\n",
    "    'description': strategy_config['description'],\n",
    "    'results': test_results\n",
    "}\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"‚úÖ STRATEGY 4 TAMAMLANDI: {strategy_name}\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nüìà Sonu√ßlar:\")\n",
    "for metric, value in test_results.items():\n",
    "    if isinstance(value, (int, float)):\n",
    "        print(f\"   {metric}: {value:.4f}\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276748ea",
   "metadata": {},
   "source": [
    "# Top-K Label Analysis: Test model capacity with different label subset sizes\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"üß™ TOP-K LABEL ANALYSIS\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "from src.strategies import filter_top_k_labels\n",
    "\n",
    "# Test different K values\n",
    "k_values = [100, 50, 20, 10, 5]\n",
    "topk_results = {}\n",
    "\n",
    "for k in k_values:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üî¨ Testing Top-{k} Labels\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Filter dataset to top-k labels\n",
    "    filtered_train_ds, filtered_label_list, label_mapping = filter_top_k_labels(\n",
    "        data['train_dataset'], \n",
    "        label_names, \n",
    "        k=k\n",
    "    )\n",
    "    filtered_test_ds, _, _ = filter_top_k_labels(\n",
    "        data['test_dataset'], \n",
    "        label_names, \n",
    "        k=k\n",
    "    )\n",
    "    \n",
    "    print(f\"üìä Dataset Statistics:\")\n",
    "    print(f\"   Top-{k} labels selected\")\n",
    "    print(f\"   Train samples: {len(filtered_train_ds)}\")\n",
    "    print(f\"   Test samples: {len(filtered_test_ds)}\")\n",
    "    print(f\"   Labels: {filtered_label_list[:5]}...\")\n",
    "    \n",
    "    # Create dataloaders\n",
    "    topk_train_loader = DataLoader(\n",
    "        filtered_train_ds,\n",
    "        batch_size=BASE_CONFIG['batch_size'],\n",
    "        shuffle=True\n",
    "    )\n",
    "    topk_test_loader = DataLoader(\n",
    "        filtered_test_ds,\n",
    "        batch_size=BASE_CONFIG['batch_size'],\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    # Load model for this K\n",
    "    print(f\"\\nüîß Loading model for {k} labels...\")\n",
    "    topk_model = load_model(\n",
    "        model_name=BASE_CONFIG['model_name'],\n",
    "        num_labels=k,\n",
    "        device=BASE_CONFIG['device'],\n",
    "        use_focal_loss=False,\n",
    "        pos_weight=None\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    print(f\"\\nüöÄ Training on Top-{k}...\")\n",
    "    topk_history = train_model(\n",
    "        model=topk_model,\n",
    "        train_dataloader=topk_train_loader,\n",
    "        num_epochs=BASE_CONFIG['num_epochs'],\n",
    "        learning_rate=BASE_CONFIG['learning_rate'],\n",
    "        device=BASE_CONFIG['device']\n",
    "    )\n",
    "    \n",
    "    # Evaluate model\n",
    "    print(f\"\\nüìä Evaluating Top-{k}...\")\n",
    "    topk_test_results = evaluate_model(\n",
    "        model=topk_model,\n",
    "        test_dataloader=topk_test_loader,\n",
    "        label_names=filtered_label_list,\n",
    "        device=BASE_CONFIG['device']\n",
    "    )\n",
    "    \n",
    "    # Store results\n",
    "    topk_results[f'top_{k}'] = {\n",
    "        'k': k,\n",
    "        'num_train': len(filtered_train_ds),\n",
    "        'num_test': len(filtered_test_ds),\n",
    "        'metrics': topk_test_results,\n",
    "        'labels': filtered_label_list\n",
    "    }\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"\\n‚úÖ Top-{k} Results:\")\n",
    "    for metric, value in topk_test_results.items():\n",
    "        if isinstance(value, (int, float)):\n",
    "            print(f\"   {metric}: {value:.4f}\")\n",
    "\n",
    "# Create comparison DataFrame\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"üìä TOP-K COMPARISON TABLE\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "topk_comparison = []\n",
    "for key, data in topk_results.items():\n",
    "    metrics = data['metrics']\n",
    "    topk_comparison.append({\n",
    "        'K': data['k'],\n",
    "        'Train Samples': data['num_train'],\n",
    "        'Test Samples': data['num_test'],\n",
    "        'Micro F1': metrics.get('micro_f1', 0),\n",
    "        'Hamming Loss': metrics.get('hamming_loss', 0),\n",
    "        'Micro Precision': metrics.get('micro_precision', 0),\n",
    "        'Micro Recall': metrics.get('micro_recall', 0),\n",
    "        'Recall@5': metrics.get('recall_at_5', 0),\n",
    "        'Precision@5': metrics.get('precision_at_5', 0)\n",
    "    })\n",
    "\n",
    "df_topk = pd.DataFrame(topk_comparison)\n",
    "df_topk = df_topk.sort_values('K', ascending=False)\n",
    "print(df_topk.to_string(index=False))\n",
    "\n",
    "# Save CSV\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "topk_csv = OUTPUT_DIR / f'topk_analysis_{timestamp}.csv'\n",
    "df_topk.to_csv(topk_csv, index=False)\n",
    "print(f\"\\n‚úÖ CSV saved: {topk_csv}\")\n",
    "\n",
    "# Create visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Plot 1: F1 Score vs K\n",
    "ax = axes[0, 0]\n",
    "ax.plot(df_topk['K'], df_topk['Micro F1'], marker='o', linewidth=2, markersize=8, label='Micro F1', color='blue')\n",
    "ax.set_xlabel('Number of Labels (K)', fontsize=12)\n",
    "ax.set_ylabel('Micro F1 Score', fontsize=12)\n",
    "ax.set_title('Model Performance vs Label Count', fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend()\n",
    "ax.invert_xaxis()  # Higher K on left\n",
    "\n",
    "# Plot 2: Hamming Loss vs K\n",
    "ax = axes[0, 1]\n",
    "ax.plot(df_topk['K'], df_topk['Hamming Loss'], marker='s', linewidth=2, markersize=8, label='Hamming Loss', color='red')\n",
    "ax.set_xlabel('Number of Labels (K)', fontsize=12)\n",
    "ax.set_ylabel('Hamming Loss (lower is better)', fontsize=12)\n",
    "ax.set_title('Hamming Loss vs Label Count', fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend()\n",
    "ax.invert_xaxis()  # Higher K on left\n",
    "\n",
    "# Plot 3: Recall@5 vs K\n",
    "ax = axes[1, 0]\n",
    "ax.bar(range(len(df_topk)), df_topk['Recall@5'], alpha=0.7, color='coral')\n",
    "ax.set_xticks(range(len(df_topk)))\n",
    "ax.set_xticklabels([f'Top-{k}' for k in df_topk['K']])\n",
    "ax.set_ylabel('Recall@5', fontsize=12)\n",
    "ax.set_title('Top-5 Recall by Label Count', fontsize=14, fontweight='bold')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "for i, v in enumerate(df_topk['Recall@5']):\n",
    "    ax.text(i, v + 0.02, f'{v:.3f}', ha='center', fontsize=10)\n",
    "\n",
    "# Plot 4: All metrics comparison\n",
    "ax = axes[1, 1]\n",
    "x = range(len(df_topk))\n",
    "width = 0.2\n",
    "ax.bar([i - width*1.5 for i in x], df_topk['Micro F1'], width, label='F1', alpha=0.8)\n",
    "ax.bar([i - width*0.5 for i in x], df_topk['Micro Precision'], width, label='Precision', alpha=0.8)\n",
    "ax.bar([i + width*0.5 for i in x], df_topk['Micro Recall'], width, label='Recall', alpha=0.8)\n",
    "ax.bar([i + width*1.5 for i in x], df_topk['Recall@5'], width, label='Recall@5', alpha=0.8)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([f'Top-{k}' for k in df_topk['K']])\n",
    "ax.set_ylabel('Score', fontsize=12)\n",
    "ax.set_title('All Metrics Comparison', fontsize=14, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / f'topk_analysis_{timestamp}.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úÖ Histogram saved: {OUTPUT_DIR / f'topk_analysis_{timestamp}.png'}\")\n",
    "\n",
    "# Store in all_test_results for comparison\n",
    "for key, data in topk_results.items():\n",
    "    all_test_results[key] = {\n",
    "        'config': f\"Top-{data['k']} Labels\",\n",
    "        'description': f\"Baseline BCE with {data['k']} most frequent labels\",\n",
    "        'results': data['metrics']\n",
    "    }\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"‚úÖ TOP-K ANALYSIS COMPLETE\")\n",
    "print(f\"{'='*60}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ada1353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top-K Label Analysis: Test model capacity with different label subset sizes\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"üß™ TOP-K LABEL ANALYSIS\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "from src.strategies import filter_top_k_labels\n",
    "\n",
    "# Test different K values\n",
    "k_values = [100, 50, 20, 10, 5]\n",
    "topk_results = {}\n",
    "\n",
    "for k in k_values:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üî¨ Testing Top-{k} Labels\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Filter dataset to top-k labels\n",
    "    filtered_train_ds, filtered_label_list, label_mapping = filter_top_k_labels(\n",
    "        data['train_dataset'], \n",
    "        label_names, \n",
    "        k=k\n",
    "    )\n",
    "    filtered_test_ds, _, _ = filter_top_k_labels(\n",
    "        data['test_dataset'], \n",
    "        label_names, \n",
    "        k=k\n",
    "    )\n",
    "    \n",
    "    print(f\"üìä Dataset Statistics:\")\n",
    "    print(f\"   Top-{k} labels selected\")\n",
    "    print(f\"   Train samples: {len(filtered_train_ds)}\")\n",
    "    print(f\"   Test samples: {len(filtered_test_ds)}\")\n",
    "    print(f\"   Labels: {filtered_label_list[:5]}...\")\n",
    "    \n",
    "    # Create dataloaders\n",
    "    topk_train_loader = DataLoader(\n",
    "        filtered_train_ds,\n",
    "        batch_size=BASE_CONFIG['batch_size'],\n",
    "        shuffle=True\n",
    "    )\n",
    "    topk_test_loader = DataLoader(\n",
    "        filtered_test_ds,\n",
    "        batch_size=BASE_CONFIG['batch_size'],\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    # Load model for this K\n",
    "    print(f\"\\nüîß Loading model for {k} labels...\")\n",
    "    topk_model = load_model(\n",
    "        model_name=BASE_CONFIG['model_name'],\n",
    "        num_labels=k,\n",
    "        device=BASE_CONFIG['device'],\n",
    "        use_focal_loss=False,\n",
    "        pos_weight=None\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    print(f\"\\nüöÄ Training on Top-{k}...\")\n",
    "    topk_history = train_model(\n",
    "        model=topk_model,\n",
    "        train_dataloader=topk_train_loader,\n",
    "        num_epochs=BASE_CONFIG['num_epochs'],\n",
    "        learning_rate=BASE_CONFIG['learning_rate'],\n",
    "        device=BASE_CONFIG['device']\n",
    "    )\n",
    "    \n",
    "    # Evaluate model\n",
    "    print(f\"\\nüìä Evaluating Top-{k}...\")\n",
    "    topk_test_results = evaluate_model(\n",
    "        model=topk_model,\n",
    "        test_dataloader=topk_test_loader,\n",
    "        label_names=filtered_label_list,\n",
    "        device=BASE_CONFIG['device']\n",
    "    )\n",
    "    \n",
    "    # Store results\n",
    "    topk_results[f'top_{k}'] = {\n",
    "        'k': k,\n",
    "        'num_train': len(filtered_train_ds),\n",
    "        'num_test': len(filtered_test_ds),\n",
    "        'metrics': topk_test_results,\n",
    "        'labels': filtered_label_list\n",
    "    }\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"\\n‚úÖ Top-{k} Results:\")\n",
    "    for metric, value in topk_test_results.items():\n",
    "        if isinstance(value, (int, float)):\n",
    "            print(f\"   {metric}: {value:.4f}\")\n",
    "\n",
    "# Create comparison DataFrame\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"üìä TOP-K COMPARISON TABLE\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "topk_comparison = []\n",
    "for key, data in topk_results.items():\n",
    "    metrics = data['metrics']\n",
    "    topk_comparison.append({\n",
    "        'K': data['k'],\n",
    "        'Train Samples': data['num_train'],\n",
    "        'Test Samples': data['num_test'],\n",
    "        'Micro F1': metrics.get('micro_f1', 0),\n",
    "        'Micro Precision': metrics.get('micro_precision', 0),\n",
    "        'Micro Recall': metrics.get('micro_recall', 0),\n",
    "        'Recall@5': metrics.get('recall_at_5', 0),\n",
    "        'Precision@5': metrics.get('precision_at_5', 0)\n",
    "    })\n",
    "\n",
    "df_topk = pd.DataFrame(topk_comparison)\n",
    "df_topk = df_topk.sort_values('K', ascending=False)\n",
    "print(df_topk.to_string(index=False))\n",
    "\n",
    "# Save CSV\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "topk_csv = OUTPUT_DIR / f'topk_analysis_{timestamp}.csv'\n",
    "df_topk.to_csv(topk_csv, index=False)\n",
    "print(f\"\\n‚úÖ CSV saved: {topk_csv}\")\n",
    "\n",
    "# Create visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Plot 1: F1 Score vs K\n",
    "ax = axes[0, 0]\n",
    "ax.plot(df_topk['K'], df_topk['Micro F1'], marker='o', linewidth=2, markersize=8, label='Micro F1')\n",
    "ax.set_xlabel('Number of Labels (K)', fontsize=12)\n",
    "ax.set_ylabel('Micro F1 Score', fontsize=12)\n",
    "ax.set_title('Model Performance vs Label Count', fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend()\n",
    "ax.invert_xaxis()  # Higher K on left\n",
    "\n",
    "# Plot 2: Precision vs Recall\n",
    "ax = axes[0, 1]\n",
    "ax.scatter(df_topk['Micro Recall'], df_topk['Micro Precision'], \n",
    "           s=200, alpha=0.6, c=df_topk['K'], cmap='viridis')\n",
    "for i, k in enumerate(df_topk['K']):\n",
    "    ax.annotate(f'K={k}', \n",
    "                (df_topk.iloc[i]['Micro Recall'], df_topk.iloc[i]['Micro Precision']),\n",
    "                fontsize=10, ha='right')\n",
    "ax.set_xlabel('Micro Recall', fontsize=12)\n",
    "ax.set_ylabel('Micro Precision', fontsize=12)\n",
    "ax.set_title('Precision-Recall Trade-off', fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Recall@5 vs K\n",
    "ax = axes[1, 0]\n",
    "ax.bar(range(len(df_topk)), df_topk['Recall@5'], alpha=0.7, color='coral')\n",
    "ax.set_xticks(range(len(df_topk)))\n",
    "ax.set_xticklabels([f'Top-{k}' for k in df_topk['K']])\n",
    "ax.set_ylabel('Recall@5', fontsize=12)\n",
    "ax.set_title('Top-5 Recall by Label Count', fontsize=14, fontweight='bold')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "for i, v in enumerate(df_topk['Recall@5']):\n",
    "    ax.text(i, v + 0.02, f'{v:.3f}', ha='center', fontsize=10)\n",
    "\n",
    "# Plot 4: All metrics comparison\n",
    "ax = axes[1, 1]\n",
    "x = range(len(df_topk))\n",
    "width = 0.2\n",
    "ax.bar([i - width*1.5 for i in x], df_topk['Micro F1'], width, label='F1', alpha=0.8)\n",
    "ax.bar([i - width*0.5 for i in x], df_topk['Micro Precision'], width, label='Precision', alpha=0.8)\n",
    "ax.bar([i + width*0.5 for i in x], df_topk['Micro Recall'], width, label='Recall', alpha=0.8)\n",
    "ax.bar([i + width*1.5 for i in x], df_topk['Recall@5'], width, label='Recall@5', alpha=0.8)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([f'Top-{k}' for k in df_topk['K']])\n",
    "ax.set_ylabel('Score', fontsize=12)\n",
    "ax.set_title('All Metrics Comparison', fontsize=14, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / f'topk_analysis_{timestamp}.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úÖ Histogram saved: {OUTPUT_DIR / f'topk_analysis_{timestamp}.png'}\")\n",
    "\n",
    "# Store in all_test_results for comparison\n",
    "for key, data in topk_results.items():\n",
    "    all_test_results[key] = {\n",
    "        'config': f\"Top-{data['k']} Labels\",\n",
    "        'description': f\"Baseline BCE with {data['k']} most frequent labels\",\n",
    "        'results': data['metrics']\n",
    "    }\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"‚úÖ TOP-K ANALYSIS COMPLETE\")\n",
    "print(f\"{'='*60}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe3c589",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä RESULTS COMPARISON\n",
    "\n",
    "√áalƒ±≈ütƒ±rdƒ±ƒüƒ±nƒ±z stratejilerin sonu√ßlarƒ±nƒ± kar≈üƒ±la≈ütƒ±rmak i√ßin bu h√ºcreyi √ßalƒ±≈ütƒ±rƒ±n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8301b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(all_test_results) == 0:\n",
    "    print(\"‚ö†Ô∏è Hen√ºz hi√ßbir strateji test edilmedi!\")\n",
    "else:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üìä STRATEGY COMPARISON RESULTS\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    # Create comparison table\n",
    "    comparison_data = []\n",
    "    for strategy_name, data in all_test_results.items():\n",
    "        results = data['results']\n",
    "        comparison_data.append({\n",
    "            'Strategy': strategy_name,\n",
    "            'Micro F1': results.get('micro_f1', 0),\n",
    "            'Hamming Loss': results.get('hamming_loss', 0),\n",
    "            'Recall@5': results.get('recall_at_5', 0),\n",
    "            'Precision@5': results.get('precision_at_5', 0),\n",
    "            'Recall@10': results.get('recall_at_10', 0),\n",
    "            'Precision@10': results.get('precision_at_10', 0),\n",
    "            'Example Accuracy': results.get('example_based_accuracy', 0)\n",
    "        })\n",
    "    \n",
    "    df_comparison = pd.DataFrame(comparison_data)\n",
    "    df_comparison = df_comparison.sort_values('Micro F1', ascending=False)\n",
    "    \n",
    "    print(df_comparison.to_string(index=False))\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    \n",
    "    # Find best strategy\n",
    "    best_strategy = df_comparison.iloc[0]['Strategy']\n",
    "    best_f1 = df_comparison.iloc[0]['Micro F1']\n",
    "    best_hamming = df_comparison.iloc[0]['Hamming Loss']\n",
    "    \n",
    "    print(f\"\\nüèÜ EN ƒ∞Yƒ∞ STRATEJƒ∞: {best_strategy}\")\n",
    "    print(f\"   Micro F1 Score: {best_f1:.4f}\")\n",
    "    print(f\"   Hamming Loss: {best_hamming:.4f} (lower is better)\")\n",
    "    \n",
    "    # Compare to baseline if exists\n",
    "    if 'baseline' in all_test_results:\n",
    "        baseline_f1 = all_test_results['baseline']['results'].get('micro_f1', 0)\n",
    "        improvement = ((best_f1 - baseline_f1) / baseline_f1 * 100) if baseline_f1 > 0 else 0\n",
    "        print(f\"   Baseline'a g√∂re iyile≈ütirme: {improvement:+.2f}%\")\n",
    "    \n",
    "    # Plot comparison\n",
    "    if len(all_test_results) > 1:\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "        \n",
    "        # F1 scores comparison\n",
    "        strategies = df_comparison['Strategy'].tolist()\n",
    "        micro_f1 = df_comparison['Micro F1'].tolist()\n",
    "        recall_at_5 = df_comparison['Recall@5'].tolist()\n",
    "        \n",
    "        x = np.arange(len(strategies))\n",
    "        width = 0.35\n",
    "        \n",
    "        axes[0].bar(x - width/2, micro_f1, width, label='Micro F1', alpha=0.8)\n",
    "        axes[0].bar(x + width/2, recall_at_5, width, label='Recall@5', alpha=0.8)\n",
    "        axes[0].set_xlabel('Strategy')\n",
    "        axes[0].set_ylabel('Score')\n",
    "        axes[0].set_title('Micro F1 & Recall@5 Comparison')\n",
    "        axes[0].set_xticks(x)\n",
    "        axes[0].set_xticklabels(strategies, rotation=45, ha='right')\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # Hamming Loss comparison\n",
    "        hamming_loss_vals = df_comparison['Hamming Loss'].tolist()\n",
    "        \n",
    "        axes[1].bar(x, hamming_loss_vals, alpha=0.8, color='coral')\n",
    "        axes[1].set_xlabel('Strategy')\n",
    "        axes[1].set_ylabel('Hamming Loss (lower is better)')\n",
    "        axes[1].set_title('Hamming Loss Comparison')\n",
    "        axes[1].set_xticks(x)\n",
    "        axes[1].set_xticklabels(strategies, rotation=45, ha='right')\n",
    "        axes[1].grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c58a4da",
   "metadata": {},
   "source": [
    "if len(all_test_results) == 0:\n",
    "    print(\"‚ö†Ô∏è Kaydedilecek sonu√ß yok!\")\n",
    "else:\n",
    "    # Helper function to convert numpy arrays to lists\n",
    "    def convert_to_serializable(obj):\n",
    "        \"\"\"Recursively convert numpy arrays to lists for JSON serialization.\"\"\"\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        elif isinstance(obj, dict):\n",
    "            return {key: convert_to_serializable(value) for key, value in obj.items()}\n",
    "        elif isinstance(obj, list):\n",
    "            return [convert_to_serializable(item) for item in obj]\n",
    "        elif isinstance(obj, (np.integer, np.floating)):\n",
    "            return float(obj)\n",
    "        else:\n",
    "            return obj\n",
    "    \n",
    "    # Save results to JSON\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    results_file = OUTPUT_DIR / f'strategy_comparison_{timestamp}.json'\n",
    "    \n",
    "    # Convert results to JSON-serializable format\n",
    "    serializable_results = convert_to_serializable(all_test_results)\n",
    "    \n",
    "    with open(results_file, 'w') as f:\n",
    "        json.dump(serializable_results, f, indent=2)\n",
    "    \n",
    "    print(f\"‚úÖ Sonu√ßlar kaydedildi: {results_file}\")\n",
    "    \n",
    "    # Save comparison table to CSV\n",
    "    comparison_data = []\n",
    "    for strategy_name, data in all_test_results.items():\n",
    "        results = data['results']\n",
    "        comparison_data.append({\n",
    "            'Strategy': strategy_name,\n",
    "            'Micro_F1': results.get('micro_f1', 0),\n",
    "            'Hamming_Loss': results.get('hamming_loss', 0),\n",
    "            'Recall_at_5': results.get('recall_at_5', 0),\n",
    "            'Precision_at_5': results.get('precision_at_5', 0),\n",
    "            'Recall_at_10': results.get('recall_at_10', 0),\n",
    "            'Precision_at_10': results.get('precision_at_10', 0),\n",
    "            'Example_Based_Accuracy': results.get('example_based_accuracy', 0)\n",
    "        })\n",
    "    \n",
    "    df_comparison = pd.DataFrame(comparison_data)\n",
    "    csv_file = OUTPUT_DIR / f'strategy_comparison_{timestamp}.csv'\n",
    "    df_comparison.to_csv(csv_file, index=False)\n",
    "    \n",
    "    print(f\"‚úÖ CSV tablosu kaydedildi: {csv_file}\")\n",
    "    \n",
    "    # Download files if on Colab\n",
    "    try:\n",
    "        from google.colab import files\n",
    "        files.download(str(results_file))\n",
    "        files.download(str(csv_file))\n",
    "        print(\"‚úÖ Dosyalar indirildi\")\n",
    "    except ImportError:\n",
    "        print(\"üí° Colab deƒüil, dosyalar sadece kaydedildi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652cd657",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(all_test_results) == 0:\n",
    "    print(\"‚ö†Ô∏è Kaydedilecek sonu√ß yok!\")\n",
    "else:\n",
    "    # Helper function to convert numpy arrays to lists\n",
    "    def convert_to_serializable(obj):\n",
    "        \"\"\"Recursively convert numpy arrays to lists for JSON serialization.\"\"\"\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        elif isinstance(obj, dict):\n",
    "            return {key: convert_to_serializable(value) for key, value in obj.items()}\n",
    "        elif isinstance(obj, list):\n",
    "            return [convert_to_serializable(item) for item in obj]\n",
    "        elif isinstance(obj, (np.integer, np.floating)):\n",
    "            return float(obj)\n",
    "        else:\n",
    "            return obj\n",
    "    \n",
    "    # Save results to JSON\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    results_file = OUTPUT_DIR / f'strategy_comparison_{timestamp}.json'\n",
    "    \n",
    "    # Convert results to JSON-serializable format\n",
    "    serializable_results = convert_to_serializable(all_test_results)\n",
    "    \n",
    "    with open(results_file, 'w') as f:\n",
    "        json.dump(serializable_results, f, indent=2)\n",
    "    \n",
    "    print(f\"‚úÖ Sonu√ßlar kaydedildi: {results_file}\")\n",
    "    \n",
    "    # Save comparison table to CSV\n",
    "    comparison_data = []\n",
    "    for strategy_name, data in all_test_results.items():\n",
    "        results = data['results']\n",
    "        comparison_data.append({\n",
    "            'Strategy': strategy_name,\n",
    "            'Micro_F1': results.get('micro_f1', 0),\n",
    "            'Recall_at_5': results.get('recall_at_5', 0),\n",
    "            'Precision_at_5': results.get('precision_at_5', 0),\n",
    "            'Recall_at_10': results.get('recall_at_10', 0),\n",
    "            'Precision_at_10': results.get('precision_at_10', 0),\n",
    "            'Example_Based_Accuracy': results.get('example_based_accuracy', 0)\n",
    "        })\n",
    "    \n",
    "    df_comparison = pd.DataFrame(comparison_data)\n",
    "    csv_file = OUTPUT_DIR / f'strategy_comparison_{timestamp}.csv'\n",
    "    df_comparison.to_csv(csv_file, index=False)\n",
    "    \n",
    "    print(f\"‚úÖ CSV tablosu kaydedildi: {csv_file}\")\n",
    "    \n",
    "    # Download files if on Colab\n",
    "    try:\n",
    "        from google.colab import files\n",
    "        files.download(str(results_file))\n",
    "        files.download(str(csv_file))\n",
    "        print(\"‚úÖ Dosyalar indirildi\")\n",
    "    except ImportError:\n",
    "        print(\"üí° Colab deƒüil, dosyalar sadece kaydedildi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93aeaaa",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìù NOTLAR VE √ñNERƒ∞LER\n",
    "\n",
    "**Model:**\n",
    "- **CTI-BERT** (ibm-research/CTI-BERT): Cyber Threat Intelligence'a √∂zel BERT\n",
    "- G√ºvenlik metinlerinde genel BERT'ten daha iyi performans\n",
    "- Pre-trained on security-specific corpus\n",
    "\n",
    "**Dataset:**\n",
    "- Hybrid mode kullanƒ±lƒ±yor (tumeteor + sarahwei + Zainabsa99)\n",
    "- Toplam ~16k sample, daha iyi label coverage\n",
    "- Tek dataset kullanmak i√ßin: `use_hybrid=False` parametresi ekle\n",
    "\n",
    "**Test Sƒ±rasƒ±:**\n",
    "1. **Baseline** - Her zaman ilk olarak √ßalƒ±≈ütƒ±rƒ±n (referans performans)\n",
    "2. **Weighted BCE** - En umut verici strateji, Micro F1 > 0.15 bekleniyor\n",
    "3. Weighted BCE ba≈üarƒ±lƒ± olursa ‚Üí Bitti, kullan!\n",
    "4. Weighted BCE ba≈üarƒ±sƒ±z olursa ‚Üí **Focal Loss (Œ≥=2)** dene\n",
    "5. Hala k√∂t√ºyse ‚Üí **Focal Loss (Œ≥=5)** dene\n",
    "6. Hepsi ba≈üarƒ±sƒ±z olursa ‚Üí **Top-100** ile model √∂ƒürenme kapasitesini kontrol et\n",
    "\n",
    "**Ba≈üarƒ± Kriterleri:**\n",
    "- Micro F1 > 0.15 ‚Üí ƒ∞yi performans\n",
    "- Recall@5 > 0.30 ‚Üí √áok iyi performans\n",
    "- Micro F1 < 0.10 ‚Üí Strateji i≈üe yaramadƒ±\n",
    "\n",
    "**CTI-BERT ile Beklenen ƒ∞yile≈ümeler:**\n",
    "- Cyber security terminolojisini daha iyi anlama\n",
    "- MITRE ATT&CK kavramlarƒ±nda daha y√ºksek accuracy\n",
    "- Rare technique'leri daha iyi tanƒ±ma\n",
    "\n",
    "**Metrikler:**\n",
    "- **Micro F1:** Genel performans (threshold-based)\n",
    "- **Recall@5:** Top-5 tahmin i√ßinde ka√ß doƒüru label var\n",
    "- **Precision@5:** Top-5 tahminlerin ka√ßƒ± doƒüru\n",
    "- **Example-Based Accuracy:** T√ºm label'larƒ±n tam e≈üle≈ümesi (√ßok strict)\n",
    "\n",
    "**S√ºre:**\n",
    "- Her strateji ~30-45 dakika s√ºrer\n",
    "- CTI-BERT model download ilk seferde ~5 dakika ekstra\n",
    "- Modular yakla≈üƒ±m sayesinde istediƒüiniz zaman durup sonu√ßlara bakabilirsiniz\n",
    "\n",
    "**Hatƒ±rlatmalar:**\n",
    "- Herbir strateji h√ºcresi baƒüƒ±msƒ±zdƒ±r, istediƒüiniz sƒ±rada √ßalƒ±≈ütƒ±rabilirsiniz\n",
    "- Sonu√ßlar `all_test_results` dictionary'sinde saklanƒ±r\n",
    "- Comparison h√ºcresini dilediƒüiniz zaman √ßalƒ±≈ütƒ±rƒ±p ara sonu√ßlara bakabilirsiniz\n",
    "- CTI-BERT ilk indirilirken cache'lenir, sonraki √ßalƒ±≈ütƒ±rmalar hƒ±zlƒ±dƒ±r\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
