{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91ac4c2c",
   "metadata": {},
   "source": [
    "# CTI-BERT TTP Tagging - GPU Training Pipeline\n",
    "\n",
    "Bu notebook, Google Colab GPU ile VS Code entegrasyonu Ã¼zerinden CTI-BERT modelini eÄŸitmek iÃ§in hazÄ±rlanmÄ±ÅŸtÄ±r.\n",
    "\n",
    "**Dataset:** tumeteor/Security-TTP-Mapping (20,736 samples, 499 MITRE ATT&CK techniques)\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸš€ HÄ±zlÄ± BaÅŸlangÄ±Ã§\n",
    "\n",
    "1. **VS Code'da Colab baÄŸlantÄ±sÄ±nÄ± kontrol et**\n",
    "2. **GPU'nun aktif olduÄŸundan emin ol** (Runtime â†’ Change runtime type â†’ GPU)\n",
    "3. **Run All** veya hÃ¼creleri sÄ±rayla Ã§alÄ±ÅŸtÄ±r\n",
    "\n",
    "---\n",
    "\n",
    "## âš™ï¸ YapÄ±landÄ±rma\n",
    "\n",
    "AÅŸaÄŸÄ±daki parametreleri deÄŸiÅŸtirebilirsin:\n",
    "- `MODEL_NAME`: \"bert-base-uncased\" veya \"jackaduma/SecBERT\"\n",
    "- `NUM_EPOCHS`: 3 (varsayÄ±lan)\n",
    "- `BATCH_SIZE`: 16 (GPU memory'ye gÃ¶re ayarla)\n",
    "- `LEARNING_RATE`: 2e-5 (varsayÄ±lan)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a63ef70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "else:\n",
    "    print(\"Running on CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78419b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import ast\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create output directory\n",
    "output_dir = f\"./outputs/colab_training_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "print(f\"Output directory: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6779da61",
   "metadata": {},
   "source": [
    "## 1. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba83f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "print(\"Loading dataset...\")\n",
    "dataset = load_dataset(\"tumeteor/Security-TTP-Mapping\")\n",
    "\n",
    "train_df = pd.DataFrame(dataset['train'])\n",
    "test_df = pd.DataFrame(dataset['validation'])  # Using validation as test\n",
    "\n",
    "print(f\"Train samples: {len(train_df)}\")\n",
    "print(f\"Test samples: {len(test_df)}\")\n",
    "print(f\"\\nSample data:\")\n",
    "print(train_df.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f4fb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract unique labels\n",
    "def get_label_list(df):\n",
    "    all_labels = set()\n",
    "    for labels in df['labels']:\n",
    "        label_list = ast.literal_eval(labels)\n",
    "        all_labels.update(label_list)\n",
    "    return sorted(list(all_labels))\n",
    "\n",
    "label_list = get_label_list(train_df)\n",
    "num_labels = len(label_list)\n",
    "label_to_id = {label: idx for idx, label in enumerate(label_list)}\n",
    "\n",
    "print(f\"Total unique MITRE ATT&CK techniques: {num_labels}\")\n",
    "print(f\"First 10 techniques: {label_list[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23551bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels to binary vectors\n",
    "def encode_labels(df):\n",
    "    encoded = []\n",
    "    for labels in df['labels']:\n",
    "        label_vector = [0] * num_labels\n",
    "        label_items = ast.literal_eval(labels)\n",
    "        for label in label_items:\n",
    "            if label in label_to_id:\n",
    "                label_vector[label_to_id[label]] = 1\n",
    "        encoded.append(label_vector)\n",
    "    return encoded\n",
    "\n",
    "train_labels = encode_labels(train_df)\n",
    "test_labels = encode_labels(test_df)\n",
    "\n",
    "print(f\"Label encoding complete!\")\n",
    "print(f\"Sample label vector shape: {len(train_labels[0])}\")\n",
    "print(f\"Sample label vector (first 20): {train_labels[0][:20]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6659c21f",
   "metadata": {},
   "source": [
    "## 2. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6043bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "MODEL_NAME = \"bert-base-uncased\"  # Change to \"jackaduma/SecBERT\" for domain model\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "print(f\"Tokenizing texts...\")\n",
    "train_encodings = tokenizer(\n",
    "    train_df['text1'].tolist(),\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    max_length=MAX_LENGTH,\n",
    "    return_tensors=None\n",
    ")\n",
    "\n",
    "test_encodings = tokenizer(\n",
    "    test_df['text1'].tolist(),\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    max_length=MAX_LENGTH,\n",
    "    return_tensors=None\n",
    ")\n",
    "\n",
    "print(\"Tokenization complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c38c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "class CTIDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = CTIDataset(train_encodings, train_labels)\n",
    "test_dataset = CTIDataset(test_encodings, test_labels)\n",
    "\n",
    "print(f\"Datasets created: {len(train_dataset)} train, {len(test_dataset)} test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6edb371",
   "metadata": {},
   "source": [
    "## 3. Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055971c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTForMultiLabelClassification(nn.Module):\n",
    "    def __init__(self, model_name, num_labels, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        config = AutoConfig.from_pretrained(model_name)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(config.hidden_size, num_labels)\n",
    "        \n",
    "        # Initialize classifier\n",
    "        self.classifier.weight.data.normal_(mean=0.0, std=config.initializer_range)\n",
    "        self.classifier.bias.data.zero_()\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.last_hidden_state[:, 0, :]  # [CLS] token\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.BCEWithLogitsLoss()\n",
    "            loss = loss_fct(logits, labels)\n",
    "        \n",
    "        return {'loss': loss, 'logits': logits}\n",
    "\n",
    "# Initialize model\n",
    "model = BERTForMultiLabelClassification(MODEL_NAME, num_labels).to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model loaded on {device}\")\n",
    "print(f\"Total parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caae7b61",
   "metadata": {},
   "source": [
    "## 4. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dacb916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_EPOCHS = 3\n",
    "WARMUP_STEPS = 500\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "total_steps = len(train_loader) * NUM_EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, WARMUP_STEPS, total_steps)\n",
    "\n",
    "print(f\"Training configuration:\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Total steps: {total_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6c50fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_epoch(model, dataloader, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\")\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs['loss']\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix({'loss': loss.item()})\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# Train the model\n",
    "history = {'train_loss': [], 'epoch': []}\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{NUM_EPOCHS}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    avg_loss = train_epoch(model, train_loader, optimizer, scheduler, device)\n",
    "    \n",
    "    print(f\"Average training loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    history['train_loss'].append(avg_loss)\n",
    "    history['epoch'].append(epoch + 1)\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d1bb4c",
   "metadata": {},
   "source": [
    "## 5. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92543c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction function\n",
    "def predict(model, dataloader, device, threshold=0.5):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Predicting\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs['logits']\n",
    "            predictions = torch.sigmoid(logits)\n",
    "            predictions = (predictions > threshold).float()\n",
    "            \n",
    "            all_predictions.append(predictions.cpu().numpy())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "    \n",
    "    all_predictions = np.vstack(all_predictions)\n",
    "    all_labels = np.vstack(all_labels)\n",
    "    \n",
    "    return all_predictions, all_labels\n",
    "\n",
    "# Get predictions\n",
    "predictions, labels = predict(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cae667a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics\n",
    "micro_f1 = f1_score(labels, predictions, average='micro', zero_division=0)\n",
    "micro_precision = precision_score(labels, predictions, average='micro', zero_division=0)\n",
    "micro_recall = recall_score(labels, predictions, average='micro', zero_division=0)\n",
    "\n",
    "macro_f1 = f1_score(labels, predictions, average='macro', zero_division=0)\n",
    "macro_precision = precision_score(labels, predictions, average='macro', zero_division=0)\n",
    "macro_recall = recall_score(labels, predictions, average='macro', zero_division=0)\n",
    "\n",
    "samples_f1 = f1_score(labels, predictions, average='samples', zero_division=0)\n",
    "subset_accuracy = accuracy_score(labels, predictions)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EVALUATION RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nMicro-averaged metrics (overall):\")\n",
    "print(f\"  F1 Score:  {micro_f1:.4f}\")\n",
    "print(f\"  Precision: {micro_precision:.4f}\")\n",
    "print(f\"  Recall:    {micro_recall:.4f}\")\n",
    "\n",
    "print(f\"\\nMacro-averaged metrics (per label):\")\n",
    "print(f\"  F1 Score:  {macro_f1:.4f}\")\n",
    "print(f\"  Precision: {macro_precision:.4f}\")\n",
    "print(f\"  Recall:    {macro_recall:.4f}\")\n",
    "\n",
    "print(f\"\\nOther metrics:\")\n",
    "print(f\"  Samples F1:      {samples_f1:.4f}\")\n",
    "print(f\"  Subset Accuracy: {subset_accuracy:.4f}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b783eca",
   "metadata": {},
   "source": [
    "## 6. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8d70fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and results\n",
    "model_path = os.path.join(output_dir, 'cti_bert_model.pt')\n",
    "results_path = os.path.join(output_dir, 'results.json')\n",
    "labels_path = os.path.join(output_dir, 'labels.json')\n",
    "\n",
    "# Save model\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f\"Model saved: {model_path}\")\n",
    "\n",
    "# Save label list\n",
    "with open(labels_path, 'w') as f:\n",
    "    json.dump(label_list, f, indent=2)\n",
    "print(f\"Labels saved: {labels_path}\")\n",
    "\n",
    "# Save results\n",
    "results = {\n",
    "    'model': MODEL_NAME,\n",
    "    'num_labels': num_labels,\n",
    "    'train_samples': len(train_dataset),\n",
    "    'test_samples': len(test_dataset),\n",
    "    'metrics': {\n",
    "        'micro_f1': float(micro_f1),\n",
    "        'micro_precision': float(micro_precision),\n",
    "        'micro_recall': float(micro_recall),\n",
    "        'macro_f1': float(macro_f1),\n",
    "        'macro_precision': float(macro_precision),\n",
    "        'macro_recall': float(macro_recall),\n",
    "        'samples_f1': float(samples_f1),\n",
    "        'subset_accuracy': float(subset_accuracy)\n",
    "    },\n",
    "    'training_history': history,\n",
    "    'hyperparameters': {\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'learning_rate': LEARNING_RATE,\n",
    "        'num_epochs': NUM_EPOCHS,\n",
    "        'max_length': MAX_LENGTH,\n",
    "        'warmup_steps': WARMUP_STEPS\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(results_path, 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"Results saved: {results_path}\")\n",
    "print(f\"\\nAll outputs saved to: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872dd1d1",
   "metadata": {},
   "source": [
    "## 7. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbb70ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Training loss plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history['epoch'], history['train_loss'], marker='o', linewidth=2, markersize=8)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.title('Training Loss over Epochs', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "loss_plot_path = os.path.join(output_dir, 'training_loss.png')\n",
    "plt.savefig(loss_plot_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"Loss plot saved: {loss_plot_path}\")\n",
    "plt.show()\n",
    "\n",
    "# Metrics comparison\n",
    "metrics_names = ['Micro F1', 'Macro F1', 'Samples F1']\n",
    "metrics_values = [micro_f1, macro_f1, samples_f1]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(metrics_names, metrics_values, color=['#3498db', '#2ecc71', '#f39c12'], width=0.6)\n",
    "plt.ylabel('Score', fontsize=12)\n",
    "plt.title('Model Performance Metrics', fontsize=14, fontweight='bold')\n",
    "plt.ylim([0, 1])\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, v in enumerate(metrics_values):\n",
    "    plt.text(i, v + 0.02, f'{v:.4f}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "metrics_plot_path = os.path.join(output_dir, 'metrics_comparison.png')\n",
    "plt.savefig(metrics_plot_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"Metrics plot saved: {metrics_plot_path}\")\n",
    "plt.show()\n",
    "\n",
    "# Precision-Recall comparison\n",
    "pr_metrics = ['Precision', 'Recall', 'F1']\n",
    "micro_values = [micro_precision, micro_recall, micro_f1]\n",
    "macro_values = [macro_precision, macro_recall, macro_f1]\n",
    "\n",
    "x = np.arange(len(pr_metrics))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "bars1 = ax.bar(x - width/2, micro_values, width, label='Micro-avg', color='#3498db')\n",
    "bars2 = ax.bar(x + width/2, macro_values, width, label='Macro-avg', color='#e74c3c')\n",
    "\n",
    "ax.set_xlabel('Metrics', fontsize=12)\n",
    "ax.set_ylabel('Score', fontsize=12)\n",
    "ax.set_title('Precision, Recall, F1 Comparison', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(pr_metrics)\n",
    "ax.legend(fontsize=11)\n",
    "ax.set_ylim([0, 1])\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "pr_plot_path = os.path.join(output_dir, 'precision_recall_comparison.png')\n",
    "plt.savefig(pr_plot_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"P-R comparison plot saved: {pr_plot_path}\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"TRAINING COMPLETE!\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"\\nAll results saved to: {output_dir}\")\n",
    "print(f\"\\nFiles created:\")\n",
    "print(f\"  - cti_bert_model.pt\")\n",
    "print(f\"  - results.json\")\n",
    "print(f\"  - labels.json\")\n",
    "print(f\"  - training_loss.png\")\n",
    "print(f\"  - metrics_comparison.png\")\n",
    "print(f\"  - precision_recall_comparison.png\")\n",
    "print(f\"{'='*70}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
