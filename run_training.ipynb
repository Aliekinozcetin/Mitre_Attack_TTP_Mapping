{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68a68433",
   "metadata": {},
   "source": [
    "# ğŸš€ MITRE ATT&CK TTP Mapping - Colab EÄŸitim\n",
    "\n",
    "Google Colab'da GPU ile hÄ±zlÄ± eÄŸitim iÃ§in hazÄ±rlanmÄ±ÅŸ notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c86f1534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.0+cu126\n",
      "CUDA available: True\n",
      "GPU: Tesla T4\n",
      "CUDA version: 12.6\n",
      "Memory: 14.74 GB\n"
     ]
    }
   ],
   "source": [
    "# GPU kontrolÃ¼\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "else:\n",
    "    print(\"âš ï¸  GPU yok - CPU kullanÄ±lacak (yavaÅŸ olabilir)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207c780b",
   "metadata": {},
   "source": [
    "## 1. Kurulum\n",
    "\n",
    "Gerekli kÃ¼tÃ¼phaneleri yÃ¼kle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7033ef65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ KÃ¼tÃ¼phaneler yÃ¼klendi!\n"
     ]
    }
   ],
   "source": [
    "# Gerekli kÃ¼tÃ¼phaneleri yÃ¼kle\n",
    "!pip install -q transformers datasets torch scikit-learn tqdm\n",
    "\n",
    "print(\"âœ“ KÃ¼tÃ¼phaneler yÃ¼klendi!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60221a93",
   "metadata": {},
   "source": [
    "## 2. EÄŸitim BaÅŸlat ğŸš€\n",
    "\n",
    "AÅŸaÄŸÄ±daki hÃ¼crelerden birini Ã§alÄ±ÅŸtÄ±r. SonuÃ§lar `outputs/` klasÃ¶rÃ¼ne kaydedilir:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca62153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ BaÅŸlangÄ±Ã§ dizini: /content\n",
      "ğŸ”§ Google Colab tespit edildi!\n",
      "âœ… Proje zaten mevcut\n",
      "âœ… Proje dizinine geÃ§ildi: /content/Mitre_Attack_TTP_Mapping\n",
      "âœ… src/ klasÃ¶rÃ¼ bulundu\n",
      "ğŸ“‚ Ä°Ã§erik: main.py, src, .gitattributes, COLAB_GUIDE.md, run_training.ipynb, outputs, requirements.txt, CTI_MITRE_ATTACK_DATASETS.md\n",
      "ğŸ¯ HazÄ±r! EÄŸitime baÅŸlayabilirsin.\n"
     ]
    }
   ],
   "source": [
    "# Proje kurulumu\n",
    "import os\n",
    "import sys\n",
    "\n",
    "print(\"ğŸ“¥ Proje hazÄ±rlanÄ±yor...\")\n",
    "\n",
    "# GitHub'dan projeyi klonla\n",
    "if not os.path.exists('/content/Mitre_Attack_TTP_Mapping'):\n",
    "    !git clone https://github.com/Aliekinozcetin/Mitre_Attack_TTP_Mapping.git\n",
    "else:\n",
    "    print(\"âœ… Proje zaten mevcut\")\n",
    "\n",
    "# Proje dizinine geÃ§\n",
    "os.chdir('/content/Mitre_Attack_TTP_Mapping')\n",
    "sys.path.insert(0, '/content/Mitre_Attack_TTP_Mapping')\n",
    "\n",
    "print(f\"âœ… HazÄ±r! EÄŸitim baÅŸlatÄ±labilir.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a1dcfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ EÄŸitim baÅŸlÄ±yor...\n",
      "Model: bert-base-uncased\n",
      "Device: cuda\n",
      "Epochs: 3, Batch: 32\n",
      "\n",
      "ğŸ“Š Veri yÃ¼kleniyor...\n",
      "Loading dataset: tumeteor/Security-TTP-Mapping\n",
      "Available splits: ['train', 'validation', 'test']\n",
      "Available splits: ['train', 'validation', 'test']\n",
      "Using validation split as test set\n",
      "Train samples: 14936\n",
      "Test samples: 2630\n",
      "Total unique labels (MITRE ATT&CK Techniques): 499\n",
      "Using validation split as test set\n",
      "Train samples: 14936\n",
      "Test samples: 2630\n",
      "Total unique labels (MITRE ATT&CK Techniques): 499\n",
      "Loading tokenizer: bert-base-uncased\n",
      "Loading tokenizer: bert-base-uncased\n",
      "Tokenizing texts...\n",
      "Tokenizing texts...\n",
      "Data preparation complete!\n",
      "ğŸ¤– Model yÃ¼kleniyor...\n",
      "Loading model: bert-base-uncased\n",
      "Number of labels: 499\n",
      "Data preparation complete!\n",
      "ğŸ¤– Model yÃ¼kleniyor...\n",
      "Loading model: bert-base-uncased\n",
      "Number of labels: 499\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 64.12 MiB is free. Process 677541 has 14.68 GiB memory in use. Of the allocated memory 14.52 GiB is allocated by PyTorch, and 33.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-52518293.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# Model yÃ¼kle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ğŸ¤– Model yÃ¼kleniyor...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m# EÄŸit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/Mitre_Attack_TTP_Mapping/src/model.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(model_name, num_labels, device)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;31m# Move to device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'cuda'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Model loaded on GPU\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mcuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m   1082\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m         \"\"\"\n\u001b[0;32m-> 1084\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1085\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1086\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mipu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mSelf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    928\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    928\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    928\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    955\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 957\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    958\u001b[0m             \u001b[0mp_should_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1082\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m         \"\"\"\n\u001b[0;32m-> 1084\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1085\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1086\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mipu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mSelf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 64.12 MiB is free. Process 677541 has 14.68 GiB memory in use. Of the allocated memory 14.52 GiB is allocated by PyTorch, and 33.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# BERT-base ile eÄŸitim\n",
    "from src.data_loader import prepare_data\n",
    "from src.model import load_model\n",
    "from src.train import train_model\n",
    "from src.evaluate import evaluate_model\n",
    "import torch\n",
    "import json\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "print(\"ğŸš€ EÄŸitim baÅŸlÄ±yor...\")\n",
    "\n",
    "# KonfigÃ¼rasyon\n",
    "model_name = \"bert-base-uncased\"\n",
    "epochs = 3\n",
    "batch_size = 32\n",
    "learning_rate = 2e-5\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Model: {model_name}\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Epochs: {epochs}, Batch: {batch_size}\\n\")\n",
    "\n",
    "# Veri hazÄ±rla\n",
    "print(\"ğŸ“Š Veri yÃ¼kleniyor...\")\n",
    "data = prepare_data(model_name=model_name, max_length=512)\n",
    "\n",
    "# Model yÃ¼kle\n",
    "print(\"ğŸ¤– Model yÃ¼kleniyor...\")\n",
    "model = load_model(model_name=model_name, num_labels=data['num_labels'], device=device)\n",
    "\n",
    "# EÄŸit\n",
    "print(\"ğŸ’ª EÄŸitim baÅŸladÄ±...\")\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_dir = f\"./outputs/{model_name.replace('/', '_')}_{timestamp}\"\n",
    "\n",
    "history = train_model(\n",
    "    model=model,\n",
    "    train_dataset=data['train_dataset'],\n",
    "    test_dataset=data['test_dataset'],\n",
    "    output_dir=output_dir,\n",
    "    batch_size=batch_size,\n",
    "    learning_rate=learning_rate,\n",
    "    num_epochs=epochs,\n",
    "    warmup_steps=500,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# DeÄŸerlendir\n",
    "print(\"ğŸ“ˆ Model deÄŸerlendiriliyor...\")\n",
    "metrics = evaluate_model(\n",
    "    model=model,\n",
    "    test_dataset=data['test_dataset'],\n",
    "    batch_size=batch_size,\n",
    "    device=device,\n",
    "    threshold=0.5,\n",
    "    label_list=data['label_list']\n",
    ")\n",
    "\n",
    "# SonuÃ§larÄ± kaydet\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "with open(f\"{output_dir}/labels.json\", 'w') as f:\n",
    "    json.dump(data['label_list'], f, indent=2)\n",
    "\n",
    "with open(f\"{output_dir}/training_history.json\", 'w') as f:\n",
    "    json.dump(history, f, indent=2)\n",
    "\n",
    "metrics_to_save = {k: float(v) if isinstance(v, (float, int)) else v \n",
    "                   for k, v in metrics.items() \n",
    "                   if k not in ['predictions', 'labels']}\n",
    "with open(f\"{output_dir}/evaluation_metrics.json\", 'w') as f:\n",
    "    json.dump(metrics_to_save, f, indent=2)\n",
    "\n",
    "print(f\"\\nâœ… EÄŸitim tamamlandÄ±!\")\n",
    "print(f\"ğŸ“ SonuÃ§lar: {output_dir}\")\n",
    "print(f\"ğŸ“ˆ Micro F1: {metrics['micro_f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d5fbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SonuÃ§larÄ± ZIP olarak indir\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "if os.path.exists('./outputs') and os.listdir('./outputs'):\n",
    "    print(\"ğŸ“¦ ZIP hazÄ±rlanÄ±yor...\")\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    zip_name = f\"training_results_{timestamp}.zip\"\n",
    "    \n",
    "    !zip -r {zip_name} outputs/\n",
    "    \n",
    "    from google.colab import files\n",
    "    files.download(zip_name)\n",
    "    \n",
    "    print(f\"âœ… {zip_name} indirildi!\")\n",
    "else:\n",
    "    print(\"âŒ HenÃ¼z eÄŸitim sonucu yok - Ã¶nce eÄŸitimi Ã§alÄ±ÅŸtÄ±r!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfe3d02",
   "metadata": {},
   "source": [
    "## 3. SonuÃ§larÄ± Ä°ndir ğŸ“¥\n",
    "\n",
    "EÄŸitim bitince sonuÃ§larÄ± ZIP olarak indir:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb19b473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SecBERT (security domain) ile eÄŸitim\n",
    "from src.data_loader import prepare_data\n",
    "from src.model import load_model\n",
    "from src.train import train_model\n",
    "from src.evaluate import evaluate_model\n",
    "import torch\n",
    "import json\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "print(\"ğŸš€ EÄŸitim baÅŸlÄ±yor...\")\n",
    "\n",
    "# KonfigÃ¼rasyon\n",
    "model_name = \"jackaduma/SecBERT\"\n",
    "epochs = 3\n",
    "batch_size = 16\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Model: {model_name}\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Epochs: {epochs}, Batch: {batch_size}\\n\")\n",
    "\n",
    "# Veri hazÄ±rla\n",
    "print(\"ğŸ“Š Veri yÃ¼kleniyor...\")\n",
    "data = prepare_data(model_name=model_name, max_length=512)\n",
    "\n",
    "# Model yÃ¼kle\n",
    "print(\"ğŸ¤– Model yÃ¼kleniyor...\")\n",
    "model = load_model(model_name=model_name, num_labels=data['num_labels'], device=device)\n",
    "\n",
    "# EÄŸit\n",
    "print(\"ğŸ’ª EÄŸitim baÅŸladÄ±...\")\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_dir = f\"./outputs/{model_name.replace('/', '_')}_{timestamp}\"\n",
    "\n",
    "history = train_model(\n",
    "    model=model,\n",
    "    train_dataset=data['train_dataset'],\n",
    "    test_dataset=data['test_dataset'],\n",
    "    output_dir=output_dir,\n",
    "    batch_size=batch_size,\n",
    "    learning_rate=2e-5,\n",
    "    num_epochs=epochs,\n",
    "    warmup_steps=500,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# DeÄŸerlendir\n",
    "print(\"ğŸ“ˆ Model deÄŸerlendiriliyor...\")\n",
    "metrics = evaluate_model(\n",
    "    model=model,\n",
    "    test_dataset=data['test_dataset'],\n",
    "    batch_size=batch_size,\n",
    "    device=device,\n",
    "    threshold=0.5,\n",
    "    label_list=data['label_list']\n",
    ")\n",
    "\n",
    "# SonuÃ§larÄ± kaydet\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "with open(f\"{output_dir}/labels.json\", 'w') as f:\n",
    "    json.dump(data['label_list'], f, indent=2)\n",
    "\n",
    "with open(f\"{output_dir}/training_history.json\", 'w') as f:\n",
    "    json.dump(history, f, indent=2)\n",
    "\n",
    "metrics_to_save = {k: float(v) if isinstance(v, (float, int)) else v \n",
    "                   for k, v in metrics.items() \n",
    "                   if k not in ['predictions', 'labels']}\n",
    "with open(f\"{output_dir}/evaluation_metrics.json\", 'w') as f:\n",
    "    json.dump(metrics_to_save, f, indent=2)\n",
    "\n",
    "print(f\"\\nâœ… EÄŸitim tamamlandÄ±!\")\n",
    "print(f\"ğŸ“ SonuÃ§lar: {output_dir}\")\n",
    "print(f\"ğŸ“ˆ Micro F1: {metrics['micro_f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6495edca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SonuÃ§larÄ± gÃ¶rÃ¼ntÃ¼le\n",
    "import json\n",
    "import os\n",
    "\n",
    "output_path = './outputs'\n",
    "if os.path.exists(output_path):\n",
    "    output_dirs = [d for d in os.listdir(output_path) if os.path.isdir(os.path.join(output_path, d))]\n",
    "    \n",
    "    if output_dirs:\n",
    "        latest_dir = sorted(output_dirs)[-1]\n",
    "        print(f\"ğŸ“ En son eÄŸitim: {latest_dir}\\n\")\n",
    "        \n",
    "        metrics_file = f\"{output_path}/{latest_dir}/evaluation_metrics.json\"\n",
    "        if os.path.exists(metrics_file):\n",
    "            with open(metrics_file) as f:\n",
    "                metrics = json.load(f)\n",
    "            \n",
    "            print(\"ğŸ“ˆ DeÄŸerlendirme Metrikleri:\")\n",
    "            print(f\"  Micro F1:    {metrics.get('micro_f1', 0):.4f}\")\n",
    "            print(f\"  Macro F1:    {metrics.get('macro_f1', 0):.4f}\")\n",
    "            print(f\"  Samples F1:  {metrics.get('samples_f1', 0):.4f}\")\n",
    "            print(f\"  Precision:   {metrics.get('micro_precision', 0):.4f}\")\n",
    "            print(f\"  Recall:      {metrics.get('micro_recall', 0):.4f}\")\n",
    "        \n",
    "        history_file = f\"{output_path}/{latest_dir}/training_history.json\"\n",
    "        if os.path.exists(history_file):\n",
    "            with open(history_file) as f:\n",
    "                history = json.load(f)\n",
    "            print(f\"\\nğŸ“‰ Final Loss: {history['train_loss'][-1]:.4f}\")\n",
    "    else:\n",
    "        print(\"âš ï¸  HenÃ¼z eÄŸitim sonucu yok\")\n",
    "else:\n",
    "    print(\"âš ï¸  outputs/ klasÃ¶rÃ¼ bulunamadÄ±\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
