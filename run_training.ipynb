{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c86f1534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.0+cu126\n",
      "CUDA available: True\n",
      "GPU: Tesla T4\n",
      "Memory: 14.74 GB\n"
     ]
    }
   ],
   "source": [
    "# GPU kontrolÃ¼\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207c780b",
   "metadata": {},
   "source": [
    "## Kurulum\n",
    "\n",
    "Sadece bir kez Ã§alÄ±ÅŸtÄ±r:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7033ef65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ KÃ¼tÃ¼phaneler yÃ¼klendi!\n"
     ]
    }
   ],
   "source": [
    "# Colab'da gerekli kÃ¼tÃ¼phaneleri yÃ¼kle\n",
    "!pip install -q transformers datasets torch scikit-learn tqdm\n",
    "\n",
    "print(\"âœ“ KÃ¼tÃ¼phaneler yÃ¼klendi!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e6a2ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'Mitre_Attack_TTP_Mapping'...\n",
      "remote: Enumerating objects: 27, done.\u001b[K\n",
      "remote: Counting objects: 100% (27/27), done.\u001b[K\n",
      "remote: Compressing objects: 100% (23/23), done.\u001b[K\n",
      "remote: Total 27 (delta 1), reused 27 (delta 1), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (27/27), 26.58 KiB | 13.29 MiB/s, done.\n",
      "Resolving deltas: 100% (1/1), done.\n",
      "/content/Mitre_Attack_TTP_Mapping\n",
      "âœ“ Proje dosyalarÄ± yÃ¼klendi!\n",
      "remote: Enumerating objects: 27, done.\u001b[K\n",
      "remote: Counting objects: 100% (27/27), done.\u001b[K\n",
      "remote: Compressing objects: 100% (23/23), done.\u001b[K\n",
      "remote: Total 27 (delta 1), reused 27 (delta 1), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (27/27), 26.58 KiB | 13.29 MiB/s, done.\n",
      "Resolving deltas: 100% (1/1), done.\n",
      "/content/Mitre_Attack_TTP_Mapping\n",
      "âœ“ Proje dosyalarÄ± yÃ¼klendi!\n"
     ]
    }
   ],
   "source": [
    "# GitHub'dan projeyi klonla\n",
    "!rm -rf Mitre_Attack_TTP_Mapping\n",
    "!git clone https://github.com/Aliekinozcetin/Mitre_Attack_TTP_Mapping.git\n",
    "%cd Mitre_Attack_TTP_Mapping\n",
    "\n",
    "print(\"âœ“ Proje dosyalarÄ± yÃ¼klendi!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60221a93",
   "metadata": {},
   "source": [
    "## EÄŸitim BaÅŸlat ðŸš€\n",
    "\n",
    "AÅŸaÄŸÄ±daki hÃ¼crelerden birini Ã§alÄ±ÅŸtÄ±r:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76a1dcfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "CTI-BERT TTP TAGGING - TRAINING PIPELINE\n",
      "======================================================================\n",
      "\n",
      "Configuration:\n",
      "  Model: bert-base-uncased\n",
      "  Epochs: 3\n",
      "  Batch size: 16\n",
      "  Learning rate: 2e-05\n",
      "  Max length: 512\n",
      "  Device: cuda\n",
      "======================================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "STEP 1: DATA PREPARATION\n",
      "======================================================================\n",
      "\n",
      "Loading dataset: tumeteor/Security-TTP-Mapping\n",
      "README.md: 4.02kB [00:00, 21.5MB/s]\n",
      "README.md: 4.02kB [00:00, 21.5MB/s]\n",
      "derived_procedure_train.tsv: 1.13MB [00:00, 49.5MB/s]\n",
      "expert_train.tsv: 128kB [00:00, 135MB/s]\n",
      "derived_procedure_train.tsv: 1.13MB [00:00, 49.5MB/s]\n",
      "expert_train.tsv: 128kB [00:00, 135MB/s]\n",
      "procedure_train.tsv: 874kB [00:00, 160MB/s]\n",
      "procedure_train.tsv: 874kB [00:00, 160MB/s]\n",
      "tram_train.tsv: 546kB [00:00, 142MB/s]\n",
      "tram_train.tsv: 546kB [00:00, 142MB/s]\n",
      "derived_procedure_dev.tsv: 215kB [00:00, 108MB/s]\n",
      "derived_procedure_dev.tsv: 215kB [00:00, 108MB/s]\n",
      "expert_dev.tsv: 19.0kB [00:00, 49.8MB/s]\n",
      "procedure_dev.tsv: 153kB [00:00, 83.2MB/s]\n",
      "expert_dev.tsv: 19.0kB [00:00, 49.8MB/s]\n",
      "procedure_dev.tsv: 153kB [00:00, 83.2MB/s]\n",
      "tram_dev.tsv: 97.4kB [00:00, 86.2MB/s]\n",
      "tram_dev.tsv: 97.4kB [00:00, 86.2MB/s]\n",
      "derived_procedure_test.tsv: 226kB [00:00, 110MB/s]\n",
      "derived_procedure_test.tsv: 226kB [00:00, 110MB/s]\n",
      "expert_test.tsv: 79.7kB [00:00, 97.8MB/s]\n",
      "expert_test.tsv: 79.7kB [00:00, 97.8MB/s]\n",
      "procedure_test.tsv: 184kB [00:00, 139MB/s]\n",
      "procedure_test.tsv: 184kB [00:00, 139MB/s]\n",
      "tram_test.tsv: 116kB [00:00, 114MB/s]\n",
      "Generating train split: 100% 14936/14936 [00:00<00:00, 218794.53 examples/s]\n",
      "Generating validation split: 100% 2630/2630 [00:00<00:00, 165219.12 examples/s]\n",
      "Generating test split: 100% 3170/3170 [00:00<00:00, 174707.55 examples/s]\n",
      "Available splits: ['train', 'validation', 'test']\n",
      "tram_test.tsv: 116kB [00:00, 114MB/s]\n",
      "Generating train split: 100% 14936/14936 [00:00<00:00, 218794.53 examples/s]\n",
      "Generating validation split: 100% 2630/2630 [00:00<00:00, 165219.12 examples/s]\n",
      "Generating test split: 100% 3170/3170 [00:00<00:00, 174707.55 examples/s]\n",
      "Available splits: ['train', 'validation', 'test']\n",
      "Using validation split as test set\n",
      "Train samples: 14936\n",
      "Test samples: 2630\n",
      "Total unique labels (MITRE ATT&CK Techniques): 499\n",
      "Using validation split as test set\n",
      "Train samples: 14936\n",
      "Test samples: 2630\n",
      "Total unique labels (MITRE ATT&CK Techniques): 499\n",
      "Loading tokenizer: bert-base-uncased\n",
      "tokenizer_config.json: 100% 48.0/48.0 [00:00<00:00, 414kB/s]\n",
      "Loading tokenizer: bert-base-uncased\n",
      "tokenizer_config.json: 100% 48.0/48.0 [00:00<00:00, 414kB/s]\n",
      "config.json: 100% 570/570 [00:00<00:00, 5.15MB/s]\n",
      "config.json: 100% 570/570 [00:00<00:00, 5.15MB/s]\n",
      "vocab.txt: 100% 232k/232k [00:00<00:00, 1.89MB/s]\n",
      "vocab.txt: 100% 232k/232k [00:00<00:00, 1.89MB/s]\n",
      "tokenizer.json: 100% 466k/466k [00:00<00:00, 3.68MB/s]\n",
      "tokenizer.json: 100% 466k/466k [00:00<00:00, 3.68MB/s]\n",
      "Tokenizing texts...\n",
      "Tokenizing texts...\n",
      "Data preparation complete!\n",
      "\n",
      "Label list saved to: ./outputs/bert-base-uncased_20251130_103210/labels.json\n",
      "\n",
      "======================================================================\n",
      "STEP 2: MODEL INITIALIZATION\n",
      "======================================================================\n",
      "\n",
      "Loading model: bert-base-uncased\n",
      "Number of labels: 499\n",
      "Data preparation complete!\n",
      "\n",
      "Label list saved to: ./outputs/bert-base-uncased_20251130_103210/labels.json\n",
      "\n",
      "======================================================================\n",
      "STEP 2: MODEL INITIALIZATION\n",
      "======================================================================\n",
      "\n",
      "Loading model: bert-base-uncased\n",
      "Number of labels: 499\n",
      "2025-11-30 10:32:22.675631: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1764498742.694963     525 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1764498742.700893     525 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1764498742.715742     525 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764498742.715768     525 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764498742.715773     525 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764498742.715777     525 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-11-30 10:32:22.720479: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-11-30 10:32:22.675631: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1764498742.694963     525 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1764498742.700893     525 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1764498742.715742     525 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764498742.715768     525 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764498742.715773     525 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764498742.715777     525 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-11-30 10:32:22.720479: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "model.safetensors: 100% 440M/440M [00:02<00:00, 189MB/s]  \n",
      "model.safetensors: 100% 440M/440M [00:02<00:00, 189MB/s]\n",
      "Model loaded on GPU\n",
      "Total parameters: 109,865,971\n",
      "Trainable parameters: 109,865,971\n",
      "\n",
      "======================================================================\n",
      "STEP 3: MODEL TRAINING\n",
      "======================================================================\n",
      "\n",
      "\n",
      "==================================================\n",
      "Starting training:\n",
      "  Device: cuda\n",
      "  Batch size: 16\n",
      "  Learning rate: 2e-05\n",
      "  Number of epochs: 3\n",
      "  Total steps: 2802\n",
      "==================================================\n",
      "\n",
      "\n",
      "Epoch 1/3\n",
      "--------------------------------------------------\n",
      "Training:   0% 0/934 [00:00<?, ?it/s]Model loaded on GPU\n",
      "Total parameters: 109,865,971\n",
      "Trainable parameters: 109,865,971\n",
      "\n",
      "======================================================================\n",
      "STEP 3: MODEL TRAINING\n",
      "======================================================================\n",
      "\n",
      "\n",
      "==================================================\n",
      "Starting training:\n",
      "  Device: cuda\n",
      "  Batch size: 16\n",
      "  Learning rate: 2e-05\n",
      "  Number of epochs: 3\n",
      "  Total steps: 2802\n",
      "==================================================\n",
      "\n",
      "\n",
      "Epoch 1/3\n",
      "--------------------------------------------------\n",
      "Training: 100% 934/934 [23:38<00:00,  1.52s/it, loss=0.0399]\n",
      "Average training loss: 0.2284\n",
      "Training: 100% 934/934 [23:38<00:00,  1.52s/it, loss=0.0399]\n",
      "Average training loss: 0.2284\n",
      "Checkpoint saved: ./outputs/bert-base-uncased_20251130_103210/checkpoint_epoch_1.pt\n",
      "\n",
      "Epoch 2/3\n",
      "--------------------------------------------------\n",
      "Training:   0% 0/934 [00:00<?, ?it/s]Checkpoint saved: ./outputs/bert-base-uncased_20251130_103210/checkpoint_epoch_1.pt\n",
      "\n",
      "Epoch 2/3\n",
      "--------------------------------------------------\n",
      "Training: 100% 934/934 [23:38<00:00,  1.52s/it, loss=0.0211]\n",
      "Average training loss: 0.0263\n",
      "Training: 100% 934/934 [23:38<00:00,  1.52s/it, loss=0.0211]\n",
      "Average training loss: 0.0263\n",
      "Checkpoint saved: ./outputs/bert-base-uncased_20251130_103210/checkpoint_epoch_2.pt\n",
      "\n",
      "Epoch 3/3\n",
      "--------------------------------------------------\n",
      "Training:   0% 0/934 [00:00<?, ?it/s]Checkpoint saved: ./outputs/bert-base-uncased_20251130_103210/checkpoint_epoch_2.pt\n",
      "\n",
      "Epoch 3/3\n",
      "--------------------------------------------------\n",
      "Training: 100% 934/934 [23:36<00:00,  1.52s/it, loss=0.0178]\n",
      "Average training loss: 0.0189\n",
      "Training: 100% 934/934 [23:36<00:00,  1.52s/it, loss=0.0178]\n",
      "Average training loss: 0.0189\n",
      "Checkpoint saved: ./outputs/bert-base-uncased_20251130_103210/checkpoint_epoch_3.pt\n",
      "Checkpoint saved: ./outputs/bert-base-uncased_20251130_103210/checkpoint_epoch_3.pt\n",
      "\n",
      "Final model saved: ./outputs/bert-base-uncased_20251130_103210/final_model.pt\n",
      "\n",
      "Training history saved to: ./outputs/bert-base-uncased_20251130_103210/training_history.json\n",
      "\n",
      "======================================================================\n",
      "STEP 4: MODEL EVALUATION\n",
      "======================================================================\n",
      "\n",
      "\n",
      "==================================================\n",
      "Starting evaluation:\n",
      "  Device: cuda\n",
      "  Batch size: 16\n",
      "  Threshold: 0.5\n",
      "  Test samples: 2630\n",
      "==================================================\n",
      "\n",
      "Predicting:   0% 0/165 [00:00<?, ?it/s]\n",
      "Final model saved: ./outputs/bert-base-uncased_20251130_103210/final_model.pt\n",
      "\n",
      "Training history saved to: ./outputs/bert-base-uncased_20251130_103210/training_history.json\n",
      "\n",
      "======================================================================\n",
      "STEP 4: MODEL EVALUATION\n",
      "======================================================================\n",
      "\n",
      "\n",
      "==================================================\n",
      "Starting evaluation:\n",
      "  Device: cuda\n",
      "  Batch size: 16\n",
      "  Threshold: 0.5\n",
      "  Test samples: 2630\n",
      "==================================================\n",
      "\n",
      "Predicting: 100% 165/165 [01:23<00:00,  1.98it/s]\n",
      "Predicting: 100% 165/165 [01:23<00:00,  1.98it/s]\n",
      "\n",
      "==================================================\n",
      "EVALUATION RESULTS\n",
      "==================================================\n",
      "\n",
      "Micro-averaged metrics (overall):\n",
      "  F1 Score:  0.0000\n",
      "  Precision: 0.0000\n",
      "  Recall:    0.0000\n",
      "\n",
      "Macro-averaged metrics (per label):\n",
      "  F1 Score:  0.0000\n",
      "  Precision: 0.0000\n",
      "  Recall:    0.0000\n",
      "\n",
      "Other metrics:\n",
      "  Samples F1:      0.0000\n",
      "  Subset Accuracy: 0.0042\n",
      "==================================================\n",
      "\n",
      "Evaluation metrics saved to: ./outputs/bert-base-uncased_20251130_103210/evaluation_metrics.json\n",
      "\n",
      "======================================================================\n",
      "PIPELINE COMPLETE!\n",
      "======================================================================\n",
      "\n",
      "Results saved to: ./outputs/bert-base-uncased_20251130_103210\n",
      "\n",
      "Key Metrics:\n",
      "  Micro F1:    0.0000\n",
      "  Macro F1:    0.0000\n",
      "  Samples F1:  0.0000\n",
      "======================================================================\n",
      "\n",
      "\n",
      "==================================================\n",
      "EVALUATION RESULTS\n",
      "==================================================\n",
      "\n",
      "Micro-averaged metrics (overall):\n",
      "  F1 Score:  0.0000\n",
      "  Precision: 0.0000\n",
      "  Recall:    0.0000\n",
      "\n",
      "Macro-averaged metrics (per label):\n",
      "  F1 Score:  0.0000\n",
      "  Precision: 0.0000\n",
      "  Recall:    0.0000\n",
      "\n",
      "Other metrics:\n",
      "  Samples F1:      0.0000\n",
      "  Subset Accuracy: 0.0042\n",
      "==================================================\n",
      "\n",
      "Evaluation metrics saved to: ./outputs/bert-base-uncased_20251130_103210/evaluation_metrics.json\n",
      "\n",
      "======================================================================\n",
      "PIPELINE COMPLETE!\n",
      "======================================================================\n",
      "\n",
      "Results saved to: ./outputs/bert-base-uncased_20251130_103210\n",
      "\n",
      "Key Metrics:\n",
      "  Micro F1:    0.0000\n",
      "  Macro F1:    0.0000\n",
      "  Samples F1:  0.0000\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# BERT-base ile eÄŸitim (3 epoch, batch 16)\n",
    "!python main.py \\\n",
    "    --model bert-base-uncased \\\n",
    "    --epochs 3 \\\n",
    "    --batch_size 16 \\\n",
    "    --learning_rate 2e-5 \\\n",
    "    --device cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb19b473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "CTI-BERT TTP TAGGING - TRAINING PIPELINE\n",
      "======================================================================\n",
      "\n",
      "Configuration:\n",
      "  Model: jackaduma/SecBERT\n",
      "  Epochs: 3\n",
      "  Batch size: 16\n",
      "  Learning rate: 2e-05\n",
      "  Max length: 512\n",
      "  Device: cuda\n",
      "======================================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "STEP 1: DATA PREPARATION\n",
      "======================================================================\n",
      "\n",
      "Loading dataset: tumeteor/Security-TTP-Mapping\n",
      "Available splits: ['train', 'validation', 'test']\n",
      "Available splits: ['train', 'validation', 'test']\n",
      "Using validation split as test set\n",
      "Train samples: 14936\n",
      "Test samples: 2630\n",
      "Total unique labels (MITRE ATT&CK Techniques): 499\n",
      "Using validation split as test set\n",
      "Train samples: 14936\n",
      "Test samples: 2630\n",
      "Total unique labels (MITRE ATT&CK Techniques): 499\n",
      "Loading tokenizer: jackaduma/SecBERT\n",
      "Loading tokenizer: jackaduma/SecBERT\n",
      "config.json: 100% 467/467 [00:00<00:00, 2.00MB/s]\n",
      "config.json: 100% 467/467 [00:00<00:00, 2.00MB/s]\n",
      "vocab.txt: 378kB [00:00, 17.6MB/s]\n",
      "vocab.txt: 378kB [00:00, 17.6MB/s]\n",
      "Tokenizing texts...\n",
      "Tokenizing texts...\n",
      "Data preparation complete!\n",
      "\n",
      "Label list saved to: ./outputs/jackaduma_SecBERT_20251130_114706/labels.json\n",
      "\n",
      "======================================================================\n",
      "STEP 2: MODEL INITIALIZATION\n",
      "======================================================================\n",
      "\n",
      "Loading model: jackaduma/SecBERT\n",
      "Number of labels: 499\n",
      "Data preparation complete!\n",
      "\n",
      "Label list saved to: ./outputs/jackaduma_SecBERT_20251130_114706/labels.json\n",
      "\n",
      "======================================================================\n",
      "STEP 2: MODEL INITIALIZATION\n",
      "======================================================================\n",
      "\n",
      "Loading model: jackaduma/SecBERT\n",
      "Number of labels: 499\n",
      "2025-11-30 11:47:14.690679: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1764503234.709492   18797 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1764503234.715283   18797 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1764503234.730120   18797 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764503234.730142   18797 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764503234.730146   18797 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764503234.730149   18797 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-11-30 11:47:14.734470: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-11-30 11:47:14.690679: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1764503234.709492   18797 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1764503234.715283   18797 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1764503234.730120   18797 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764503234.730142   18797 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764503234.730146   18797 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764503234.730149   18797 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-11-30 11:47:14.734470: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "model.safetensors: 100% 336M/336M [00:02<00:00, 137MB/s]  \n",
      "model.safetensors: 100% 336M/336M [00:02<00:00, 137MB/s]\n",
      "Model loaded on GPU\n",
      "Total parameters: 83,834,611\n",
      "Trainable parameters: 83,834,611\n",
      "\n",
      "======================================================================\n",
      "STEP 3: MODEL TRAINING\n",
      "======================================================================\n",
      "\n",
      "\n",
      "==================================================\n",
      "Starting training:\n",
      "  Device: cuda\n",
      "  Batch size: 16\n",
      "  Learning rate: 2e-05\n",
      "  Number of epochs: 3\n",
      "  Total steps: 2802\n",
      "==================================================\n",
      "\n",
      "\n",
      "Epoch 1/3\n",
      "--------------------------------------------------\n",
      "Training:   0% 0/934 [00:00<?, ?it/s]Model loaded on GPU\n",
      "Total parameters: 83,834,611\n",
      "Trainable parameters: 83,834,611\n",
      "\n",
      "======================================================================\n",
      "STEP 3: MODEL TRAINING\n",
      "======================================================================\n",
      "\n",
      "\n",
      "==================================================\n",
      "Starting training:\n",
      "  Device: cuda\n",
      "  Batch size: 16\n",
      "  Learning rate: 2e-05\n",
      "  Number of epochs: 3\n",
      "  Total steps: 2802\n",
      "==================================================\n",
      "\n",
      "\n",
      "Epoch 1/3\n",
      "--------------------------------------------------\n",
      "Training: 100% 934/934 [12:00<00:00,  1.30it/s, loss=0.0254]\n",
      "Average training loss: 0.1906\n",
      "Training: 100% 934/934 [12:00<00:00,  1.30it/s, loss=0.0254]\n",
      "Average training loss: 0.1906\n",
      "Checkpoint saved: ./outputs/jackaduma_SecBERT_20251130_114706/checkpoint_epoch_1.pt\n",
      "\n",
      "Epoch 2/3\n",
      "--------------------------------------------------\n",
      "Training:   0% 0/934 [00:00<?, ?it/s]Checkpoint saved: ./outputs/jackaduma_SecBERT_20251130_114706/checkpoint_epoch_1.pt\n",
      "\n",
      "Epoch 2/3\n",
      "--------------------------------------------------\n",
      "Training: 100% 934/934 [12:00<00:00,  1.30it/s, loss=0.0166]\n",
      "Average training loss: 0.0197\n",
      "Training: 100% 934/934 [12:00<00:00,  1.30it/s, loss=0.0166]\n",
      "Average training loss: 0.0197\n",
      "Checkpoint saved: ./outputs/jackaduma_SecBERT_20251130_114706/checkpoint_epoch_2.pt\n",
      "\n",
      "Epoch 3/3\n",
      "--------------------------------------------------\n",
      "Training:   0% 0/934 [00:00<?, ?it/s]Checkpoint saved: ./outputs/jackaduma_SecBERT_20251130_114706/checkpoint_epoch_2.pt\n",
      "\n",
      "Epoch 3/3\n",
      "--------------------------------------------------\n",
      "Training: 100% 934/934 [12:01<00:00,  1.30it/s, loss=0.0189]\n",
      "Average training loss: 0.0160\n",
      "Training: 100% 934/934 [12:01<00:00,  1.30it/s, loss=0.0189]\n",
      "Average training loss: 0.0160\n",
      "Checkpoint saved: ./outputs/jackaduma_SecBERT_20251130_114706/checkpoint_epoch_3.pt\n",
      "Checkpoint saved: ./outputs/jackaduma_SecBERT_20251130_114706/checkpoint_epoch_3.pt\n",
      "\n",
      "Final model saved: ./outputs/jackaduma_SecBERT_20251130_114706/final_model.pt\n",
      "\n",
      "Training history saved to: ./outputs/jackaduma_SecBERT_20251130_114706/training_history.json\n",
      "\n",
      "======================================================================\n",
      "STEP 4: MODEL EVALUATION\n",
      "======================================================================\n",
      "\n",
      "\n",
      "==================================================\n",
      "Starting evaluation:\n",
      "  Device: cuda\n",
      "  Batch size: 16\n",
      "  Threshold: 0.5\n",
      "  Test samples: 2630\n",
      "==================================================\n",
      "\n",
      "Predicting:   0% 0/165 [00:00<?, ?it/s]\n",
      "Final model saved: ./outputs/jackaduma_SecBERT_20251130_114706/final_model.pt\n",
      "\n",
      "Training history saved to: ./outputs/jackaduma_SecBERT_20251130_114706/training_history.json\n",
      "\n",
      "======================================================================\n",
      "STEP 4: MODEL EVALUATION\n",
      "======================================================================\n",
      "\n",
      "\n",
      "==================================================\n",
      "Starting evaluation:\n",
      "  Device: cuda\n",
      "  Batch size: 16\n",
      "  Threshold: 0.5\n",
      "  Test samples: 2630\n",
      "==================================================\n",
      "\n",
      "Predicting: 100% 165/165 [00:43<00:00,  3.83it/s]\n",
      "Predicting: 100% 165/165 [00:43<00:00,  3.83it/s]\n",
      "\n",
      "==================================================\n",
      "EVALUATION RESULTS\n",
      "==================================================\n",
      "\n",
      "Micro-averaged metrics (overall):\n",
      "  F1 Score:  0.0000\n",
      "  Precision: 0.0000\n",
      "  Recall:    0.0000\n",
      "\n",
      "Macro-averaged metrics (per label):\n",
      "  F1 Score:  0.0000\n",
      "  Precision: 0.0000\n",
      "  Recall:    0.0000\n",
      "\n",
      "Other metrics:\n",
      "  Samples F1:      0.0000\n",
      "  Subset Accuracy: 0.0042\n",
      "==================================================\n",
      "\n",
      "Evaluation metrics saved to: ./outputs/jackaduma_SecBERT_20251130_114706/evaluation_metrics.json\n",
      "\n",
      "======================================================================\n",
      "PIPELINE COMPLETE!\n",
      "======================================================================\n",
      "\n",
      "Results saved to: ./outputs/jackaduma_SecBERT_20251130_114706\n",
      "\n",
      "Key Metrics:\n",
      "  Micro F1:    0.0000\n",
      "  Macro F1:    0.0000\n",
      "  Samples F1:  0.0000\n",
      "======================================================================\n",
      "\n",
      "\n",
      "==================================================\n",
      "EVALUATION RESULTS\n",
      "==================================================\n",
      "\n",
      "Micro-averaged metrics (overall):\n",
      "  F1 Score:  0.0000\n",
      "  Precision: 0.0000\n",
      "  Recall:    0.0000\n",
      "\n",
      "Macro-averaged metrics (per label):\n",
      "  F1 Score:  0.0000\n",
      "  Precision: 0.0000\n",
      "  Recall:    0.0000\n",
      "\n",
      "Other metrics:\n",
      "  Samples F1:      0.0000\n",
      "  Subset Accuracy: 0.0042\n",
      "==================================================\n",
      "\n",
      "Evaluation metrics saved to: ./outputs/jackaduma_SecBERT_20251130_114706/evaluation_metrics.json\n",
      "\n",
      "======================================================================\n",
      "PIPELINE COMPLETE!\n",
      "======================================================================\n",
      "\n",
      "Results saved to: ./outputs/jackaduma_SecBERT_20251130_114706\n",
      "\n",
      "Key Metrics:\n",
      "  Micro F1:    0.0000\n",
      "  Macro F1:    0.0000\n",
      "  Samples F1:  0.0000\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SecBERT (security domain model) ile eÄŸitim\n",
    "!python main.py \\\n",
    "    --model jackaduma/SecBERT \\\n",
    "    --epochs 3 \\\n",
    "    --batch_size 16 \\\n",
    "    --device cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "578c426c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Metrics zip oluÅŸturuldu: 0.01 MB\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "download(\"download_438d2911-f4ed-4d46-ae4e-73c13429face\", \"results_metrics.zip\", 8302)",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sadece metrikleri ve son modeli zip'le (checkpoint'ler hariÃ§)\n",
    "from google.colab import files\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# GeÃ§ici klasÃ¶r oluÅŸtur\n",
    "os.makedirs('/content/results_small', exist_ok=True)\n",
    "\n",
    "# Her model klasÃ¶rÃ¼nden sadece Ã¶nemli dosyalarÄ± kopyala\n",
    "for model_dir in os.listdir('./outputs'):\n",
    "    src = f'./outputs/{model_dir}'\n",
    "    dst = f'/content/results_small/{model_dir}'\n",
    "    os.makedirs(dst, exist_ok=True)\n",
    "    \n",
    "    # Sadece bu dosyalarÄ± kopyala\n",
    "    for file in ['labels.json', 'training_history.json', 'evaluation_metrics.json', 'summary.json']:\n",
    "        src_file = f'{src}/{file}'\n",
    "        if os.path.exists(src_file):\n",
    "            shutil.copy(src_file, f'{dst}/{file}')\n",
    "\n",
    "# Zip oluÅŸtur (checkpoint'ler olmadan - ~1MB)\n",
    "shutil.make_archive('/content/results_metrics', 'zip', '/content/results_small')\n",
    "print(f\"âœ“ Metrics zip oluÅŸturuldu: {os.path.getsize('/content/results_metrics.zip') / 1024 / 1024:.2f} MB\")\n",
    "files.download('/content/results_metrics.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb24fc03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Final model zip: 685.11 MB\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "download(\"download_9d473ac6-ed5b-4bbd-bdff-3d04c4c29cf7\", \"results_final.zip\", 718389241)",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Son model + metrikler (~1-2GB)\n",
    "from google.colab import files\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "os.makedirs('/content/results_final', exist_ok=True)\n",
    "\n",
    "for model_dir in os.listdir('./outputs'):\n",
    "    src = f'./outputs/{model_dir}'\n",
    "    dst = f'/content/results_final/{model_dir}'\n",
    "    os.makedirs(dst, exist_ok=True)\n",
    "    \n",
    "    # Sadece final_model.pt ve metrikler\n",
    "    for file in ['final_model.pt', 'labels.json', 'training_history.json', \n",
    "                 'evaluation_metrics.json', 'summary.json']:\n",
    "        src_file = f'{src}/{file}'\n",
    "        if os.path.exists(src_file):\n",
    "            shutil.copy(src_file, f'{dst}/{file}')\n",
    "\n",
    "shutil.make_archive('/content/results_final', 'zip', '/content/results_final')\n",
    "size_mb = os.path.getsize('/content/results_final.zip') / 1024 / 1024\n",
    "print(f\"âœ“ Final model zip: {size_mb:.2f} MB\")\n",
    "files.download('/content/results_final.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb49086",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "mount failed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-1107941076.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Drive'a baÄŸlan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Metrikleri Drive'a kopyala\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0;34m'https://research.google.com/colaboratory/faq.html#drive-timeout'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         )\n\u001b[0;32m--> 272\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m       \u001b[0;31m# Terminate the DriveFS binary before killing bash.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: mount failed"
     ]
    }
   ],
   "source": [
    "## Manuel Ä°ndirme TalimatlarÄ± ðŸ“¥\n",
    "\n",
    "**Colab'da dosyalarÄ± bulmak iÃ§in:**\n",
    "\n",
    "1. Sol taraftaki **ðŸ“ Files** ikonuna tÄ±kla\n",
    "2. `/content/results_metrics.zip` dosyasÄ±nÄ± bul\n",
    "3. Dosyaya **saÄŸ tÄ±kla** > **Download** seÃ§\n",
    "\n",
    "**Veya terminalde gÃ¶rÃ¼ntÃ¼le:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f7c6ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cp: cannot create regular file '/content/drive/MyDrive/': No such file or directory\n",
      "ls: cannot access '/content/drive/MyDrive/results_metrics.zip': No such file or directory\n",
      "âœ… BaÅŸarÄ±lÄ±! Google Drive > MyDrive klasÃ¶rÃ¼nde 'results_metrics.zip' dosyasÄ±nÄ± bul\n",
      "ls: cannot access '/content/drive/MyDrive/results_metrics.zip': No such file or directory\n",
      "âœ… BaÅŸarÄ±lÄ±! Google Drive > MyDrive klasÃ¶rÃ¼nde 'results_metrics.zip' dosyasÄ±nÄ± bul\n"
     ]
    }
   ],
   "source": [
    "# Drive'Ä± mount et ve kopyala\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "# Mount et\n",
    "if not os.path.exists('/content/drive/MyDrive'):\n",
    "    print(\"Drive mount ediliyor...\")\n",
    "    drive.mount('/content/drive', force_remount=True)\n",
    "else:\n",
    "    print(\"Drive zaten mount edilmiÅŸ\")\n",
    "\n",
    "# Kopyala\n",
    "!cp /content/results_metrics.zip /content/drive/MyDrive/\n",
    "\n",
    "# Kontrol et\n",
    "!ls -lh /content/drive/MyDrive/results_metrics.zip\n",
    "\n",
    "print(\"âœ… Google Drive > MyDrive/results_metrics.zip kaydedildi!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d1ee74",
   "metadata": {},
   "source": [
    "## Google Drive'a Kaydet ðŸ’¾"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa00f98c",
   "metadata": {},
   "source": [
    "## SonuÃ§larÄ± Ä°ndir ðŸ“¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abd7276c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 root root 5.8G Nov 30 12:49 /content/results.zip\n"
     ]
    }
   ],
   "source": [
    "!ls -lh /content/results.zip"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
